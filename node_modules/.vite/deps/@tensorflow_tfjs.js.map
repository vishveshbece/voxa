{
  "version": 3,
  "sources": ["../../../../tfjs-core/src/public/chained_ops/abs.ts", "../../../../tfjs-core/src/public/chained_ops/acos.ts", "../../../../tfjs-core/src/public/chained_ops/acosh.ts", "../../../../tfjs-core/src/public/chained_ops/add.ts", "../../../../tfjs-core/src/public/chained_ops/all.ts", "../../../../tfjs-core/src/public/chained_ops/any.ts", "../../../../tfjs-core/src/public/chained_ops/arg_max.ts", "../../../../tfjs-core/src/public/chained_ops/arg_min.ts", "../../../../tfjs-core/src/public/chained_ops/as_scalar.ts", "../../../../tfjs-core/src/public/chained_ops/as_type.ts", "../../../../tfjs-core/src/public/chained_ops/as1d.ts", "../../../../tfjs-core/src/public/chained_ops/as2d.ts", "../../../../tfjs-core/src/public/chained_ops/as3d.ts", "../../../../tfjs-core/src/public/chained_ops/as4d.ts", "../../../../tfjs-core/src/public/chained_ops/as5d.ts", "../../../../tfjs-core/src/public/chained_ops/asin.ts", "../../../../tfjs-core/src/public/chained_ops/asinh.ts", "../../../../tfjs-core/src/public/chained_ops/atan.ts", "../../../../tfjs-core/src/public/chained_ops/atan2.ts", "../../../../tfjs-core/src/public/chained_ops/atanh.ts", "../../../../tfjs-core/src/public/chained_ops/avg_pool.ts", "../../../../tfjs-core/src/public/chained_ops/batch_to_space_nd.ts", "../../../../tfjs-core/src/public/chained_ops/batchnorm.ts", "../../../../tfjs-core/src/public/chained_ops/broadcast_to.ts", "../../../../tfjs-core/src/public/chained_ops/cast.ts", "../../../../tfjs-core/src/public/chained_ops/ceil.ts", "../../../../tfjs-core/src/public/chained_ops/clip_by_value.ts", "../../../../tfjs-core/src/public/chained_ops/concat.ts", "../../../../tfjs-core/src/public/chained_ops/conv1d.ts", "../../../../tfjs-core/src/public/chained_ops/conv2d_transpose.ts", "../../../../tfjs-core/src/public/chained_ops/conv2d.ts", "../../../../tfjs-core/src/public/chained_ops/cos.ts", "../../../../tfjs-core/src/public/chained_ops/cosh.ts", "../../../../tfjs-core/src/public/chained_ops/cumprod.ts", "../../../../tfjs-core/src/public/chained_ops/cumsum.ts", "../../../../tfjs-core/src/public/chained_ops/depth_to_space.ts", "../../../../tfjs-core/src/public/chained_ops/depthwise_conv2d.ts", "../../../../tfjs-core/src/public/chained_ops/dilation2d.ts", "../../../../tfjs-core/src/public/chained_ops/div_no_nan.ts", "../../../../tfjs-core/src/public/chained_ops/div.ts", "../../../../tfjs-core/src/public/chained_ops/dot.ts", "../../../../tfjs-core/src/public/chained_ops/elu.ts", "../../../../tfjs-core/src/public/chained_ops/equal.ts", "../../../../tfjs-core/src/public/chained_ops/erf.ts", "../../../../tfjs-core/src/public/chained_ops/euclidean_norm.ts", "../../../../tfjs-core/src/public/chained_ops/exp.ts", "../../../../tfjs-core/src/public/chained_ops/expand_dims.ts", "../../../../tfjs-core/src/public/chained_ops/expm1.ts", "../../../../tfjs-core/src/public/chained_ops/fft.ts", "../../../../tfjs-core/src/public/chained_ops/flatten.ts", "../../../../tfjs-core/src/public/chained_ops/floor.ts", "../../../../tfjs-core/src/public/chained_ops/floorDiv.ts", "../../../../tfjs-core/src/public/chained_ops/gather.ts", "../../../../tfjs-core/src/public/chained_ops/greater_equal.ts", "../../../../tfjs-core/src/public/chained_ops/greater.ts", "../../../../tfjs-core/src/public/chained_ops/ifft.ts", "../../../../tfjs-core/src/public/chained_ops/irfft.ts", "../../../../tfjs-core/src/public/chained_ops/is_finite.ts", "../../../../tfjs-core/src/public/chained_ops/is_inf.ts", "../../../../tfjs-core/src/public/chained_ops/is_nan.ts", "../../../../tfjs-core/src/public/chained_ops/leaky_relu.ts", "../../../../tfjs-core/src/public/chained_ops/less_equal.ts", "../../../../tfjs-core/src/public/chained_ops/less.ts", "../../../../tfjs-core/src/public/chained_ops/local_response_normalization.ts", "../../../../tfjs-core/src/public/chained_ops/log_sigmoid.ts", "../../../../tfjs-core/src/public/chained_ops/log_softmax.ts", "../../../../tfjs-core/src/public/chained_ops/log_sum_exp.ts", "../../../../tfjs-core/src/public/chained_ops/log.ts", "../../../../tfjs-core/src/public/chained_ops/log1p.ts", "../../../../tfjs-core/src/public/chained_ops/logical_and.ts", "../../../../tfjs-core/src/public/chained_ops/logical_not.ts", "../../../../tfjs-core/src/public/chained_ops/logical_or.ts", "../../../../tfjs-core/src/public/chained_ops/logical_xor.ts", "../../../../tfjs-core/src/public/chained_ops/mat_mul.ts", "../../../../tfjs-core/src/public/chained_ops/max_pool.ts", "../../../../tfjs-core/src/public/chained_ops/max.ts", "../../../../tfjs-core/src/public/chained_ops/maximum.ts", "../../../../tfjs-core/src/public/chained_ops/mean.ts", "../../../../tfjs-core/src/public/chained_ops/min.ts", "../../../../tfjs-core/src/public/chained_ops/minimum.ts", "../../../../tfjs-core/src/public/chained_ops/mirror_pad.ts", "../../../../tfjs-core/src/public/chained_ops/mod.ts", "../../../../tfjs-core/src/public/chained_ops/mul.ts", "../../../../tfjs-core/src/public/chained_ops/neg.ts", "../../../../tfjs-core/src/public/chained_ops/norm.ts", "../../../../tfjs-core/src/public/chained_ops/not_equal.ts", "../../../../tfjs-core/src/public/chained_ops/one_hot.ts", "../../../../tfjs-core/src/public/chained_ops/ones_like.ts", "../../../../tfjs-core/src/public/chained_ops/pad.ts", "../../../../tfjs-core/src/public/chained_ops/pool.ts", "../../../../tfjs-core/src/public/chained_ops/pow.ts", "../../../../tfjs-core/src/public/chained_ops/prelu.ts", "../../../../tfjs-core/src/public/chained_ops/prod.ts", "../../../../tfjs-core/src/public/chained_ops/reciprocal.ts", "../../../../tfjs-core/src/public/chained_ops/relu.ts", "../../../../tfjs-core/src/public/chained_ops/relu6.ts", "../../../../tfjs-core/src/public/chained_ops/reshape_as.ts", "../../../../tfjs-core/src/public/chained_ops/reshape.ts", "../../../../tfjs-core/src/public/chained_ops/resize_bilinear.ts", "../../../../tfjs-core/src/public/chained_ops/resize_nearest_neighbor.ts", "../../../../tfjs-core/src/public/chained_ops/reverse.ts", "../../../../tfjs-core/src/public/chained_ops/rfft.ts", "../../../../tfjs-core/src/public/chained_ops/round.ts", "../../../../tfjs-core/src/public/chained_ops/rsqrt.ts", "../../../../tfjs-core/src/public/chained_ops/selu.ts", "../../../../tfjs-core/src/public/chained_ops/separable_conv2d.ts", "../../../../tfjs-core/src/public/chained_ops/sigmoid.ts", "../../../../tfjs-core/src/public/chained_ops/sign.ts", "../../../../tfjs-core/src/public/chained_ops/sin.ts", "../../../../tfjs-core/src/public/chained_ops/sinh.ts", "../../../../tfjs-core/src/public/chained_ops/slice.ts", "../../../../tfjs-core/src/public/chained_ops/softmax.ts", "../../../../tfjs-core/src/public/chained_ops/softplus.ts", "../../../../tfjs-core/src/public/chained_ops/space_to_batch_nd.ts", "../../../../tfjs-core/src/public/chained_ops/split.ts", "../../../../tfjs-core/src/public/chained_ops/sqrt.ts", "../../../../tfjs-core/src/public/chained_ops/square.ts", "../../../../tfjs-core/src/public/chained_ops/squared_difference.ts", "../../../../tfjs-core/src/public/chained_ops/squeeze.ts", "../../../../tfjs-core/src/public/chained_ops/stack.ts", "../../../../tfjs-core/src/public/chained_ops/step.ts", "../../../../tfjs-core/src/public/chained_ops/strided_slice.ts", "../../../../tfjs-core/src/public/chained_ops/sub.ts", "../../../../tfjs-core/src/public/chained_ops/sum.ts", "../../../../tfjs-core/src/public/chained_ops/tan.ts", "../../../../tfjs-core/src/public/chained_ops/tanh.ts", "../../../../tfjs-core/src/public/chained_ops/tile.ts", "../../../../tfjs-core/src/public/chained_ops/to_bool.ts", "../../../../tfjs-core/src/public/chained_ops/to_float.ts", "../../../../tfjs-core/src/public/chained_ops/to_int.ts", "../../../../tfjs-core/src/public/chained_ops/topk.ts", "../../../../tfjs-core/src/public/chained_ops/transpose.ts", "../../../../tfjs-core/src/public/chained_ops/unique.ts", "../../../../tfjs-core/src/public/chained_ops/unsorted_segment_sum.ts", "../../../../tfjs-core/src/public/chained_ops/unstack.ts", "../../../../tfjs-core/src/public/chained_ops/where.ts", "../../../../tfjs-core/src/public/chained_ops/zeros_like.ts", "../../../../tfjs-backend-cpu/src/backend_cpu.ts", "../../../../tfjs-backend-cpu/src/version.ts", "../../../../tfjs-backend-cpu/src/base.ts", "../../../../tfjs-backend-cpu/src/kernels/Elu.ts", "../../../../tfjs-backend-cpu/src/kernels/LeakyRelu.ts", "../../../../tfjs-backend-cpu/src/kernels/Prelu.ts", "../../../../tfjs-backend-cpu/src/kernels/Relu.ts", "../../../../tfjs-backend-cpu/src/kernels/Relu6.ts", "../../../../tfjs-backend-cpu/src/utils/fused_utils.ts", "../../../../tfjs-backend-cpu/src/kernels/Reshape.ts", "../../../../tfjs-backend-cpu/src/kernels/BatchMatMul.ts", "../../../../tfjs-backend-cpu/src/kernels/_FusedMatMul.ts", "../../../../tfjs-backend-cpu/src/kernels/Acos.ts", "../../../../tfjs-backend-cpu/src/kernels/Acosh.ts", "../../../../tfjs-backend-cpu/src/kernels/AddN.ts", "../../../../tfjs-backend-cpu/src/kernels/All.ts", "../../../../tfjs-backend-cpu/src/kernels/Any.ts", "../../../../tfjs-backend-cpu/src/kernels/ArgMax.ts", "../../../../tfjs-backend-cpu/src/kernels/ArgMin.ts", "../../../../tfjs-backend-cpu/src/kernels/Asin.ts", "../../../../tfjs-backend-cpu/src/kernels/Asinh.ts", "../../../../tfjs-backend-cpu/src/kernels/Atan.ts", "../../../../tfjs-backend-cpu/src/kernels/Atan2.ts", "../../../../tfjs-backend-cpu/src/kernels/Atanh.ts", "../../../../tfjs-backend-cpu/src/utils/pool_utils.ts", "../../../../tfjs-backend-cpu/src/kernels/AvgPool.ts", "../../../../tfjs-backend-cpu/src/kernels/AvgPool3D.ts", "../../../../tfjs-backend-cpu/src/kernels/AvgPool3DGrad.ts", "../../../../tfjs-backend-cpu/src/kernels/AvgPoolGrad.ts", "../../../../tfjs-backend-cpu/src/kernels/BatchNorm.ts", "../../../../tfjs-backend-cpu/src/kernels/BatchToSpaceND.ts", "../../../../tfjs-backend-cpu/src/kernels/Bincount.ts", "../../../../tfjs-backend-cpu/src/kernels/BroadcastArgs.ts", "../../../../tfjs-backend-cpu/src/kernels/ClipByValue.ts", "../../../../tfjs-backend-cpu/src/kernels/ComplexAbs.ts", "../../../../tfjs-backend-cpu/src/kernels/Imag.ts", "../../../../tfjs-backend-cpu/src/kernels/Concat.ts", "../../../../tfjs-backend-cpu/src/kernels/Conv2D.ts", "../../../../tfjs-backend-cpu/src/kernels/Conv2DBackpropFilter.ts", "../../../../tfjs-backend-cpu/src/kernels/Conv2DBackpropInput.ts", "../../../../tfjs-backend-cpu/src/kernels/Conv3D.ts", "../../../../tfjs-backend-cpu/src/kernels/Conv3DBackpropFilterV2.ts", "../../../../tfjs-backend-cpu/src/kernels/Conv3DBackpropInputV2.ts", "../../../../tfjs-backend-cpu/src/kernels/Cos.ts", "../../../../tfjs-backend-cpu/src/kernels/Cosh.ts", "../../../../tfjs-backend-cpu/src/kernels/CropAndResize.ts", "../../../../tfjs-backend-cpu/src/kernels/Cumprod.ts", "../../../../tfjs-backend-cpu/src/kernels/Cumsum.ts", "../../../../tfjs-backend-cpu/src/kernels/DenseBincount.ts", "../../../../tfjs-backend-cpu/src/kernels/DepthToSpace.ts", "../../../../tfjs-backend-cpu/src/kernels/DepthwiseConv2dNative.ts", "../../../../tfjs-backend-cpu/src/kernels/DepthwiseConv2dNativeBackpropFilter.ts", "../../../../tfjs-backend-cpu/src/kernels/DepthwiseConv2dNativeBackpropInput.ts", "../../../../tfjs-backend-cpu/src/kernels/Diag.ts", "../../../../tfjs-backend-cpu/src/kernels/Dilation2D.ts", "../../../../tfjs-backend-cpu/src/kernels/Dilation2DBackpropFilter.ts", "../../../../tfjs-backend-cpu/src/kernels/Dilation2DBackpropInput.ts", "../../../../tfjs-backend-cpu/src/kernels/Sum.ts", "../../../../tfjs-backend-cpu/src/kernels/Einsum.ts", "../../../../tfjs-backend-cpu/src/kernels/EluGrad.ts", "../../../../tfjs-backend-cpu/src/kernels/Erf.ts", "../../../../tfjs-backend-cpu/src/kernels/ExpandDims.ts", "../../../../tfjs-backend-cpu/src/kernels/RealDiv.ts", "../../../../tfjs-backend-cpu/src/utils/fft_utils.ts", "../../../../tfjs-backend-cpu/src/kernels/FFT.ts", "../../../../tfjs-backend-cpu/src/kernels/Fill.ts", "../../../../tfjs-backend-cpu/src/kernels/FlipLeftRight.ts", "../../../../tfjs-backend-cpu/src/kernels/FloorDiv.ts", "../../../../tfjs-backend-cpu/src/kernels/FusedConv2D.ts", "../../../../tfjs-backend-cpu/src/kernels/FusedDepthwiseConv2D.ts", "../../../../tfjs-backend-cpu/src/kernels/GatherNd.ts", "../../../../tfjs-backend-cpu/src/kernels/GatherV2.ts", "../../../../tfjs-backend-cpu/src/kernels/IFFT.ts", "../../../../tfjs-backend-cpu/src/kernels/IsFinite.ts", "../../../../tfjs-backend-cpu/src/kernels/IsInf.ts", "../../../../tfjs-backend-cpu/src/kernels/IsNaN.ts", "../../../../tfjs-backend-cpu/src/kernels/LinSpace.ts", "../../../../tfjs-backend-cpu/src/kernels/Log1p.ts", "../../../../tfjs-backend-cpu/src/kernels/LogicalAnd.ts", "../../../../tfjs-backend-cpu/src/kernels/LogicalNot.ts", "../../../../tfjs-backend-cpu/src/kernels/LogicalOr.ts", "../../../../tfjs-backend-cpu/src/kernels/LRN.ts", "../../../../tfjs-backend-cpu/src/kernels/LRNGrad.ts", "../../../../tfjs-backend-cpu/src/kernels/Max.ts", "../../../../tfjs-backend-cpu/src/kernels/MaxPool.ts", "../../../../tfjs-backend-cpu/src/kernels/MaxPool3D.ts", "../../../../tfjs-backend-cpu/src/kernels/MaxPool3DGrad.ts", "../../../../tfjs-backend-cpu/src/kernels/MaxPoolGrad.ts", "../../../../tfjs-backend-cpu/src/kernels/MaxPoolWithArgmax_impl.ts", "../../../../tfjs-backend-cpu/src/kernels/MaxPoolWithArgmax.ts", "../../../../tfjs-backend-cpu/src/kernels/Mean.ts", "../../../../tfjs-backend-cpu/src/kernels/Min.ts", "../../../../tfjs-backend-cpu/src/kernels/MirrorPad.ts", "../../../../tfjs-backend-cpu/src/kernels/Mod.ts", "../../../../tfjs-backend-cpu/src/kernels/Multinomial.ts", "../../../../tfjs-backend-cpu/src/kernels/Softmax.ts", "../../../../tfjs-backend-cpu/src/kernels/NonMaxSuppressionV3.ts", "../../../../tfjs-backend-cpu/src/kernels/NonMaxSuppressionV4.ts", "../../../../tfjs-backend-cpu/src/kernels/NonMaxSuppressionV5.ts", "../../../../tfjs-backend-cpu/src/kernels/OneHot.ts", "../../../../tfjs-backend-cpu/src/kernels/ZerosLike.ts", "../../../../tfjs-backend-cpu/src/kernels/OnesLike.ts", "../../../../tfjs-backend-cpu/src/kernels/Pack.ts", "../../../../tfjs-backend-cpu/src/kernels/PadV2.ts", "../../../../tfjs-backend-cpu/src/kernels/Pow.ts", "../../../../tfjs-backend-cpu/src/kernels/RaggedGather.ts", "../../../../tfjs-backend-cpu/src/kernels/RaggedTensorToTensor.ts", "../../../../tfjs-backend-cpu/src/kernels/Range.ts", "../../../../tfjs-backend-cpu/src/kernels/Reciprocal.ts", "../../../../tfjs-backend-cpu/src/kernels/ResizeBilinear.ts", "../../../../tfjs-backend-cpu/src/kernels/ResizeBilinearGrad.ts", "../../../../tfjs-backend-cpu/src/kernels/ResizeNearestNeighbor.ts", "../../../../tfjs-backend-cpu/src/kernels/ResizeNearestNeighborGrad.ts", "../../../../tfjs-backend-cpu/src/kernels/Reverse.ts", "../../../../tfjs-backend-cpu/src/kernels/RotateWithOffset.ts", "../../../../tfjs-backend-cpu/src/kernels/Round.ts", "../../../../tfjs-backend-cpu/src/kernels/ScatterNd.ts", "../../../../tfjs-backend-cpu/src/kernels/SearchSorted_impl.ts", "../../../../tfjs-backend-cpu/src/kernels/SearchSorted.ts", "../../../../tfjs-backend-cpu/src/kernels/Select.ts", "../../../../tfjs-backend-cpu/src/kernels/Selu.ts", "../../../../tfjs-backend-cpu/src/kernels/Sign.ts", "../../../../tfjs-backend-cpu/src/kernels/Sin.ts", "../../../../tfjs-backend-cpu/src/kernels/Sinh.ts", "../../../../tfjs-backend-cpu/src/kernels/Softplus.ts", "../../../../tfjs-backend-cpu/src/kernels/SpaceToBatchND.ts", "../../../../tfjs-backend-cpu/src/kernels/SparseFillEmptyRows.ts", "../../../../tfjs-backend-cpu/src/kernels/SparseReshape.ts", "../../../../tfjs-backend-cpu/src/kernels/SparseSegmentMean.ts", "../../../../tfjs-backend-cpu/src/kernels/SparseSegmentSum.ts", "../../../../tfjs-backend-cpu/src/kernels/SparseToDense.ts", "../../../../tfjs-backend-cpu/src/kernels/SplitV.ts", "../../../../tfjs-backend-cpu/src/kernels/Square.ts", "../../../../tfjs-backend-cpu/src/kernels/Step.ts", "../../../../tfjs-backend-cpu/src/kernels/StridedSlice.ts", "../../../../tfjs-backend-cpu/src/kernels/StringNGrams.ts", "../../../../tfjs-backend-cpu/src/kernels/StringSplit.ts", "../../../../tfjs-backend-cpu/src/kernels/StringToHashBucketFast.ts", "../../../../tfjs-backend-cpu/src/kernels/Tan.ts", "../../../../tfjs-backend-cpu/src/kernels/Tanh.ts", "../../../../tfjs-backend-cpu/src/kernels/Tile.ts", "../../../../tfjs-backend-cpu/src/kernels/TopK.ts", "../../../../tfjs-backend-cpu/src/kernels/Transform.ts", "../../../../tfjs-backend-cpu/src/kernels/Unique.ts", "../../../../tfjs-backend-cpu/src/kernels/Unpack.ts", "../../../../tfjs-backend-cpu/src/kernels/UnsortedSegmentSum.ts", "../../../../tfjs-backend-cpu/src/register_all_kernels.ts", "../../@tensorflow/tfjs/src/version.ts", "../../@tensorflow/tfjs/src/index.ts"],
  "sourcesContent": ["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {abs} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    abs<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.abs = function<T extends Tensor>(this: T) {\n  this.throwIfDisposed();\n  return abs(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {acos} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    acos<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.acos = function<T extends Tensor>(this: T) {\n  this.throwIfDisposed();\n  return acos(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {acosh} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    acosh<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.acosh = function<T extends Tensor>(this: T) {\n  this.throwIfDisposed();\n  return acosh(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {add} from '../../ops/add';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    add<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.add = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return add(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {all} from '../../ops/all';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    all<T extends Tensor>(this: T, axis?: number|number[], keepDims?: boolean):\n        T;\n  }\n}\n\ngetGlobalTensorClass().prototype.all = function<T extends Tensor>(\n    this: T, axis?: number|number[], keepDims?: boolean): T {\n  this.throwIfDisposed();\n  return all(this, axis, keepDims);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {any} from '../../ops/any';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    any<T extends Tensor>(this: T, axis?: number|number[], keepDims?: boolean):\n        T;\n  }\n}\n\ngetGlobalTensorClass().prototype.any = function<T extends Tensor>(\n    this: T, axis?: number|number[], keepDims?: boolean): T {\n  this.throwIfDisposed();\n  return any(this, axis, keepDims);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {argMax} from '../../ops/arg_max';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    argMax<T extends Tensor>(axis?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.argMax = function<T extends Tensor>(\n    axis?: number): T {\n  this.throwIfDisposed();\n  return argMax(this, axis);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {argMin} from '../../ops/arg_min';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    argMin<T extends Tensor>(axis?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.argMin = function<T extends Tensor>(\n    axis: number): T {\n  this.throwIfDisposed();\n  return argMin(this, axis);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {reshape} from '../../ops/reshape';\nimport {getGlobalTensorClass, Scalar, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\nimport {assert} from '../../util';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    asScalar<T extends Tensor>(): Scalar;\n  }\n}\n\n/**\n * Converts a size-1 `tf.Tensor` to a `tf.Scalar`.\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.asScalar = function<T extends Tensor>(this: T):\n    Scalar {\n  this.throwIfDisposed();\n  assert(this.size === 1, () => 'The array must have only 1 element.');\n  return reshape(this, []);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {cast} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {DataType, Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    asType<T extends Tensor>(this: T, dtype: DataType): T;\n  }\n}\n\n/**\n * Casts a `tf.Tensor` to a specified dtype.\n *\n * @param dtype Data-type to cast the tensor to.\n *\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.asType = function<T extends Tensor>(\n    this: T, dtype: DataType): T {\n  this.throwIfDisposed();\n  return cast<T>(this, dtype);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {reshape} from '../../ops/reshape';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    as1D<T extends Tensor>(): Tensor1D;\n  }\n}\n\n/**\n * Converts a `tf.Tensor` to a `tf.Tensor1D`.\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.as1D = function<T extends Tensor>(): T {\n  this.throwIfDisposed();\n  return reshape(this, [this.size]) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {reshape} from '../../ops/reshape';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    as2D<T extends Tensor>(rows: number, columns: number): Tensor2D;\n  }\n}\n\n/**\n * Converts a `tf.Tensor` to a `tf.Tensor2D`.\n *\n * @param rows Number of rows in `tf.Tensor2D`.\n * @param columns Number of columns in `tf.Tensor2D`.\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.as2D = function<T extends Tensor>(\n    rows: number, columns: number): T {\n  this.throwIfDisposed();\n  return reshape(this, [rows, columns]) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {reshape} from '../../ops/reshape';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    as3D<T extends Tensor>(rows: number, columns: number, depth: number):\n        Tensor3D;\n  }\n}\n\n/**\n * Converts a `tf.Tensor` to a `tf.Tensor3D`.\n *\n * @param rows Number of rows in `tf.Tensor3D`.\n * @param columns Number of columns in `tf.Tensor3D`.\n * @param depth Depth of `tf.Tensor3D`.\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.as3D = function<T extends Tensor>(\n    rows: number, columns: number, depth: number): T {\n  this.throwIfDisposed();\n  return reshape(this, [rows, columns, depth]) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {reshape} from '../../ops/reshape';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    as4D<T extends Tensor>(\n        rows: number, columns: number, depth: number, depth2: number): Tensor4D;\n  }\n}\n\n/**\n * Converts a `tf.Tensor` to a `tf.Tensor4D`.\n *\n * @param rows Number of rows in `tf.Tensor4D`.\n * @param columns Number of columns in `tf.Tensor4D`.\n * @param depth Depth of `tf.Tensor4D`.\n * @param depth2 4th dimension of `tf.Tensor4D`.\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.as4D = function<T extends Tensor>(\n    rows: number, columns: number, depth: number, depth2: number): T {\n  this.throwIfDisposed();\n  return reshape(this, [rows, columns, depth, depth2]) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {reshape} from '../../ops/reshape';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    as5D<T extends Tensor>(\n        rows: number, columns: number, depth: number, depth2: number,\n        depth3: number): Tensor5D;\n  }\n}\n\n/**\n * Converts a `tf.Tensor` to a `tf.Tensor5D`.\n *\n * @param rows Number of rows in `tf.Tensor5D`.\n * @param columns Number of columns in `tf.Tensor5D`.\n * @param depth Depth of `tf.Tensor5D`.\n * @param depth2 4th dimension of `tf.Tensor5D`.\n * @param depth3 5th dimension of 'tf.Tensor5D'\n *\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.as5D = function<T extends Tensor>(\n    rows: number, columns: number, depth: number, depth2: number,\n    depth3: number): T {\n  this.throwIfDisposed();\n  return reshape(this, [rows, columns, depth, depth2, depth3]) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {asin} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    asin<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.asin = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return asin(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {asinh} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    asinh<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.asinh = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return asinh(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {atan} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    atan<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.atan = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return atan(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {atan2} from '../../ops/atan2';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    atan2<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.atan2 = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return atan2(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {atanh} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    atanh<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.atanh = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return atanh(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {ExplicitPadding} from '../../ops/conv_util';\nimport {avgPool} from '../../ops/avg_pool';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    avgPool<T extends Tensor3D|Tensor4D>(\n        filterSize: [number, number]|number, strides: [number, number]|number,\n        pad: 'valid'|'same'|number|ExplicitPadding,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.avgPool =\n    function<T extends Tensor3D|Tensor4D>(\n        this: T, filterSize: [number, number]|number,\n        strides: [number, number]|number,\n        pad: 'valid'|'same'|number|ExplicitPadding,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T {\n  this.throwIfDisposed();\n  return avgPool(this, filterSize, strides, pad, dimRoundingMode);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {batchToSpaceND} from '../../ops/batch_to_space_nd';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    batchToSpaceND<R extends Rank>(blockShape: number[], crops: number[][]):\n        Tensor<R>;\n  }\n}\n\ngetGlobalTensorClass().prototype.batchToSpaceND = function<R extends Rank>(\n    blockShape: number[], crops: number[][]): Tensor<R> {\n  this.throwIfDisposed();\n  return batchToSpaceND(this, blockShape, crops);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {batchNorm} from '../../ops/batchnorm';\nimport {getGlobalTensorClass, Tensor, Tensor1D} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    batchNorm<T extends Tensor>(\n        mean: Tensor<R>|Tensor1D|TensorLike,\n        variance: Tensor<R>|Tensor1D|TensorLike,\n        offset?: Tensor<R>|Tensor1D|TensorLike,\n        scale?: Tensor<R>|Tensor1D|TensorLike,\n        varianceEpsilon?: number): Tensor<R>;\n  }\n}\n\ngetGlobalTensorClass().prototype.batchNorm = function<R extends Rank>(\n    mean: Tensor<R>|Tensor1D|TensorLike,\n    variance: Tensor<R>|Tensor1D|TensorLike,\n    offset?: Tensor<R>|Tensor1D|TensorLike,\n    scale?: Tensor<R>|Tensor1D|TensorLike,\n    varianceEpsilon?: number): Tensor<R> {\n  this.throwIfDisposed();\n  return batchNorm(this, mean, variance, offset, scale, varianceEpsilon);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {broadcastTo} from '../../ops/broadcast_to';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, ShapeMap} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    broadcastTo<R extends Rank>(shape: ShapeMap[R]): Tensor<R>;\n  }\n}\n\ngetGlobalTensorClass().prototype.broadcastTo = function<R extends Rank>(\n    shape: ShapeMap[R]): Tensor<R> {\n  this.throwIfDisposed();\n  return broadcastTo(this, shape);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {cast} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {DataType, Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    cast<T extends Tensor>(dtype: DataType): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.cast = function<T extends Tensor>(\n    dtype: DataType): T {\n  this.throwIfDisposed();\n  return cast(this, dtype) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {ceil} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    ceil<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.ceil = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return ceil(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {clipByValue} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    clipByValue<T extends Tensor>(min: number, max: number): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.clipByValue = function<T extends Tensor>(\n    min: number, max: number): T {\n  this.throwIfDisposed();\n  return clipByValue(this, min, max) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {concat} from '../../ops/concat';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    concat<T extends Tensor>(tensors: T|Array<T|TensorLike>, axis?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.concat = function<T extends Tensor>(\n    x: T|Array<T|TensorLike>, axis?: number): T {\n  this.throwIfDisposed();\n  if (x instanceof Tensor) {\n    x = [x];\n  }\n  return concat([this, ...x], axis) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {conv1d} from '../../ops/conv1d';\nimport {ExplicitPadding} from '../../ops/conv_util';\nimport {getGlobalTensorClass, Tensor2D, Tensor3D} from '../../tensor';\nimport {Rank, TensorLike3D} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    conv1d<T extends Tensor2D|Tensor3D>(\n        filter: Tensor3D|TensorLike3D, stride: number,\n        pad: 'valid'|'same'|number|ExplicitPadding, dataFormat?: 'NWC'|'NCW',\n        dilation?: number, dimRoundingMode?: 'floor'|'round'|'ceil'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.conv1d = function<T extends Tensor2D|Tensor3D>(\n    filter: Tensor3D|TensorLike3D, stride: number,\n    pad: 'valid'|'same'|number|ExplicitPadding, dataFormat?: 'NWC'|'NCW',\n    dilation?: number, dimRoundingMode?: 'floor'|'round'|'ceil'): T {\n  this.throwIfDisposed();\n  return conv1d(\n             this, filter, stride, pad, dataFormat, dilation,\n             dimRoundingMode) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {conv2dTranspose} from '../../ops/conv2d_transpose';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank, TensorLike4D} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    conv2dTranspose<T extends Tensor3D|Tensor4D>(\n        filter: Tensor4D|TensorLike4D,\n        outputShape: [number, number, number, number]|[number, number, number],\n        strides: [number, number]|number, pad: 'valid'|'same'|number,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.conv2dTranspose =\n    function<T extends Tensor3D|Tensor4D>(\n        filter: Tensor4D|TensorLike4D,\n        outputShape: [number, number, number, number]|[number, number, number],\n        strides: [number, number]|number, pad: 'valid'|'same'|number,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T {\n  this.throwIfDisposed();\n  return conv2dTranspose(\n             this, filter, outputShape, strides, pad, dimRoundingMode) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {conv2d} from '../../ops/conv2d';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank, TensorLike4D} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    conv2d<T extends Tensor3D|Tensor4D>(\n        filter: Tensor4D|TensorLike4D, strides: [number, number]|number,\n        pad: 'valid'|'same'|number, dataFormat?: 'NHWC'|'NCHW',\n        dilations?: [number, number]|number,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.conv2d = function<T extends Tensor3D|Tensor4D>(\n    filter: Tensor4D|TensorLike4D, strides: [number, number]|number,\n    pad: 'valid'|'same'|number, dataFormat?: 'NHWC'|'NCHW',\n    dilations?: [number, number]|number,\n    dimRoundingMode?: 'floor'|'round'|'ceil'): T {\n  this.throwIfDisposed();\n  return conv2d(\n             this, filter, strides, pad, dataFormat, dilations,\n             dimRoundingMode) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {cos} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    cos<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.cos = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return cos(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {cosh} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    cosh<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.cosh = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return cosh(this);\n};\n", "/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the 'License');\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an 'AS IS' BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { cumprod } from '../../ops/cumprod';\nimport { getGlobalTensorClass, Tensor } from '../../tensor';\nimport { Rank } from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    cumprod<R extends Rank>(\n      axis?: number,\n      exclusive?: boolean,\n      reverse?: boolean\n    ): Tensor<R>;\n  }\n}\n\ngetGlobalTensorClass().prototype.cumprod = function <R extends Rank>(\n  axis?: number,\n  exclusive?: boolean,\n  reverse?: boolean\n): Tensor<R> {\n  this.throwIfDisposed();\n  return cumprod(this, axis, exclusive, reverse);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {cumsum} from '../../ops/cumsum';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    cumsum<R extends Rank>(\n        axis?: number, exclusive?: boolean, reverse?: boolean): Tensor<R>;\n  }\n}\n\ngetGlobalTensorClass().prototype.cumsum = function<R extends Rank>(\n    axis?: number, exclusive?: boolean, reverse?: boolean): Tensor<R> {\n  this.throwIfDisposed();\n  return cumsum(this, axis, exclusive, reverse);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {depthToSpace} from '../../ops/depth_to_space';\nimport {getGlobalTensorClass, Tensor4D} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    depthToSpace<T extends Tensor4D>(\n        blockSize: number, dataFormat: 'NHWC'|'NCHW'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.depthToSpace = function<T extends Tensor4D>(\n    blockSize: number, dataFormat: 'NHWC'|'NCHW'): T {\n  this.throwIfDisposed();\n  return depthToSpace(this, blockSize, dataFormat) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {depthwiseConv2d} from '../../ops/depthwise_conv2d';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank, TensorLike4D} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    depthwiseConv2d<T extends Tensor3D|Tensor4D>(\n        filter: Tensor4D|TensorLike4D, strides: [number, number]|number,\n        pad: 'valid'|'same'|number, dataFormat?: 'NHWC'|'NCHW',\n        dilations?: [number, number]|number,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.depthwiseConv2d =\n    function<T extends Tensor3D|Tensor4D>(\n        filter: Tensor4D|TensorLike4D, strides: [number, number]|number,\n        pad: 'valid'|'same'|number, dataFormat?: 'NHWC'|'NCHW',\n        dilations?: [number, number]|number,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T {\n  this.throwIfDisposed();\n  return depthwiseConv2d(\n             this, filter, strides, pad, dataFormat, dilations,\n             dimRoundingMode) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {dilation2d} from '../../ops/dilation2d';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank, TensorLike3D} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    dilation2d<T extends Tensor3D|Tensor4D>(\n        filter: Tensor3D|TensorLike3D, strides: [number, number]|number,\n        pad: 'valid'|'same', dilations?: [number, number]|number,\n        dataFormat?: 'NHWC'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.dilation2d =\n    function<T extends Tensor3D|Tensor4D>(\n        filter: Tensor3D|TensorLike3D, strides: [number, number]|number,\n        pad: 'valid'|'same', dilations?: [number, number]|number,\n        dataFormat?: 'NHWC'): T {\n  this.throwIfDisposed();\n  return dilation2d(this, filter, strides, pad, dilations, dataFormat) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {divNoNan} from '../../ops/div_no_nan';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    divNoNan<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.divNoNan = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return divNoNan(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {div} from '../../ops/div';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    div<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.div = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return div(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {dot} from '../../ops/dot';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    dot<T extends Tensor>(b: Tensor|TensorLike): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.dot = function<T extends Tensor>(\n    b: T|TensorLike): Tensor {\n  this.throwIfDisposed();\n  return dot(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {elu} from '../../ops/elu';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    elu<T extends Tensor>(): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.elu = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return elu(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {equal} from '../../ops/equal';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    equal<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.equal = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return equal(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {erf} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    erf<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.erf = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return erf(this);\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {euclideanNorm} from '../../ops/euclidean_norm';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    euclideanNorm<T extends Tensor>(\n        this: T, axis?: number|number[], keepDims?: boolean): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.euclideanNorm = function<T extends Tensor>(\n    this: T, axis?: number|number[], keepDims?: boolean): T {\n  this.throwIfDisposed();\n  return euclideanNorm(this, axis, keepDims) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {exp} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    exp<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.exp = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return exp(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {expandDims} from '../../ops/expand_dims';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    expandDims<T extends Tensor>(axis?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.expandDims = function<T extends Tensor>(\n    axis?: number): T {\n  this.throwIfDisposed();\n  return expandDims(this, axis);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {expm1} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    expm1<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.expm1 = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return expm1(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {fft} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    fft<T extends Tensor>(this: Tensor): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.fft = function<T extends Tensor>(this: Tensor):\n    T {\n  this.throwIfDisposed();\n  return fft(this) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {reshape} from '../../ops/reshape';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    flatten<T extends Tensor>(): Tensor1D;\n  }\n}\n\n/**\n * Flatten a Tensor to a 1D array.\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.flatten = function<T extends Tensor>(): T {\n  this.throwIfDisposed();\n  return reshape(this, [this.size]) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {floor} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    floor<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.floor = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return floor(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {floorDiv} from '../../ops/floorDiv';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    floorDiv<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.floorDiv = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return floorDiv(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {gather} from '../../ops/gather';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    gather<T extends Tensor>(\n        this: T, indices: Tensor|TensorLike, axis?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.gather = function<T extends Tensor>(\n    this: T, indices: Tensor|TensorLike, axis?: number): T {\n  this.throwIfDisposed();\n  return gather(this, indices, axis);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {greaterEqual} from '../../ops/greater_equal';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    greaterEqual<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.greaterEqual = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return greaterEqual(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {greater} from '../../ops/greater';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    greater<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.greater = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return greater(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {ifft} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    ifft<T extends Tensor>(this: Tensor): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.ifft = function<T extends Tensor>(\n    this: Tensor): T {\n  this.throwIfDisposed();\n  return ifft(this) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {irfft} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    irfft<T extends Tensor>(this: Tensor): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.irfft = function<T extends Tensor>(\n    this: Tensor): T {\n  this.throwIfDisposed();\n  return irfft(this) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {isFinite} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    isFinite<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.isFinite = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return isFinite(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {isInf} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    isInf<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.isInf = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return isInf(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {isNaN} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    isNaN<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.isNaN = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return isNaN(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {leakyRelu} from '../../ops/leaky_relu';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    leakyRelu<T extends Tensor>(alpha: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.leakyRelu = function<T extends Tensor>(\n    this: T, alpha: number): T {\n  this.throwIfDisposed();\n  return leakyRelu(this, alpha);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {lessEqual} from '../../ops/less_equal';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    lessEqual<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.lessEqual = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return lessEqual(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {less} from '../../ops/less';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    less<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.less = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return less(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {localResponseNormalization} from '../../ops/local_response_normalization';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    localResponseNormalization<T extends Tensor>(\n        depthRadius?: number, bias?: number, alpha?: number, beta?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.localResponseNormalization =\n    function<T extends Tensor>(\n        depthRadius?: number, bias?: number, alpha?: number, beta?: number): T {\n  this.throwIfDisposed();\n  return localResponseNormalization(this, depthRadius, bias, alpha, beta) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {logSigmoid} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    logSigmoid<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.logSigmoid = function<T extends Tensor>(\n    this: T): T {\n  this.throwIfDisposed();\n  return logSigmoid(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {logSoftmax} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    logSoftmax<T extends Tensor>(this: T, axis?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.logSoftmax = function<T extends Tensor>(\n    this: T, axis?: number): T {\n  this.throwIfDisposed();\n  return logSoftmax(this, axis);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {logSumExp} from '../../ops/log_sum_exp';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    logSumExp<T extends Tensor>(\n        this: T, axis?: number|number[], keepDims?: boolean): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.logSumExp = function<T extends Tensor>(\n    this: T, axis?: number|number[], keepDims?: boolean): T {\n  this.throwIfDisposed();\n  return logSumExp(this, axis, keepDims);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {log} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    log<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.log = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return log(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {log1p} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    log1p<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.log1p = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return log1p(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {logicalAnd} from '../../ops/logical_and';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    logicalAnd<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.logicalAnd = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return logicalAnd(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {logicalNot} from '../../ops/logical_not';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    logicalNot<T extends Tensor>(): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.logicalNot = function<T extends Tensor>(): T {\n  this.throwIfDisposed();\n  return logicalNot(this) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {logicalOr} from '../../ops/logical_or';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    logicalOr<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.logicalOr = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return logicalOr(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {logicalXor} from '../../ops/logical_xor';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    logicalXor<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.logicalXor = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return logicalXor(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {matMul} from '../../ops/mat_mul';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    matMul<T extends Tensor>(\n        b: Tensor|TensorLike, transposeA?: boolean,\n        transposeB?: boolean): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.matMul = function<T extends Tensor>(\n    this: T, b: Tensor|TensorLike, transposeA?: boolean,\n    transposeB?: boolean): Tensor {\n  this.throwIfDisposed();\n  return matMul(this, b, transposeA, transposeB);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {ExplicitPadding} from '../../ops/conv_util';\nimport {maxPool} from '../../ops/max_pool';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    maxPool<T extends Tensor3D|Tensor4D>(\n        filterSize: [number, number]|number, strides: [number, number]|number,\n        pad: 'valid'|'same'|number|ExplicitPadding,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.maxPool =\n    function<T extends Tensor3D|Tensor4D>(\n        this: T, filterSize: [number, number]|number,\n        strides: [number, number]|number,\n        pad: 'valid'|'same'|number|ExplicitPadding,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T {\n  this.throwIfDisposed();\n  return maxPool(this, filterSize, strides, pad, dimRoundingMode);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {max} from '../../ops/max';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    max<T extends Tensor>(axis?: number|number[], keepDims?: boolean): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.max = function<T extends Tensor>(\n    axis?: number|number[], keepDims?: boolean): T {\n  this.throwIfDisposed();\n  return max(this, axis, keepDims);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {maximum} from '../../ops/maximum';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    maximum<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.maximum = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return maximum(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {mean} from '../../ops/mean';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    mean<T extends Tensor>(axis?: number|number[], keepDims?: boolean): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.mean = function<T extends Tensor>(\n    axis?: number|number[], keepDims?: boolean): T {\n  this.throwIfDisposed();\n  return mean(this, axis, keepDims);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {min} from '../../ops/min';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    min<T extends Tensor>(axis?: number|number[], keepDims?: boolean): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.min = function<T extends Tensor>(\n    axis?: number|number[], keepDims?: boolean): T {\n  this.throwIfDisposed();\n  return min(this, axis, keepDims);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {minimum} from '../../ops/minimum';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    minimum<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.minimum = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return minimum(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {mirrorPad} from '../../ops/mirror_pad';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    mirrorPad<T extends Tensor>(\n        paddings: Array<[number, number]>, mode: 'reflect'|'symmetric'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.mirrorPad = function<T extends Tensor>(\n    this: T, paddings: Array<[number, number]>,\n    mode: 'reflect'|'symmetric'): T {\n  this.throwIfDisposed();\n  return mirrorPad(this, paddings, mode);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {mod} from '../../ops/mod';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    mod<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.mod = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return mod(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {mul} from '../../ops/mul';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    mul<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.mul = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return mul(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {neg} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    neg<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.neg = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return neg(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {norm} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    norm<T extends Tensor>(\n        ord?: number|'euclidean'|'fro', axis?: number|number[],\n        keepDims?: boolean): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.norm = function<T extends Tensor>(\n    ord?: number|'euclidean'|'fro', axis?: number|number[],\n    keepDims?: boolean) {\n  this.throwIfDisposed();\n  return norm(this, ord, axis, keepDims) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {notEqual} from '../../ops/not_equal';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    notEqual<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.notEqual = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return notEqual(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {oneHot} from '../../ops/one_hot';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    oneHot(depth: number, onValue: number, offValue: number): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.oneHot = function(\n    depth: number, onValue = 1, offValue = 0): Tensor {\n  this.throwIfDisposed();\n  return oneHot(this, depth, onValue, offValue);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {onesLike} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    onesLike<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.onesLike = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return onesLike(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {pad} from '../../ops/pad';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    pad<T extends Tensor>(\n        paddings: Array<[number, number]>, constantValue?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.pad = function<T extends Tensor>(\n    this: T, paddings: Array<[number, number]>, constantValue: number): T {\n  this.throwIfDisposed();\n  return pad(this, paddings, constantValue);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {ExplicitPadding} from '../../ops/conv_util';\nimport {pool} from '../../ops/pool';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    pool<T extends Tensor3D|Tensor4D>(\n        windowShape: [number, number]|number, poolingType: 'avg'|'max',\n        padding: 'valid'|'same'|number|ExplicitPadding,\n        diationRate?: [number, number]|number,\n        strides?: [number, number]|number,\n        dimRoundingMode?: 'floor'|'round'|'ceil'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.pool = function<T extends Tensor3D|Tensor4D>(\n    this: T, windowShape: [number, number]|number, poolingType: 'max'|'avg',\n    padding: 'valid'|'same'|number|ExplicitPadding,\n    dilationRate?: [number, number]|number,\n    strides?: [number, number]|number,\n    dimRoundingMode?: 'floor'|'round'|'ceil'): T {\n  this.throwIfDisposed();\n  return pool(this, windowShape, poolingType, padding, dilationRate, strides,\n              dimRoundingMode);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {pow} from '../../ops/pow';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    pow<T extends Tensor>(exp: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.pow = function<T extends Tensor>(\n    exp: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return pow(this, exp);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {prelu} from '../../ops/prelu';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    prelu<T extends Tensor>(alpha: T|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.prelu = function<T extends Tensor>(\n    this: T, alpha: T|TensorLike): T {\n  this.throwIfDisposed();\n  return prelu(this, alpha);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {prod} from '../../ops/prod';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    prod<T extends Tensor>(this: T, axis?: number|number[], keepDims?: boolean):\n        T;\n  }\n}\n\ngetGlobalTensorClass().prototype.prod = function<T extends Tensor>(\n    this: T, axis?: number|number[], keepDims?: boolean): T {\n  this.throwIfDisposed();\n  return prod(this, axis, keepDims);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {reciprocal} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    reciprocal<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.reciprocal = function<T extends Tensor>(\n    this: T): T {\n  this.throwIfDisposed();\n  return reciprocal(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {relu} from '../../ops/relu';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    relu<T extends Tensor>(): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.relu = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return relu(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {relu6} from '../../ops/relu6';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    relu6<T extends Tensor>(): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.relu6 = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return relu6(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {reshape} from '../../ops/reshape';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    reshapeAs<T extends Tensor>(x: T): T;\n  }\n}\n\n/**\n * Reshapes the tensor into the shape of the provided tensor.\n *\n * @param x The tensor of required shape.\n *\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.reshapeAs = function<T extends Tensor>(x: T):\n    T {\n  this.throwIfDisposed();\n  return reshape(this, x.shape) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {reshape} from '../../ops/reshape';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    reshape<T extends Tensor>(shape: number[]): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.reshape = function<T extends Tensor>(\n    shape: number[]): T {\n  this.throwIfDisposed();\n  return reshape(this, shape) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {resizeBilinear} from '../../ops/image/resize_bilinear';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    resizeBilinear<T extends Tensor3D|Tensor4D>(\n        newShape2D: [number, number], alignCorners?: boolean,\n        halfPixelCenters?: boolean): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.resizeBilinear =\n    function<T extends Tensor3D|Tensor4D>(\n        this: T, newShape2D: [number, number], alignCorners?: boolean,\n        halfPixelCenters?: boolean): T {\n  this.throwIfDisposed();\n  return resizeBilinear(this, newShape2D, alignCorners, halfPixelCenters);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {resizeNearestNeighbor} from '../../ops/image/resize_nearest_neighbor';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    resizeNearestNeighbor<T extends Tensor3D|Tensor4D>(\n        newShape2D: [number, number], alignCorners?: boolean,\n        halfFloatCenters?: boolean): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.resizeNearestNeighbor =\n    function<T extends Tensor3D|Tensor4D>(\n        this: T, newShape2D: [number, number], alignCorners?: boolean,\n        halfFloatCenters?: boolean): T {\n  this.throwIfDisposed();\n  return resizeNearestNeighbor(\n      this, newShape2D, alignCorners, halfFloatCenters);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {reverse} from '../../ops/reverse';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    reverse<T extends Tensor>(this: T, axis?: number|number[]): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.reverse = function<T extends Tensor>(\n    this: T, axis?: number|number[]): T {\n  this.throwIfDisposed();\n  return reverse(this, axis);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {rfft} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    rfft<T extends Tensor>(this: Tensor): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.rfft = function<T extends Tensor>(\n    this: Tensor): T {\n  this.throwIfDisposed();\n  return rfft(this) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {round} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    round<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.round = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return round(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {rsqrt} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    rsqrt<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.rsqrt = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return rsqrt(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {selu} from '../../ops/selu';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    selu<T extends Tensor>(): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.selu = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return selu(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {separableConv2d} from '../../ops/separable_conv2d';\nimport {getGlobalTensorClass, Tensor3D, Tensor4D} from '../../tensor';\nimport {Rank, TensorLike, TensorLike4D} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    separableConv2d<T extends Tensor3D|Tensor4D>(\n        depthwiseFilter: Tensor4D|TensorLike4D,\n        pointwiseFilter: Tensor4D|TensorLike, strides: [number, number]|number,\n        pad: 'valid'|'same', dilation?: [number, number]|number,\n        dataFormat?: 'NHWC'|'NCHW'): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.separableConv2d =\n    function<T extends Tensor3D|Tensor4D>(\n        depthwiseFilter: Tensor4D|TensorLike4D,\n        pointwiseFilter: Tensor4D|TensorLike, strides: [number, number]|number,\n        pad: 'valid'|'same', dilation?: [number, number]|number,\n        dataFormat?: 'NHWC'|'NCHW'): T {\n  this.throwIfDisposed();\n  return separableConv2d(\n             this, depthwiseFilter, pointwiseFilter, strides, pad, dilation,\n             dataFormat) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {sigmoid} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    sigmoid<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.sigmoid = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return sigmoid(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {sign} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    sign<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.sign = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return sign(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {sin} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    sin<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.sin = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return sin(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {sinh} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    sinh<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.sinh = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return sinh(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {slice} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    slice<T extends Tensor>(\n        this: T, begin: number|number[], size?: number|number[]): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.slice = function<T extends Tensor>(\n    this: T, begin: number|number[], size?: number|number[]): T {\n  this.throwIfDisposed();\n  return slice(this, begin, size);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {softmax} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    softmax<T extends Tensor>(this: T, dim?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.softmax = function<T extends Tensor>(\n    this: T, dim: number): T {\n  this.throwIfDisposed();\n  return softmax(this, dim);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {softplus} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    softplus<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.softplus = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return softplus(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {spaceToBatchND} from '../../ops/space_to_batch_nd';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    spaceToBatchND<R extends Rank>(blockShape: number[], paddings: number[][]):\n        Tensor<R>;\n  }\n}\n\ngetGlobalTensorClass().prototype.spaceToBatchND = function<R extends Rank>(\n    blockShape: number[], paddings: number[][]): Tensor<R> {\n  this.throwIfDisposed();\n  return spaceToBatchND(this, blockShape, paddings);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {split} from '../../ops/split';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    split<T extends Tensor>(numOrSizeSplits: number[]|number, axis?: number):\n        T[];\n  }\n}\n\ngetGlobalTensorClass().prototype.split = function<T extends Tensor>(\n    numOrSizeSplits: number[]|number, axis?: number): T[] {\n  this.throwIfDisposed();\n  return split(this, numOrSizeSplits, axis);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {sqrt} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    sqrt<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.sqrt = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return sqrt(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {square} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    square<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.square = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return square(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {squaredDifference} from '../../ops/squared_difference';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    squaredDifference<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.squaredDifference = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return squaredDifference(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {squeeze} from '../../ops/squeeze';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    squeeze<T extends Tensor>(axis?: number[]): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.squeeze = function<T extends Tensor>(\n    axis?: number[]): T {\n  this.throwIfDisposed();\n  return squeeze(this, axis);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {stack} from '../../ops/stack';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    stack<T extends Tensor>(x: Tensor|Tensor[], axis?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.stack = function<T extends Tensor>(\n    x: Tensor|Tensor[], axis?: number): T {\n  this.throwIfDisposed();\n  const tensorsToBeStacked = x instanceof Tensor ? [this, x] : [this, ...x];\n  return stack(tensorsToBeStacked, axis) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {step} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    step<T extends Tensor>(this: T, alpha?: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.step = function<T extends Tensor>(\n    this: T, alpha?: number) {\n  this.throwIfDisposed();\n  return step(this, alpha);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {stridedSlice} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    stridedSlice<T extends Tensor>(\n        this: Tensor, begin: number[], end: number[], strides: number[],\n        beginMask?: number, endMask?: number, ellipsisMask?: number,\n        newAxisMask?: number, shrinkAxisMask?: number): Tensor;\n  }\n}\n\ngetGlobalTensorClass().prototype.stridedSlice = function<T extends Tensor>(\n    this: Tensor, begin: number[], end: number[], strides: number[],\n    beginMask?: number, endMask?: number, ellipsisMask?: number,\n    newAxisMask?: number, shrinkAxisMask?: number): T {\n  this.throwIfDisposed();\n  return stridedSlice(\n             this, begin, end, strides, beginMask, endMask, ellipsisMask,\n             newAxisMask, shrinkAxisMask) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {sub} from '../../ops/sub';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    sub<T extends Tensor>(b: Tensor|TensorLike): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.sub = function<T extends Tensor>(\n    b: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return sub(this, b);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {sum} from '../../ops/sum';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    sum<T extends Tensor>(axis?: number|number[], keepDims?: boolean): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.sum = function<T extends Tensor>(\n    axis?: number|number[], keepDims?: boolean): T {\n  this.throwIfDisposed();\n  return sum(this, axis, keepDims);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {tan} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    tan<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.tan = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return tan(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {tanh} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    tanh<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.tanh = function<T extends Tensor>(this: T): T {\n  this.throwIfDisposed();\n  return tanh(this);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {tile} from '../../ops/tile';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    tile<T extends Tensor>(b: number[]): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.tile = function<T extends Tensor>(\n    reps: number[]): T {\n  this.throwIfDisposed();\n  return tile(this, reps) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {cast} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    toBool<T extends Tensor>(this: T): T;\n  }\n}\n\n/**\n * Casts the array to type `bool`\n *\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.toBool = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return cast<T>(this, 'bool');\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {cast} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    toFloat<T extends Tensor>(this: T): T;\n  }\n}\n\n/**\n * Casts the array to type `float32`\n *\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.toFloat = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return cast<T>(this, 'float32');\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {cast} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    toInt<T extends Tensor>(this: T): T;\n  }\n}\n\n/**\n * Casts the array to type `int32`\n *\n * @doc {heading: 'Tensors', subheading: 'Classes'}\n */\ngetGlobalTensorClass().prototype.toInt = function<T extends Tensor>(this: T):\n    T {\n  this.throwIfDisposed();\n  return cast<T>(this, 'int32');\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {topk} from '../../ops/topk';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    topk<T extends Tensor>(this: T, k?: number, sorted?: boolean):\n        {values: T, indices: T};\n  }\n}\n\ngetGlobalTensorClass().prototype.topk = function<T extends Tensor>(\n    this: T, k?: number, sorted?: boolean): {values: T, indices: T} {\n  this.throwIfDisposed();\n  return topk(this, k, sorted) as {values: T, indices: T};\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {transpose} from '../../ops/transpose';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    transpose<T extends Tensor>(perm?: number[]): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.transpose = function<T extends Tensor>(\n    this: T, perm?: number[]): T {\n  this.throwIfDisposed();\n  return transpose(this, perm);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {unique} from '../../ops/unique';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    unique<T extends Tensor>(this: T, axis?: number): {values: T, indices: T};\n  }\n}\n\ngetGlobalTensorClass().prototype.unique = function<T extends Tensor>(\n    this: T, axis?: number): {values: T, indices: T} {\n  this.throwIfDisposed();\n  return unique(this, axis) as {values: T, indices: T};\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {unsortedSegmentSum} from '../../ops/unsorted_segment_sum';\nimport {getGlobalTensorClass, Tensor, Tensor1D} from '../../tensor';\nimport {Rank, TensorLike1D} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    unsortedSegmentSum<T extends Tensor>(\n        this: T, segmentIds: Tensor1D|TensorLike1D, numSegments: number): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.unsortedSegmentSum =\n    function<T extends Tensor>(\n        this: T, segmentIds: Tensor1D|TensorLike1D, numSegments: number): T {\n  this.throwIfDisposed();\n  return unsortedSegmentSum(this, segmentIds, numSegments);\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {unstack} from '../../ops/unstack';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    unstack<T extends Tensor>(axis?: number): T[];\n  }\n}\n\ngetGlobalTensorClass().prototype.unstack = function<T extends Tensor>(\n    axis?: number): T[] {\n  this.throwIfDisposed();\n  return unstack(this, axis) as T[];\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {where} from '../../ops/where';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank, TensorLike} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    where<T extends Tensor>(condition: Tensor|TensorLike, x: Tensor|TensorLike):\n        T;\n  }\n}\n\ngetGlobalTensorClass().prototype.where = function<T extends Tensor>(\n    condition: Tensor|TensorLike, x: Tensor|TensorLike): T {\n  this.throwIfDisposed();\n  return where(condition, this, x) as T;\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO update import path once op is modularized.\nimport {zerosLike} from '../../ops/ops';\nimport {getGlobalTensorClass, Tensor} from '../../tensor';\nimport {Rank} from '../../types';\n\ndeclare module '../../tensor' {\n  interface Tensor<R extends Rank = Rank> {\n    zerosLike<T extends Tensor>(this: T): T;\n  }\n}\n\ngetGlobalTensorClass().prototype.zerosLike = function<T extends Tensor>(\n    this: T): T {\n  this.throwIfDisposed();\n  return zerosLike(this);\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, BackendTimingInfo, buffer, DataStorage, DataType, engine, env, kernel_impls, KernelBackend, Rank, ShapeMap, Tensor, Tensor2D, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nconst whereImpl = kernel_impls.whereImpl;\nimport {assertNotComplex} from './cpu_util';\n\ninterface DataId {}\n\nexport interface TensorData<D extends DataType> {\n  values?: backend_util.BackendValues;\n  dtype: D;\n  // For complex numbers, the real and imaginary parts are stored as their own\n  // individual tensors, with a parent joining the two with the\n  // complexTensorInfos field.\n  complexTensorInfos?: {real: TensorInfo, imag: TensorInfo};\n  // refCount keeps track of how many tensors reference it. Used for memory\n  // management.\n  refCount: number;\n}\n\nexport class MathBackendCPU extends KernelBackend {\n  public blockSize = 48;\n\n  data: DataStorage<TensorData<DataType>>;\n  private firstUse = true;\n  private static nextDataId = 0;\n  private nextDataId(): number {\n    return MathBackendCPU.nextDataId++;\n  }\n\n  constructor() {\n    super();\n    this.data = new DataStorage(this, engine());\n  }\n\n  write(values: backend_util.BackendValues, shape: number[], dtype: DataType):\n      DataId {\n    if (this.firstUse) {\n      this.firstUse = false;\n      if (env().get('IS_NODE')) {\n        backend_util.warn(\n            '\\n============================\\n' +\n            'Hi, looks like you are running TensorFlow.js in ' +\n            'Node.js. To speed things up dramatically, install our node ' +\n            'backend, visit https://github.com/tensorflow/tfjs-node for more details. ' +\n            '\\n============================');\n      }\n    }\n    const dataId = {id: this.nextDataId()};\n\n    this.data.set(dataId, {values, dtype, refCount: 1});\n\n    return dataId;\n  }\n\n  /**\n   * Create a data bucket in cpu backend.\n   * @param shape Shape of the `TensorInfo`.\n   * @param dtype DType of the `TensorInfo`.\n   * @param values The value of the `TensorInfo` stored as a flattened array.\n   */\n  makeTensorInfo(\n      shape: number[], dtype: DataType,\n      values?: backend_util.BackendValues|string[]): TensorInfo {\n    let outId;\n    if (dtype === 'string' && values != null && values.length > 0 &&\n        util.isString(values[0])) {\n      const encodedValues =\n          (values as {} as string[]).map(d => util.encodeString(d));\n\n      outId = this.write(encodedValues, shape, dtype);\n    } else {\n      outId = this.write(values as TypedArray, shape, dtype);\n    }\n\n    return {dataId: outId, shape, dtype};\n  }\n\n  /** Return refCount of a `TensorData`. */\n  refCount(dataId: DataId): number {\n    if (this.data.has(dataId)) {\n      const tensorData = this.data.get(dataId);\n      return tensorData.refCount;\n    }\n    return 0;\n  }\n\n  /** Increase refCount of a `TensorData`. */\n  incRef(dataId: DataId): void {\n    const tensorData = this.data.get(dataId);\n    tensorData.refCount++;\n  }\n\n  /** Decrease refCount of a `TensorData`. */\n  decRef(dataId: DataId): void {\n    if (this.data.has(dataId)) {\n      const tensorData = this.data.get(dataId);\n      tensorData.refCount--;\n    }\n  }\n\n  move(\n      dataId: DataId, values: backend_util.BackendValues, shape: number[],\n      dtype: DataType, refCount: number): void {\n    this.data.set(dataId, {values, dtype, refCount});\n  }\n\n  numDataIds(): number {\n    return this.data.numDataIds();\n  }\n\n  async read(dataId: DataId): Promise<backend_util.BackendValues> {\n    return this.readSync(dataId);\n  }\n  readSync(dataId: DataId): backend_util.BackendValues {\n    const {dtype, complexTensorInfos} = this.data.get(dataId);\n\n    if (dtype === 'complex64') {\n      const realValues =\n          this.readSync(complexTensorInfos.real.dataId) as Float32Array;\n      const imagValues =\n          this.readSync(complexTensorInfos.imag.dataId) as Float32Array;\n      return backend_util.mergeRealAndImagArrays(realValues, imagValues);\n    }\n\n    return this.data.get(dataId).values;\n  }\n\n  bufferSync<R extends Rank, D extends DataType>(t: TensorInfo):\n      TensorBuffer<R, D> {\n    const data = this.readSync(t.dataId);\n    if (t.dtype === 'string') {\n      try {\n        // Decode the bytes into string.\n        const strings = (data as Uint8Array[]).map(d => util.decodeString(d));\n        return buffer(t.shape as ShapeMap[R], t.dtype, strings) as\n            TensorBuffer<R, D>;\n      } catch {\n        throw new Error('Failed to decode encoded string bytes into utf-8');\n      }\n    }\n    return buffer(t.shape as ShapeMap[R], t.dtype, data as TypedArray) as\n        TensorBuffer<R, D>;\n  }\n\n  makeOutput<T extends Tensor>(\n      values: backend_util.BackendValues, shape: number[], dtype: DataType): T {\n    return engine().makeTensorFromTensorInfo(\n               this.makeTensorInfo(shape, dtype, values), this) as T;\n  }\n\n  /**\n   * Dispose the memory if the dataId has 0 refCount. Return true if the memory\n   * is released or memory is not managed in this backend, false if memory is\n   * not cleared.\n   * @param dataId\n   * @oaram force Optional, remove the data regardless of refCount\n   */\n  disposeData(dataId: DataId, force = false): boolean {\n    if (this.data.has(dataId)) {\n      this.data.get(dataId).refCount--;\n      if (!force && this.data.get(dataId).refCount > 0) {\n        return false;\n      }\n\n      const {complexTensorInfos} = this.data.get(dataId);\n\n      if (complexTensorInfos != null) {\n        this.disposeData(complexTensorInfos.real.dataId, true);\n        this.disposeData(complexTensorInfos.imag.dataId, true);\n      }\n\n      this.data.delete(dataId);\n    }\n    return true;\n  }\n\n  disposeIntermediateTensorInfo(tensorInfo: TensorInfo): void {\n    this.disposeData(tensorInfo.dataId);\n  }\n\n  async time(f: () => void): Promise<BackendTimingInfo> {\n    const start = util.now();\n    f();\n    const kernelMs = util.now() - start;\n    return {kernelMs};\n  }\n\n  memory() {\n    return {\n      // Unreliable due to automatic gc. The numbers above are cumulative.\n      unreliable: true,\n      reasons:\n          ['The reported memory is an upper bound. Due to automatic garbage ' +\n           'collection, the true allocated memory may be less.']\n    };\n  }\n\n  where(condition: Tensor): Tensor2D {\n    assertNotComplex([condition], 'where');\n\n    const condVals = this.readSync(condition.dataId) as TypedArray;\n    return whereImpl(condition.shape, condVals);\n  }\n\n  dispose() {}\n\n  floatPrecision(): 16|32 {\n    return 32;\n  }\n\n  /** Returns the smallest representable number.  */\n  epsilon(): number {\n    return super.epsilon();\n  }\n}\n", "/** @license See the LICENSE file. */\n\n// This code is auto-generated, do not modify this file!\nconst version = '3.21.0';\nexport {version};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n/*\n * base.ts contains all the exports from tfjs-backend-cpu\n * without auto-kernel registration\n */\nimport {registerBackend} from '@tensorflow/tfjs-core';\nimport {MathBackendCPU} from './backend_cpu';\nimport * as shared from './shared';\n\nexport {MathBackendCPU} from './backend_cpu';\nexport {version as version_cpu} from './version';\nexport {shared};\n\n// Side effects for default initialization of MathBackendCPU\nregisterBackend('cpu', () => new MathBackendCPU(), 1 /* priority */);\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Elu, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const elu =\n    unaryKernelFunc(Elu, (xi) => xi >= 0 ? xi : (Math.exp(xi) - 1));\n\nexport const eluConfig: KernelConfig = {\n  kernelName: Elu,\n  backendName: 'cpu',\n  kernelFunc: elu,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, LeakyRelu, LeakyReluAttrs, LeakyReluInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function leakyRelu(args: {\n  inputs: LeakyReluInputs,\n  backend: MathBackendCPU,\n  attrs: LeakyReluAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {alpha} = attrs;\n\n  assertNotComplex([x], 'leakyRelu');\n\n  const xSize = util.sizeFromShape(x.shape);\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const outVals = util.getTypedArrayFromDType('float32', xSize);\n\n  for (let i = 0; i < xVals.length; i++) {\n    outVals[i] = xVals[i] < 0 ? alpha * xVals[i] : xVals[i];\n  }\n\n  return backend.makeTensorInfo(x.shape, 'float32', outVals);\n}\n\nexport const leakyReluConfig: KernelConfig = {\n  kernelName: LeakyRelu,\n  backendName: 'cpu',\n  kernelFunc: leakyRelu as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Prelu, PreluInputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\n\nconst preluImpl = createSimpleBinaryKernelImpl(\n    (xValue: number, aValue: number) => xValue < 0 ? aValue * xValue : xValue);\n\nexport function prelu(args: {inputs: PreluInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {x, alpha} = inputs;\n\n  assertNotComplex([x, alpha], 'prelu');\n\n  const aVals = backend.data.get(x.dataId).values as TypedArray;\n  const bVals = backend.data.get(alpha.dataId).values as TypedArray;\n\n  const [resultData, resultShape] =\n      preluImpl(x.shape, alpha.shape, aVals, bVals, 'float32');\n\n  return backend.makeTensorInfo(resultShape, 'float32', resultData);\n}\n\nexport const preluConfig: KernelConfig = {\n  kernelName: Prelu,\n  backendName: 'cpu',\n  kernelFunc: prelu,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Relu} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const relu = unaryKernelFunc(Relu, (xi) => Math.max(0, xi));\n\nexport const reluConfig: KernelConfig = {\n  kernelName: Relu,\n  backendName: 'cpu',\n  kernelFunc: relu,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Relu6} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const relu6 =\n    unaryKernelFunc(Relu6, (xi) => Math.min(Math.max(0, xi), 6));\n\nexport const relu6Config: KernelConfig = {\n  kernelName: Relu6,\n  backendName: 'cpu',\n  kernelFunc: relu6,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {_FusedMatMul, _FusedMatMulAttrs, _FusedMatMulInputs, backend_util, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {elu} from '../kernels/Elu';\nimport {identity} from '../kernels/Identity';\nimport {leakyRelu} from '../kernels/LeakyRelu';\nimport {prelu} from '../kernels/Prelu';\nimport {relu} from '../kernels/Relu';\nimport {relu6} from '../kernels/Relu6';\nimport {sigmoid} from '../kernels/Sigmoid';\n\nexport function applyActivation(\n    backend: MathBackendCPU, x: TensorInfo, activation: backend_util.Activation,\n    preluActivationWeights?: TensorInfo, leakyreluAlpha?: number): TensorInfo {\n  if (activation === 'linear') {\n    return identity({inputs: {x}, backend});\n  } else if (activation === 'relu') {\n    return relu({inputs: {x}, backend}) as TensorInfo;\n  } else if (activation === 'elu') {\n    return elu({inputs: {x}, backend}) as TensorInfo;\n  } else if (activation === 'relu6') {\n    return relu6({inputs: {x}, backend}) as TensorInfo;\n  } else if (activation === 'prelu') {\n    return prelu({inputs: {x, alpha: preluActivationWeights}, backend});\n  } else if (activation === 'leakyrelu') {\n    return leakyRelu({inputs: {x}, backend, attrs: {alpha: leakyreluAlpha}});\n  } else if (activation === 'sigmoid') {\n    return sigmoid({inputs: {x}, backend}) as TensorInfo;\n  }\n  throw new Error(\n      `Activation ${activation} has not been implemented for the CPU backend.`);\n}\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Reshape, ReshapeAttrs, ReshapeInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport function reshape(\n    args:\n        {inputs: ReshapeInputs, backend: MathBackendCPU, attrs: ReshapeAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {shape} = attrs;\n\n  const xSize = util.sizeFromShape(x.shape);\n  const $shape = util.inferFromImplicitShape(shape, xSize);\n  const $xSize = util.sizeFromShape($shape);\n\n  util.assert(\n      xSize === $xSize,\n      () => `The new shape (${$shape}) has ${$xSize} elements and the old ` +\n          `shape (${x.shape}) has ${xSize} elements. The new shape and old ` +\n          `shape must have the same number of elements.`);\n\n  backend.incRef(x.dataId);\n\n  const xData = backend.data.get(x.dataId);\n\n  if (xData.complexTensorInfos != null) {\n    const real = xData.complexTensorInfos.real;\n    const imag = xData.complexTensorInfos.imag;\n\n    real.shape = $shape;\n    imag.shape = $shape;\n  }\n\n  return {dataId: x.dataId, shape: $shape, dtype: x.dtype};\n}\n\nexport const reshapeConfig: KernelConfig = {\n  kernelName: Reshape,\n  backendName: 'cpu',\n  kernelFunc: reshape as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {BatchMatMul, BatchMatMulAttrs, BatchMatMulInputs, broadcast_util, buffer, KernelConfig, KernelFunc, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nimport {reshape} from './Reshape';\n\nexport function batchMatMul(args: {\n  inputs: BatchMatMulInputs,\n  attrs: BatchMatMulAttrs,\n  backend: MathBackendCPU\n}) {\n  const {inputs, backend, attrs} = args;\n  const {a, b} = inputs;\n  const {transposeA, transposeB} = attrs;\n\n  assertNotComplex([a, b], 'matMul');\n\n  const aRank = a.shape.length;\n  const bRank = b.shape.length;\n\n  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n\n  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n\n  const outerDimsA = a.shape.slice(0, -2);\n  const outerDimsB = b.shape.slice(0, -2);\n\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n\n  const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(\n      a.shape.slice(0, -2), b.shape.slice(0, -2));\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n\n  util.assert(\n      innerShapeA === innerShapeB,\n      () => `Error in matMul: inner shapes (${innerShapeA}) and (` +\n          `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +\n          `${b.shape} and transposeA=${transposeA}` +\n          ` and transposeB=${transposeB} must match.`);\n\n  const a3dShape = transposeA ? [batchDimA, innerShapeA, outerShapeA] :\n                                [batchDimA, outerShapeA, innerShapeA];\n  const b3dShape = transposeB ? [batchDimB, outerShapeB, innerShapeB] :\n                                [batchDimB, innerShapeB, outerShapeB];\n\n  // The rest of the implementation is designed to operate on rank-3 tensors\n  const a3d = reshape({inputs: {x: a}, backend, attrs: {shape: a3dShape}});\n  const b3d = reshape({inputs: {x: b}, backend, attrs: {shape: b3dShape}});\n\n  const sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n  const leftDim = transposeA ? a3d.shape[2] : a3d.shape[1];\n  const rightDim = transposeB ? b3d.shape[1] : b3d.shape[2];\n  const batchDim = Math.max(batchDimA, batchDimB);\n\n  const a3dValues = backend.data.get(a3d.dataId).values as TypedArray;\n  const b3dValues = backend.data.get(b3d.dataId).values as TypedArray;\n\n  const a3dStrides = util.computeStrides(a3d.shape);\n  const b3dStrides = util.computeStrides(b3d.shape);\n\n  const [aBatch, aOuterStep, aInnerStep] = transposeA ?\n      [a3dStrides[0], 1, a3dStrides[1]] :\n      [a3dStrides[0], a3dStrides[1], 1];\n  const [bInnerStep, bOuterStep, bBatch] = transposeB ?\n      [1, b3dStrides[1], b3dStrides[0]] :\n      [b3dStrides[1], 1, b3dStrides[0]];\n\n  const size = leftDim * rightDim;\n  const result = buffer([batchDim, leftDim, rightDim], a3d.dtype);\n\n  const resVals = result.values as TypedArray;\n  const blockSize = backend.blockSize;\n\n  for (let bi = 0; bi < batchDim; bi++) {\n    for (let i0 = 0; i0 < leftDim; i0 += blockSize) {\n      for (let j0 = 0; j0 < rightDim; j0 += blockSize) {\n        for (let k0 = 0; k0 < sharedDim; k0 += blockSize) {\n          // for when blockSize doesn't evenly divide the input\n          const iBlock = Math.min(i0 + blockSize, leftDim);\n          const jBlock = Math.min(j0 + blockSize, rightDim);\n          const kBlock = Math.min(k0 + blockSize, sharedDim);\n\n          for (let i = i0; i < iBlock; i++) {\n            for (let j = j0; j < jBlock; j++) {\n              let sum = 0.0;\n\n              for (let k = k0; k < kBlock; k++) {\n                const batchOffsetA = Math.min(bi, batchDimA - 1) * aBatch;\n                const batchOffsetB = Math.min(bi, batchDimB - 1) * bBatch;\n                const aVal =\n                    a3dValues[batchOffsetA + i * aOuterStep + k * aInnerStep];\n                const bVal =\n                    b3dValues[k * bInnerStep + j * bOuterStep + batchOffsetB];\n                sum += aVal * bVal;\n              }\n              resVals[bi * size + (i * rightDim + j)] += sum;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  backend.disposeIntermediateTensorInfo(a3d);\n  backend.disposeIntermediateTensorInfo(b3d);\n\n  // set correct shape on output.\n  return backend.makeTensorInfo(\n      outShape, result.dtype, result.values as TypedArray);\n}\n\nexport const batchMatMulConfig: KernelConfig = {\n  kernelName: BatchMatMul,\n  backendName: 'cpu',\n  kernelFunc: batchMatMul as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {_FusedMatMul, _FusedMatMulAttrs, _FusedMatMulInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {applyActivation} from '../utils/fused_utils';\n\nimport {add} from './Add';\nimport {batchMatMul} from './BatchMatMul';\n\nexport function _fusedMatMul(args: {\n  inputs: _FusedMatMulInputs,\n  attrs: _FusedMatMulAttrs,\n  backend: MathBackendCPU\n}) {\n  const {inputs, backend, attrs} = args;\n  const {a, b, bias, preluActivationWeights} = inputs;\n  const {transposeA, transposeB, activation, leakyreluAlpha} = attrs;\n\n  let current;\n  let addRes;\n  let activationRes;\n\n  const intermediates: TensorInfo[] = [];\n\n  const matMulRes =\n      batchMatMul({inputs: {a, b}, attrs: {transposeA, transposeB}, backend});\n  current = matMulRes;\n\n  if (bias) {\n    addRes = add({inputs: {a: current, b: bias}, backend}) as TensorInfo;\n    intermediates.push(current);\n    current = addRes;\n  }\n  if (activation) {\n    activationRes = applyActivation(\n        backend, current, activation, preluActivationWeights, leakyreluAlpha);\n    intermediates.push(current);\n    current = activationRes;\n  }\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return current;\n}\n\nexport const _fusedMatMulConfig: KernelConfig = {\n  kernelName: _FusedMatMul,\n  backendName: 'cpu',\n  kernelFunc: _fusedMatMul as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Acos, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const acos = unaryKernelFunc(Acos, (xi) => Math.acos(xi));\n\nexport const acosConfig: KernelConfig = {\n  kernelName: Acos,\n  backendName: 'cpu',\n  kernelFunc: acos,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Acosh, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const acosh = unaryKernelFunc(Acosh, (xi) => Math.acosh(xi));\n\nexport const acoshConfig: KernelConfig = {\n  kernelName: Acosh,\n  backendName: 'cpu',\n  kernelFunc: acosh,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {AddN, AddNInputs, buffer, KernelConfig, KernelFunc, Tensor, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function addN(args: {inputs: AddNInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const tensors = inputs as Tensor[];\n\n  assertNotComplex(inputs, 'addN');\n\n  const vals =\n      tensors.map(t => backend.data.get(t.dataId).values as TypedArray);\n  const outBuf = buffer(tensors[0].shape, tensors[0].dtype as 'float32');\n  const outVals = outBuf.values;\n  for (let i = 0; i < tensors.length; i++) {\n    const currVals = vals[i];\n    for (let j = 0; j < outVals.length; j++) {\n      outVals[j] += currVals[j];\n    }\n  }\n\n  return backend.makeTensorInfo(outBuf.shape, outBuf.dtype, outBuf.values);\n}\n\nexport const addNConfig: KernelConfig = {\n  kernelName: AddN,\n  backendName: 'cpu',\n  kernelFunc: addN as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {All, AllAttrs, AllInputs, backend_util, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {reshape} from './Reshape';\nimport {transpose} from './Transpose';\n\nexport function all(\n    args: {inputs: AllInputs, backend: MathBackendCPU, attrs: AllAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, keepDims} = attrs;\n\n  assertNotComplex(x, 'all');\n\n  const origAxes = util.parseAxisParam(axis, x.shape);\n  let axes = origAxes;\n  const permutedAxes = backend_util.getAxesPermutation(axes, x.shape.length);\n  let $x = x;\n  if (permutedAxes != null) {\n    $x = transpose({inputs: {x}, backend, attrs: {perm: permutedAxes}});\n    axes = backend_util.getInnerMostAxes(axes.length, x.shape.length);\n  }\n\n  backend_util.assertAxesAreInnerMostDims('all', axes, $x.shape.length);\n  const [outShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes($x.shape, axes);\n  const reduceSize = util.sizeFromShape(reduceShape);\n  const vals = util.makeZerosTypedArray(util.sizeFromShape(outShape), $x.dtype);\n\n  const aVals = backend.data.get($x.dataId).values as TypedArray;\n  for (let i = 0; i < vals.length; ++i) {\n    const offset = i * reduceSize;\n    let all = aVals[offset];\n    for (let j = 0; j < reduceSize; ++j) {\n      const value = aVals[offset + j];\n      all = all && value;\n    }\n    vals[i] = all;\n  }\n\n  if (permutedAxes != null) {\n    backend.disposeIntermediateTensorInfo($x);\n  }\n\n  const result = backend.makeTensorInfo(outShape, $x.dtype, vals);\n\n  if (keepDims) {\n    const expandedShape = backend_util.expandShapeToKeepDim(outShape, origAxes);\n    const reshapedResult =\n        reshape({inputs: {x: result}, backend, attrs: {shape: expandedShape}});\n\n    backend.disposeIntermediateTensorInfo(result);\n\n    return reshapedResult;\n  }\n\n  return result;\n}\n\nexport const allConfig: KernelConfig = {\n  kernelName: All,\n  backendName: 'cpu',\n  kernelFunc: all as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Any, AnyAttrs, AnyInputs, backend_util, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {reshape} from './Reshape';\nimport {transpose} from './Transpose';\n\nexport function any(\n    args: {inputs: AnyInputs, backend: MathBackendCPU, attrs: AnyAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, keepDims} = attrs;\n\n  assertNotComplex(x, 'any');\n\n  const origAxes = util.parseAxisParam(axis, x.shape);\n  let axes = origAxes;\n  const permutedAxes = backend_util.getAxesPermutation(axes, x.shape.length);\n  let $x = x;\n  if (permutedAxes != null) {\n    $x = transpose({inputs: {x}, backend, attrs: {perm: permutedAxes}});\n    axes = backend_util.getInnerMostAxes(axes.length, x.shape.length);\n  }\n\n  backend_util.assertAxesAreInnerMostDims('any', axes, $x.shape.length);\n  const [outShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes($x.shape, axes);\n  const reduceSize = util.sizeFromShape(reduceShape);\n  const vals = util.makeZerosTypedArray(util.sizeFromShape(outShape), $x.dtype);\n\n  const aVals = backend.data.get($x.dataId).values as TypedArray;\n  for (let i = 0; i < vals.length; ++i) {\n    const offset = i * reduceSize;\n    let anyVal = aVals[offset];\n    for (let j = 0; j < reduceSize; ++j) {\n      const value = aVals[offset + j];\n      anyVal = anyVal || value;\n    }\n    vals[i] = anyVal;\n  }\n\n  if (permutedAxes != null) {\n    backend.disposeIntermediateTensorInfo($x);\n  }\n\n  const result = backend.makeTensorInfo(outShape, $x.dtype, vals);\n\n  if (keepDims) {\n    const expandedShape = backend_util.expandShapeToKeepDim(outShape, origAxes);\n    const reshapedResult =\n        reshape({inputs: {x: result}, backend, attrs: {shape: expandedShape}});\n\n    backend.disposeIntermediateTensorInfo(result);\n\n    return reshapedResult;\n  }\n\n  return result;\n}\n\nexport const anyConfig: KernelConfig = {\n  kernelName: Any,\n  backendName: 'cpu',\n  kernelFunc: any as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ArgMax, ArgMaxAttrs, ArgMaxInputs, backend_util, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {transpose} from './Transpose';\n\nexport function argMax(\n    args: {inputs: ArgMaxInputs, backend: MathBackendCPU, attrs: ArgMaxAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis} = attrs;\n\n  assertNotComplex(x, 'argMax');\n\n  let axes = util.parseAxisParam(axis, x.shape);\n  const permutedAxes = backend_util.getAxesPermutation(axes, x.shape.length);\n  let $x = x;\n  const intermediateTensorInfos = [];\n  if (permutedAxes != null) {\n    $x = transpose({inputs: {x}, backend, attrs: {perm: permutedAxes}});\n    intermediateTensorInfos.push($x);\n    axes = backend_util.getInnerMostAxes(axes.length, $x.shape.length);\n  }\n\n  axes = [axes[0]];\n  backend_util.assertAxesAreInnerMostDims('argMax', axes, $x.shape.length);\n  const [outShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes($x.shape, axes);\n\n  const outSize = util.sizeFromShape(outShape);\n  const vals = util.makeZerosTypedArray(outSize, 'int32');\n  const reduceSize = util.sizeFromShape(reduceShape);\n\n  const aVals = backend.data.get($x.dataId).values as TypedArray;\n  for (let i = 0; i < vals.length; ++i) {\n    const offset = i * reduceSize;\n    let max = aVals[offset];\n    let maxIndex = 0;\n    for (let j = 0; j < reduceSize; ++j) {\n      const value = aVals[offset + j];\n      if (value > max) {\n        max = value;\n        maxIndex = j;\n      }\n    }\n    vals[i] = maxIndex;\n  }\n\n  intermediateTensorInfos.forEach(\n      t => backend.disposeIntermediateTensorInfo(t));\n\n  return backend.makeTensorInfo(outShape, 'int32', vals);\n}\n\nexport const argMaxConfig: KernelConfig = {\n  kernelName: ArgMax,\n  backendName: 'cpu',\n  kernelFunc: argMax as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ArgMin, ArgMinAttrs, ArgMinInputs, backend_util, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {transpose} from './Transpose';\n\nexport function argMin(\n    args: {inputs: ArgMinInputs, backend: MathBackendCPU, attrs: ArgMinAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis} = attrs;\n\n  assertNotComplex(x, 'argMin');\n\n  let axes = util.parseAxisParam(axis, x.shape);\n  const permutedAxes = backend_util.getAxesPermutation(axes, x.shape.length);\n  let $x = x;\n  const intermediateTensorInfos = [];\n  if (permutedAxes != null) {\n    $x = transpose({inputs: {x}, backend, attrs: {perm: permutedAxes}});\n    intermediateTensorInfos.push($x);\n    axes = backend_util.getInnerMostAxes(axes.length, $x.shape.length);\n  }\n\n  axes = [axes[0]];\n  backend_util.assertAxesAreInnerMostDims('argMin', axes, $x.shape.length);\n  const [outShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes($x.shape, axes);\n\n  const outSize = util.sizeFromShape(outShape);\n  const vals = util.makeZerosTypedArray(outSize, 'int32');\n  const reduceSize = util.sizeFromShape(reduceShape);\n\n  const aVals = backend.data.get($x.dataId).values as TypedArray;\n  for (let i = 0; i < vals.length; ++i) {\n    const offset = i * reduceSize;\n    let min = aVals[offset];\n    let minIndex = 0;\n    for (let j = 0; j < reduceSize; ++j) {\n      const value = aVals[offset + j];\n      if (value < min) {\n        min = value;\n        minIndex = j;\n      }\n    }\n    vals[i] = minIndex;\n  }\n\n  intermediateTensorInfos.forEach(\n      t => backend.disposeIntermediateTensorInfo(t));\n\n  return backend.makeTensorInfo(outShape, 'int32', vals);\n}\n\nexport const argMinConfig: KernelConfig = {\n  kernelName: ArgMin,\n  backendName: 'cpu',\n  kernelFunc: argMin as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Asin, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const asin = unaryKernelFunc(Asin, (xi) => Math.asin(xi));\n\nexport const asinConfig: KernelConfig = {\n  kernelName: Asin,\n  backendName: 'cpu',\n  kernelFunc: asin,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Asinh, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const asinh = unaryKernelFunc(Asinh, (xi) => Math.asinh(xi));\n\nexport const asinhConfig: KernelConfig = {\n  kernelName: Asinh,\n  backendName: 'cpu',\n  kernelFunc: asinh,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Atan, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const atan = unaryKernelFunc(Atan, (xi) => Math.atan(xi));\n\nexport const atanConfig: KernelConfig = {\n  kernelName: Atan,\n  backendName: 'cpu',\n  kernelFunc: atan,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Atan2, KernelConfig} from '@tensorflow/tfjs-core';\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const atan2Impl = createSimpleBinaryKernelImpl(\n    (aValue, bValue) => Math.atan2(aValue as number, bValue as number));\n\nexport const atan2 = binaryKernelFunc(Atan2, atan2Impl);\n\nexport const atan2Config: KernelConfig = {\n  kernelName: Atan2,\n  backendName: 'cpu',\n  kernelFunc: atan2,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Atanh, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const atanh = unaryKernelFunc(Atanh, (xi) => Math.atanh(xi));\n\nexport const atanhConfig: KernelConfig = {\n  kernelName: Atanh,\n  backendName: 'cpu',\n  kernelFunc: atanh,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, buffer, DataType, Rank, TensorBuffer, TypedArray} from '@tensorflow/tfjs-core';\n\nexport function pool(\n    xValues: TypedArray, xShape: number[], dtype: DataType, strides: number[],\n    convInfo: backend_util.Conv2DInfo,\n    poolType: 'max'|'avg'): TensorBuffer<Rank, DataType> {\n  const strideHeight = convInfo.strideHeight;\n  const strideWidth = convInfo.strideWidth;\n  const dilationHeight = convInfo.dilationHeight;\n  const dilationWidth = convInfo.dilationWidth;\n  const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n  const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n  const padTop = convInfo.padInfo.top;\n  const padLeft = convInfo.padInfo.left;\n\n  const initialValue =\n      (poolType === 'max' ? Number.NEGATIVE_INFINITY :\n                            Number.POSITIVE_INFINITY);\n\n  const output = buffer(convInfo.outShape, dtype);\n  const outputVals = output.values;\n\n  const outputBatchStrides =\n      convInfo.outShape[1] * convInfo.outShape[2] * convInfo.outShape[3];\n  const outputRowStrides = convInfo.outShape[2] * convInfo.outShape[3];\n  const outputColStrides = convInfo.outShape[3];\n\n  for (let b = 0; b < convInfo.batchSize; ++b) {\n    const outputBatchOffset = b * outputBatchStrides;\n    const inputBatchOffset = b * strides[0];\n    for (let d = 0; d < convInfo.inChannels; ++d) {\n      for (let yR = 0; yR < convInfo.outHeight; ++yR) {\n        const xRCorner = yR * strideHeight - padTop;\n        const xRMin = Math.max(0, xRCorner);\n        const xRMax =\n            Math.min(convInfo.inHeight, effectiveFilterHeight + xRCorner);\n        const outputRowOffset = outputBatchOffset + yR * outputRowStrides;\n        for (let yC = 0; yC < convInfo.outWidth; ++yC) {\n          const xCCorner = yC * strideWidth - padLeft;\n          const xCMin = Math.max(0, xCCorner);\n          const xCMax =\n              Math.min(convInfo.inWidth, effectiveFilterWidth + xCCorner);\n          let minMaxValue = initialValue;\n          let avgValue = 0;\n          let count = 0;\n          for (let xR = xRMin; xR < xRMax; xR += dilationHeight) {\n            const xROffset = inputBatchOffset + xR * strides[1];\n            for (let xC = xCMin; xC < xCMax; xC += dilationWidth) {\n              const xCOffset = xROffset + xC * strides[2];\n              const pixel = xValues[xCOffset + d];\n              if ((poolType === 'max' && pixel > minMaxValue)) {\n                minMaxValue = pixel;\n              } else if (poolType === 'avg') {\n                avgValue += pixel;\n                count++;\n              }\n            }\n            if (isNaN(minMaxValue)) {\n              break;\n            }\n          }\n          const outputOffset = outputRowOffset + yC * outputColStrides + d;\n          outputVals[outputOffset] =\n              poolType === 'avg' ? avgValue / count : minMaxValue;\n        }\n      }\n    }\n  }\n  return output;\n}\n\nexport function maxPoolPositions(\n    xValues: TypedArray, xShape: number[], dtype: DataType,\n    convInfo: backend_util.Conv2DInfo, flattenPositions = false,\n    includeBatchInIndex = false): TensorBuffer<Rank, 'int32'> {\n  const maxPositions = buffer(convInfo.outShape, 'int32');\n  const strideHeight = convInfo.strideHeight;\n  const strideWidth = convInfo.strideWidth;\n  const dilationHeight = convInfo.dilationHeight;\n  const dilationWidth = convInfo.dilationWidth;\n  const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n  const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n  const padTop = convInfo.padInfo.top;\n  const padLeft = convInfo.padInfo.left;\n\n  const xBuf = buffer(xShape, dtype, xValues);\n  for (let b = 0; b < convInfo.batchSize; ++b) {\n    for (let d = 0; d < convInfo.inChannels; ++d) {\n      for (let yR = 0; yR < convInfo.outHeight; ++yR) {\n        const xRCorner = yR * strideHeight - padTop;\n        let xRMin = xRCorner;\n        while (xRMin < 0) {\n          xRMin += dilationHeight;\n        }\n        // const xRMin = Math.max(0, xRCorner);\n        const xRMax =\n            Math.min(convInfo.inHeight, effectiveFilterHeight + xRCorner);\n        for (let yC = 0; yC < convInfo.outWidth; ++yC) {\n          const xCCorner = yC * strideWidth - padLeft;\n          let xCMin = xCCorner;\n          while (xCMin < 0) {\n            xCMin += dilationWidth;\n          }\n          const xCMax =\n              Math.min(convInfo.inWidth, effectiveFilterWidth + xCCorner);\n          let maxValue = Number.NEGATIVE_INFINITY;\n          let maxPosition = -1;\n\n          for (let xR = xRMin; xR < xRMax; xR += dilationHeight) {\n            const wR = xR - xRCorner;\n            for (let xC = xCMin; xC < xCMax; xC += dilationWidth) {\n              const wC = xC - xCCorner;\n              const pixel = xBuf.get(b, xR, xC, d);\n              if (pixel > maxValue) {\n                maxValue = pixel as number;\n                if (flattenPositions) {\n                  maxPosition = includeBatchInIndex ?\n                      ((b * convInfo.inHeight + xR) * convInfo.inWidth + xC) *\n                              convInfo.inChannels +\n                          d :\n                      (xR * convInfo.inWidth + xC) * convInfo.inChannels + d;\n                } else {\n                  maxPosition = wR * effectiveFilterWidth + wC;\n                }\n              }\n            }\n          }\n          maxPositions.set(maxPosition, b, yR, yC, d);\n        }\n      }\n    }\n  }\n  return maxPositions;\n}\n\nexport function pool3d(\n    xValues: TypedArray, xShape: number[], dtype: DataType, strides: number[],\n    convInfo: backend_util.Conv3DInfo,\n    poolType: 'max'|'avg'): TensorBuffer<Rank, DataType> {\n  const strideDepth = convInfo.strideDepth;\n  const strideHeight = convInfo.strideHeight;\n  const strideWidth = convInfo.strideWidth;\n  const dilationDepth = convInfo.dilationDepth;\n  const dilationHeight = convInfo.dilationHeight;\n  const dilationWidth = convInfo.dilationWidth;\n  const effectiveFilterDepth = convInfo.effectiveFilterDepth;\n  const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n  const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n  const padFront = convInfo.padInfo.front;\n  const padTop = convInfo.padInfo.top;\n  const padLeft = convInfo.padInfo.left;\n\n  const initialValue =\n      (poolType === 'max' ? Number.NEGATIVE_INFINITY :\n                            Number.POSITIVE_INFINITY);\n\n  const output = buffer(convInfo.outShape, dtype);\n  const outputVals = output.values;\n\n  const outputBatchStrides = convInfo.outShape[1] * convInfo.outShape[2] *\n      convInfo.outShape[3] * convInfo.outShape[4];\n  const outputDepthStrides =\n      convInfo.outShape[2] * convInfo.outShape[3] * convInfo.outShape[4];\n  const outputRowStrides = convInfo.outShape[3] * convInfo.outShape[4];\n  const outputColStrides = convInfo.outShape[4];\n\n  for (let batch = 0; batch < convInfo.batchSize; ++batch) {\n    const outputBatchOffset = batch * outputBatchStrides;\n    const inputBatchOffset = batch * strides[0];\n    for (let channel = 0; channel < convInfo.inChannels; ++channel) {\n      for (let yDepth = 0; yDepth < convInfo.outDepth; ++yDepth) {\n        const xDepthCorner = yDepth * strideDepth - padFront;\n        let xDepthMin = xDepthCorner;\n        while (xDepthMin < 0) {\n          xDepthMin += dilationDepth;\n        }\n        const xDepthMax =\n            Math.min(convInfo.inDepth, effectiveFilterDepth + xDepthCorner);\n        const outputDepthOffset =\n            outputBatchOffset + yDepth * outputDepthStrides;\n        for (let yRow = 0; yRow < convInfo.outHeight; ++yRow) {\n          const xRowCorner = yRow * strideHeight - padTop;\n          let xRowMin = xRowCorner;\n          while (xRowMin < 0) {\n            xRowMin += dilationHeight;\n          }\n          const xRowMax =\n              Math.min(convInfo.inHeight, effectiveFilterHeight + xRowCorner);\n          const outputRowOffset = outputDepthOffset + yRow * outputRowStrides;\n          for (let yCol = 0; yCol < convInfo.outWidth; ++yCol) {\n            const xColCorner = yCol * strideWidth - padLeft;\n            let xColMin = xColCorner;\n            while (xColMin < 0) {\n              xColMin += dilationWidth;\n            }\n            const xColMax =\n                Math.min(convInfo.inWidth, effectiveFilterWidth + xColCorner);\n            // Shader code begins\n            const outputColOffset = outputRowOffset + yCol * outputColStrides;\n            let minMaxValue = initialValue;\n            let avgValue = 0;\n            let count = 0;\n            for (let xDepth = xDepthMin; xDepth < xDepthMax;\n                 xDepth += dilationDepth) {\n              const xDepthOffset = inputBatchOffset + xDepth * strides[1];\n              for (let xRow = xRowMin; xRow < xRowMax; xRow += dilationHeight) {\n                const xRowOffset = xDepthOffset + xRow * strides[2];\n                for (let xCol = xColMin; xCol < xColMax;\n                     xCol += dilationWidth) {\n                  const xColOffset = xRowOffset + xCol * strides[3];\n                  const pixel = xValues[xColOffset + channel];\n                  if ((poolType === 'max' && pixel > minMaxValue)) {\n                    minMaxValue = pixel;\n                  } else if (poolType === 'avg') {\n                    avgValue += pixel;\n                    count++;\n                  }\n                  if (isNaN(minMaxValue)) {\n                    break;\n                  }\n                }\n                if (isNaN(minMaxValue)) {\n                  break;\n                }\n              }\n              if (isNaN(minMaxValue)) {\n                break;\n              }\n            }\n            const outputOffset = outputColOffset + channel;\n            outputVals[outputOffset] =\n                poolType === 'avg' ? avgValue / count : minMaxValue;\n          }\n        }\n      }\n    }\n  }\n\n  return output;\n}\n\nexport function maxPool3dPositions(\n    xBuf: TensorBuffer<Rank, DataType>,\n    convInfo: backend_util.Conv3DInfo): TensorBuffer<Rank, DataType> {\n  const maxPositions = buffer(convInfo.outShape, 'int32');\n  const strideDepth = convInfo.strideDepth;\n  const strideHeight = convInfo.strideHeight;\n  const strideWidth = convInfo.strideWidth;\n  const dilationDepth = convInfo.dilationDepth;\n  const dilationHeight = convInfo.dilationHeight;\n  const dilationWidth = convInfo.dilationWidth;\n  const effectiveFilterDepth = convInfo.effectiveFilterDepth;\n  const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n  const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n  const padFront = convInfo.padInfo.front;\n  const padTop = convInfo.padInfo.top;\n  const padLeft = convInfo.padInfo.left;\n\n  for (let batch = 0; batch < convInfo.batchSize; ++batch) {\n    for (let channel = 0; channel < convInfo.inChannels; ++channel) {\n      for (let yDepth = 0; yDepth < convInfo.outDepth; ++yDepth) {\n        const xDepthCorner = yDepth * strideDepth - padFront;\n        let xDepthMin = xDepthCorner;\n        while (xDepthMin < 0) {\n          xDepthMin += dilationDepth;\n        }\n        const xDepthMax =\n            Math.min(convInfo.inDepth, effectiveFilterDepth + xDepthCorner);\n        for (let yRow = 0; yRow < convInfo.outHeight; ++yRow) {\n          const xRowCorner = yRow * strideHeight - padTop;\n          let xRowMin = xRowCorner;\n          while (xRowMin < 0) {\n            xRowMin += dilationHeight;\n          }\n          const xRowMax =\n              Math.min(convInfo.inHeight, effectiveFilterHeight + xRowCorner);\n          for (let yCol = 0; yCol < convInfo.outWidth; ++yCol) {\n            const xColCorner = yCol * strideWidth - padLeft;\n            let xColMin = xColCorner;\n            while (xColMin < 0) {\n              xColMin += dilationWidth;\n            }\n            const xColMax =\n                Math.min(convInfo.inWidth, effectiveFilterWidth + xColCorner);\n\n            // Shader code begins\n            let maxValue = Number.NEGATIVE_INFINITY;\n            let maxPosition = -1;\n\n            for (let xDepth = xDepthMin; xDepth < xDepthMax;\n                 xDepth += dilationDepth) {\n              const wDepth = xDepth - xDepthCorner;\n              for (let xRow = xRowMin; xRow < xRowMax; xRow += dilationHeight) {\n                const wRow = xRow - xRowCorner;\n                for (let xCol = xColMin; xCol < xColMax;\n                     xCol += dilationWidth) {\n                  const wCol = xCol - xColCorner;\n                  const pixel = xBuf.get(batch, xDepth, xRow, xCol, channel);\n                  if (pixel >= maxValue) {\n                    maxValue = pixel as number;\n                    maxPosition =\n                        wDepth * effectiveFilterHeight * effectiveFilterWidth +\n                        wRow * effectiveFilterHeight + wCol;\n                  }\n                }\n              }\n            }\n\n            maxPositions.set(maxPosition, batch, yDepth, yRow, yCol, channel);\n          }\n        }\n      }\n    }\n  }\n\n  return maxPositions;\n}\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {AvgPool, AvgPoolAttrs, AvgPoolInputs, backend_util, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {pool} from '../utils/pool_utils';\nimport {identity} from './Identity';\n\nexport function avgPool(\n    args:\n        {inputs: AvgPoolInputs, backend: MathBackendCPU, attrs: AvgPoolAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  assertNotComplex(x, 'avgPool');\n  const {filterSize, strides, pad, dimRoundingMode} = attrs;\n  const dilations = 1;\n\n  util.assert(\n      backend_util.eitherStridesOrDilationsAreOne(strides, dilations),\n      () => 'Error in avgPool: Either strides or dilations must be 1. ' +\n          `Got strides ${strides} and dilations '${dilations}'`);\n\n  const convInfo = backend_util.computePool2DInfo(\n      x.shape as [number, number, number, number], filterSize, strides,\n      dilations, pad, dimRoundingMode);\n  let res: TensorInfo;\n\n  if (convInfo.filterWidth === 1 && convInfo.filterHeight === 1 &&\n      util.arraysEqual(convInfo.inShape, convInfo.outShape)) {\n    res = identity({inputs: {x}, backend});\n  } else {\n    const xValues = backend.data.get(x.dataId).values as TypedArray;\n    const strides = util.computeStrides(x.shape);\n    const buffer = pool(xValues, x.shape, x.dtype, strides, convInfo, 'avg');\n    res = backend.makeTensorInfo(\n        convInfo.outShape, x.dtype, buffer.values as TypedArray);\n  }\n  return res;\n}\n\nexport const avgPoolConfig: KernelConfig = {\n  kernelName: AvgPool,\n  backendName: 'cpu',\n  kernelFunc: avgPool as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {AvgPool3D, AvgPool3DAttrs, AvgPool3DInputs, backend_util, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {pool3d} from '../utils/pool_utils';\n\nexport function avgPool3D(args: {\n  inputs: AvgPool3DInputs,\n  backend: MathBackendCPU,\n  attrs: AvgPool3DAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {filterSize, strides, pad, dimRoundingMode, dataFormat} = attrs;\n\n  assertNotComplex(x, 'avgPool3d');\n\n  const convInfo = backend_util.computePool3DInfo(\n      x.shape as [number, number, number, number, number], filterSize, strides,\n      1 /* dilations */, pad, dimRoundingMode, dataFormat);\n\n  const xValues = backend.data.get(x.dataId).values as TypedArray;\n  const outBuf = pool3d(\n      xValues, x.shape, x.dtype, util.computeStrides(x.shape), convInfo, 'avg');\n\n  return backend.makeTensorInfo(outBuf.shape, 'float32', outBuf.values);\n}\n\nexport const avgPool3DConfig: KernelConfig = {\n  kernelName: AvgPool3D,\n  backendName: 'cpu',\n  kernelFunc: avgPool3D as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {AvgPool3DGrad, AvgPool3DGradAttrs, AvgPool3DGradInputs, backend_util, buffer, KernelConfig, KernelFunc, Rank, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function avgPool3DGrad(args: {\n  inputs: AvgPool3DGradInputs,\n  backend: MathBackendCPU,\n  attrs: AvgPool3DGradAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {dy, input} = inputs;\n  const {filterSize, strides, pad, dimRoundingMode} = attrs;\n\n  assertNotComplex([dy, input], 'avgPool3DGrad');\n\n  const convInfo = backend_util.computePool3DInfo(\n      input.shape as [number, number, number, number, number], filterSize,\n      strides, 1 /* dilations */, pad, dimRoundingMode);\n\n  const strideDepth = convInfo.strideDepth;\n  const strideHeight = convInfo.strideHeight;\n  const strideWidth = convInfo.strideWidth;\n  const filterDepth = convInfo.filterDepth;\n  const filterHeight = convInfo.filterHeight;\n  const filterWidth = convInfo.filterWidth;\n  const dilationDepth = convInfo.dilationDepth;\n  const dilationHeight = convInfo.dilationHeight;\n  const dilationWidth = convInfo.dilationWidth;\n  const effectiveFilterDepth = convInfo.effectiveFilterDepth;\n  const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n  const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n  const padFront = effectiveFilterDepth - 1 - convInfo.padInfo.front;\n  const padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n  const padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n  const dx = buffer(input.shape, 'float32');\n\n  const avgMultiplier = 1 / (filterDepth * filterHeight * filterWidth);\n\n  const dyBuf = backend.bufferSync<Rank, 'float32'>(dy);\n\n  for (let batch = 0; batch < convInfo.batchSize; ++batch) {\n    for (let channel = 0; channel < convInfo.inChannels; ++channel) {\n      for (let dxDepth = 0; dxDepth < convInfo.inDepth; ++dxDepth) {\n        for (let dxRow = 0; dxRow < convInfo.inHeight; ++dxRow) {\n          for (let dxCol = 0; dxCol < convInfo.inWidth; ++dxCol) {\n            // Shader code begins.\n            const dyDepthCorner = dxDepth - padFront;\n            const dyRowCorner = dxRow - padTop;\n            const dyColCorner = dxCol - padLeft;\n            let dotProd = 0;\n            for (let wDepth = 0; wDepth < effectiveFilterDepth;\n                 wDepth += dilationDepth) {\n              const dyDepth = (dyDepthCorner + wDepth) / strideDepth;\n              if (dyDepth < 0 || dyDepth >= convInfo.outDepth ||\n                  Math.floor(dyDepth) !== dyDepth) {\n                continue;\n              }\n              for (let wRow = 0; wRow < effectiveFilterHeight;\n                   wRow += dilationHeight) {\n                const dyRow = (dyRowCorner + wRow) / strideHeight;\n                if (dyRow < 0 || dyRow >= convInfo.outHeight ||\n                    Math.floor(dyRow) !== dyRow) {\n                  continue;\n                }\n                for (let wCol = 0; wCol < effectiveFilterWidth;\n                     wCol += dilationWidth) {\n                  const dyCol = (dyColCorner + wCol) / strideWidth;\n                  if (dyCol < 0 || dyCol >= convInfo.outWidth ||\n                      Math.floor(dyCol) !== dyCol) {\n                    continue;\n                  }\n\n                  const pixel =\n                      dyBuf.get(batch, dyDepth, dyRow, dyCol, channel);\n                  dotProd += pixel;\n                }\n              }\n            }\n            dx.set(\n                dotProd * avgMultiplier, batch, dxDepth, dxRow, dxCol, channel);\n          }\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(dx.shape, dx.dtype, dx.values);\n}\n\nexport const avgPool3DGradConfig: KernelConfig = {\n  kernelName: AvgPool3DGrad,\n  backendName: 'cpu',\n  kernelFunc: avgPool3DGrad as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {AvgPoolGrad, AvgPoolGradAttrs, AvgPoolGradInputs, backend_util, buffer, KernelConfig, KernelFunc, Rank, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function avgPoolGrad(args: {\n  inputs: AvgPoolGradInputs,\n  backend: MathBackendCPU,\n  attrs: AvgPoolGradAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {dy, input} = inputs;\n  const x = input;\n  assertNotComplex([dy, input], 'avgPoolGrad');\n  const {filterSize, strides, pad} = attrs;\n\n  const convInfo = backend_util.computePool2DInfo(\n      x.shape as [number, number, number, number], filterSize, strides,\n      1 /* dilations */, pad);\n  const strideHeight = convInfo.strideHeight;\n  const strideWidth = convInfo.strideWidth;\n  const filterHeight = convInfo.filterHeight;\n  const filterWidth = convInfo.filterWidth;\n  const dilationHeight = convInfo.dilationHeight;\n  const dilationWidth = convInfo.dilationWidth;\n  const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n  const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n  const padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n  const padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n  const dx =\n      buffer<Rank.R4>(x.shape as [number, number, number, number], 'float32');\n\n  const avgMultiplier = 1 / (filterHeight * filterWidth);\n\n  const dyData = backend.data.get(dy.dataId).values as Float32Array;\n  const dyBuf = buffer<Rank.R4>(\n      dy.shape as [number, number, number, number], 'float32', dyData);\n\n  for (let b = 0; b < convInfo.batchSize; ++b) {\n    for (let d = 0; d < convInfo.inChannels; ++d) {\n      for (let dxR = 0; dxR < convInfo.inHeight; ++dxR) {\n        for (let dxC = 0; dxC < convInfo.inWidth; ++dxC) {\n          // Shader code begins.\n          const dyRCorner = dxR - padTop;\n          const dyCCorner = dxC - padLeft;\n          let dotProd = 0;\n          for (let wR = 0; wR < effectiveFilterHeight; wR += dilationHeight) {\n            const dyR = (dyRCorner + wR) / strideHeight;\n            if (dyR < 0 || dyR >= convInfo.outHeight ||\n                Math.floor(dyR) !== dyR) {\n              continue;\n            }\n            for (let wC = 0; wC < effectiveFilterWidth; wC += dilationWidth) {\n              const dyC = (dyCCorner + wC) / strideWidth;\n              if (dyC < 0 || dyC >= convInfo.outWidth ||\n                  Math.floor(dyC) !== dyC) {\n                continue;\n              }\n\n              const pixel = dyBuf.get(b, dyR, dyC, d);\n              dotProd += pixel;\n            }\n          }\n          dx.set(dotProd * avgMultiplier, b, dxR, dxC, d);\n        }\n      }\n    }\n  }\n  return backend.makeTensorInfo(dx.shape, dx.dtype, dx.values);\n}\n\nexport const avgPoolGradConfig: KernelConfig = {\n  kernelName: AvgPoolGrad,\n  backendName: 'cpu',\n  kernelFunc: avgPoolGrad as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {FusedBatchNorm, FusedBatchNormAttrs, FusedBatchNormInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function batchNorm(args: {\n  inputs: FusedBatchNormInputs,\n  backend: MathBackendCPU,\n  attrs: FusedBatchNormAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, scale, offset, mean, variance} = inputs;\n\n  util.assert(\n      mean.shape.length === variance.shape.length,\n      () => 'Batch normalization gradient requires mean and variance to have ' +\n          'equal ranks.');\n  util.assert(\n      offset == null || mean.shape.length === offset.shape.length,\n      () => 'Batch normalization gradient requires mean and offset to have ' +\n          'equal ranks.');\n  util.assert(\n      scale == null || mean.shape.length === scale.shape.length,\n      () => 'Batch normalization gradient requires mean and scale to have ' +\n          'equal ranks.');\n\n  assertNotComplex([x, mean, variance, scale, offset], 'batchNorm');\n\n  let {varianceEpsilon} = attrs;\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const mVals = backend.data.get(mean.dataId).values as TypedArray;\n  const varVals = backend.data.get(variance.dataId).values as TypedArray;\n  const sVals = scale ? backend.data.get(scale.dataId).values as TypedArray :\n                        new Float32Array([1]);\n  const offVals = offset ?\n      backend.data.get(offset.dataId).values as TypedArray :\n      new Float32Array([0]);\n  const outVals = new Float32Array(xVals.length);\n\n  const offValsLength = offVals.length;\n  const sValsLength = sVals.length;\n  const varValsLength = varVals.length;\n  const mValsLength = mVals.length;\n\n  let offi = 0;\n  let mi = 0;\n  let si = 0;\n  let vi = 0;\n  for (let i = 0; i < xVals.length; ++i) {\n    outVals[i] = offVals[offi++] +\n        (xVals[i] - mVals[mi++]) * sVals[si++] /\n            Math.sqrt(varVals[vi++] + varianceEpsilon);\n    if (offi >= offValsLength) {\n      offi = 0;\n    }\n    if (mi >= mValsLength) {\n      mi = 0;\n    }\n    if (si >= sValsLength) {\n      si = 0;\n    }\n    if (vi >= varValsLength) {\n      vi = 0;\n    }\n  }\n  return backend.makeTensorInfo(x.shape, x.dtype, outVals);\n}\n\nexport const batchNormConfig: KernelConfig = {\n  kernelName: FusedBatchNorm,\n  backendName: 'cpu',\n  kernelFunc: batchNorm as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, BatchToSpaceND, BatchToSpaceNDAttrs, BatchToSpaceNDInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {reshape} from './Reshape';\nimport {slice} from './Slice';\nimport {transpose} from './Transpose';\n\nexport function batchToSpaceND(args: {\n  inputs: BatchToSpaceNDInputs,\n  backend: MathBackendCPU,\n  attrs: BatchToSpaceNDAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {blockShape, crops} = attrs;\n\n  assertNotComplex([x], 'batchToSpaceND');\n\n  const prod = blockShape.reduce((a, b) => a * b);\n\n  const reshaped = backend_util.getReshaped(x.shape, blockShape, prod);\n  const permuted = backend_util.getPermuted(reshaped.length, blockShape.length);\n  const reshapedPermuted =\n      backend_util.getReshapedPermuted(x.shape, blockShape, prod);\n  const sliceBeginCoords =\n      backend_util.getSliceBeginCoords(crops, blockShape.length);\n  const sliceSize =\n      backend_util.getSliceSize(reshapedPermuted, crops, blockShape.length);\n\n  const xReshaped = reshape({inputs: {x}, backend, attrs: {shape: reshaped}});\n  const xTransposed =\n      transpose({inputs: {x: xReshaped}, backend, attrs: {perm: permuted}});\n  const xTransposedReshaped = reshape(\n      {inputs: {x: xTransposed}, backend, attrs: {shape: reshapedPermuted}});\n  const result = slice({\n    inputs: {x: xTransposedReshaped},\n    backend,\n    attrs: {begin: sliceBeginCoords, size: sliceSize}\n  });\n\n  backend.disposeIntermediateTensorInfo(xReshaped);\n  backend.disposeIntermediateTensorInfo(xTransposed);\n  backend.disposeIntermediateTensorInfo(xTransposedReshaped);\n\n  return result;\n}\n\nexport const batchToSpaceNDConfig: KernelConfig = {\n  kernelName: BatchToSpaceND,\n  backendName: 'cpu',\n  kernelFunc: batchToSpaceND as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Bincount, BincountAttrs, BincountInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {bincountImpl} from './Bincount_impl';\n\nexport function bincount(args: {\n  inputs: BincountInputs,\n  backend: MathBackendCPU,\n  attrs: BincountAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, weights} = inputs;\n  const {size} = attrs;\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const weightsVals = backend.data.get(weights.dataId).values as TypedArray;\n\n  const outVals =\n      bincountImpl(xVals, weightsVals, weights.dtype, weights.shape, size);\n\n  return backend.makeTensorInfo([size], weights.dtype, outVals);\n}\n\nexport const bincountConfig: KernelConfig = {\n  kernelName: Bincount,\n  backendName: 'cpu',\n  kernelFunc: bincount as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, BroadcastArgs, BroadcastArgsInputs, KernelConfig, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport function broadcastArgs(args: {\n  inputs: BroadcastArgsInputs,\n  backend: MathBackendCPU,\n}): TensorInfo {\n  const {inputs, backend} = args;\n  const {s0, s1} = inputs;\n\n  const s0Vals = backend.data.get(s0.dataId).values as TypedArray;\n  const s1Vals = backend.data.get(s1.dataId).values as TypedArray;\n\n  const broadcastShape = backend_util.assertAndGetBroadcastShape(\n      Array.from(s0Vals), Array.from(s1Vals));\n\n  return backend.makeTensorInfo(\n      [broadcastShape.length], 'int32', Int32Array.from(broadcastShape));\n}\n\nexport const broadcastArgsConfig: KernelConfig = {\n  kernelName: BroadcastArgs,\n  backendName: 'cpu',\n  kernelFunc: broadcastArgs\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ClipByValue, ClipByValueAttrs, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const clipByValue = unaryKernelFunc(ClipByValue, (xi, attrs) => {\n  const clipAttrs = attrs as {} as ClipByValueAttrs;\n  if (xi > clipAttrs.clipValueMax) {\n    return clipAttrs.clipValueMax;\n  }\n  return xi < clipAttrs.clipValueMin ? clipAttrs.clipValueMin : xi;\n});\n\nexport const clipByValueConfig: KernelConfig = {\n  kernelName: ClipByValue,\n  backendName: 'cpu',\n  kernelFunc: clipByValue,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ComplexAbs, ComplexAbsInputs, KernelConfig, KernelFunc, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport const complexAbs =\n    (args: {inputs: ComplexAbsInputs, backend: MathBackendCPU}) => {\n      const {x} = args.inputs;\n      const cpuBackend = args.backend;\n      const resultValues = new Float32Array(util.sizeFromShape(x.shape));\n      const complexVals = cpuBackend.data.get(x.dataId);\n      const real = complexVals.complexTensorInfos.real;\n      const imag = complexVals.complexTensorInfos.imag;\n      const realVals = cpuBackend.data.get(real.dataId).values as Float32Array;\n      const imagVals = cpuBackend.data.get(imag.dataId).values as Float32Array;\n      for (let i = 0; i < realVals.length; i++) {\n        const real = realVals[i];\n        const imag = imagVals[i];\n        resultValues[i] = Math.hypot(real, imag);\n      }\n\n      return cpuBackend.makeOutput(resultValues, x.shape, 'float32');\n    };\n\nexport const complexAbsConfig: KernelConfig = {\n  kernelName: ComplexAbs,\n  backendName: 'cpu',\n  kernelFunc: complexAbs as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Imag, ImagInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport function imag(args: {inputs: ImagInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {input} = inputs;\n\n  const imag = backend.data.get(input.dataId).complexTensorInfos.imag;\n  const imagVal = backend.data.get(imag.dataId).values;\n\n  // When complex tensor is disposed, its underlying parts will be disposed too.\n  // Make new tensor out of the imag value of the complex. This makes sure the\n  // value is still accessible even if complex tensor is disposed.\n  return backend.makeTensorInfo(imag.shape, imag.dtype, imagVal);\n}\n\nexport const imagConfig: KernelConfig = {\n  kernelName: Imag,\n  backendName: 'cpu',\n  kernelFunc: imag as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Concat, ConcatAttrs, ConcatInputs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {complex} from './Complex';\nimport {concatImpl} from './Concat_impl';\nimport {identity} from './Identity';\nimport {imag} from './Imag';\nimport {real} from './Real';\nimport {reshape} from './Reshape';\n\nexport function concat(\n    args: {inputs: ConcatInputs, backend: MathBackendCPU, attrs: ConcatAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {axis} = attrs;\n\n  const $axis = util.parseAxisParam(axis, inputs[0].shape)[0];\n\n  const shapes = inputs.map(t => t.shape);\n  backend_util.assertParamsConsistent(shapes, $axis);\n\n  let outShape = backend_util.computeOutShape(inputs.map(t => t.shape), $axis);\n\n  if (util.sizeFromShape(outShape) === 0) {\n    return backend.makeTensorInfo(outShape, inputs[0].dtype, []);\n  }\n\n  // Keep only non-empty tensors (ignore tensors with 0 in their shape).\n  const $inputs = inputs.filter(t => util.sizeFromShape(t.shape) > 0);\n  if ($inputs.length === 1) {\n    return identity({inputs: {x: $inputs[0]}, backend});\n  }\n\n  if ($inputs[0].dtype === 'complex64') {\n    const reals = $inputs.map((t) => real({inputs: {input: t}, backend}));\n    const imags = $inputs.map((t) => imag({inputs: {input: t}, backend}));\n\n    const realConcated = concat({inputs: reals, backend, attrs: {axis: $axis}});\n    const imagConcated = concat({inputs: imags, backend, attrs: {axis: $axis}});\n\n    const result =\n        complex({inputs: {real: realConcated, imag: imagConcated}, backend});\n\n    reals.forEach(r => backend.disposeIntermediateTensorInfo(r));\n    imags.forEach(i => backend.disposeIntermediateTensorInfo(i));\n    backend.disposeIntermediateTensorInfo(realConcated);\n    backend.disposeIntermediateTensorInfo(imagConcated);\n\n    return result;\n  }\n\n  // Any concat of n-dimensional tensors across any axis can be reduced to\n  // a concatenation of two-dimensional tensors across the axis 1 by first\n  // partitioning the axes of the original tensors into those less than the\n  // axis to be concatenated and the rest. Then reshape the tensors\n  // into a two-dimensional tensor by collapsing these two sets of axes and\n  // concatenate the resulting matrices across the axis 1, finally reshaping\n  // the result to have the proper shape.\n  const inputs2D = $inputs.map(t => {\n    const innerSize = util.sizeFromShape(t.shape.slice($axis));\n    const shape = [-1, innerSize];\n    return reshape({inputs: {x: t}, backend, attrs: {shape}});\n  });\n\n  const inputsValShapes = inputs2D.map(t => {\n    return {vals: backend.data.get(t.dataId).values, shape: t.shape};\n  });\n\n  // Concats 2d tensors along axis=1.\n  outShape =\n      backend_util.computeOutShape(inputs2D.map(t => t.shape), 1 /* axis */);\n  const simplyConcat = inputs2D[0].shape[0] === 1;\n  const outVals =\n      concatImpl(inputsValShapes, outShape, inputs[0].dtype, simplyConcat);\n\n  const finalOutShape =\n      backend_util.computeOutShape($inputs.map(t => t.shape), $axis);\n\n  const outInfo =\n      backend.makeTensorInfo(finalOutShape, inputs[0].dtype, outVals);\n\n  inputs2D.forEach(t => backend.disposeIntermediateTensorInfo(t));\n\n  return outInfo;\n}\n\nexport const concatConfig: KernelConfig = {\n  kernelName: Concat,\n  backendName: 'cpu',\n  kernelFunc: concat as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Conv2D, Conv2DAttrs, Conv2DInputs, KernelConfig, KernelFunc, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function conv2D(\n    args: {inputs: Conv2DInputs, backend: MathBackendCPU, attrs: Conv2DAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, filter} = inputs;\n  const {strides, pad, dataFormat, dilations, dimRoundingMode} = attrs;\n\n  assertNotComplex([x, filter], 'conv2d');\n\n  const $dataFormat = backend_util.convertConv2DDataFormat(dataFormat);\n  const convInfo = backend_util.computeConv2DInfo(\n      x.shape as [number, number, number, number],\n      filter.shape as [number, number, number, number], strides, dilations, pad,\n      dimRoundingMode, false /* depthwise */, $dataFormat);\n\n  const filterHeight = convInfo.filterHeight;\n  const filterWidth = convInfo.filterWidth;\n  const dilationHeight = convInfo.dilationHeight;\n  const dilationWidth = convInfo.dilationWidth;\n  const padLeft = convInfo.padInfo.left;\n  const padTop = convInfo.padInfo.top;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n\n  const y = new TensorBuffer(convInfo.outShape, x.dtype as 'float32');\n\n  const xStrides = util.computeStrides(x.shape);\n  const filterStrides = util.computeStrides(filter.shape);\n\n  const xBatchStride = xStrides[0];\n  const xRowStride = isChannelsLast ? xStrides[1] : xStrides[2];\n  const xColStride = isChannelsLast ? xStrides[2] : 1;\n  const xChannelStride = isChannelsLast ? 1 : xStrides[1];\n  const yBatchStride = y.strides[0];\n  const yRowStride = isChannelsLast ? y.strides[1] : y.strides[2];\n  const yColStride = isChannelsLast ? y.strides[2] : 1;\n  const yChannelStride = isChannelsLast ? 1 : y.strides[1];\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const wVals = backend.data.get(filter.dataId).values as TypedArray;\n  const yVals = y.values;\n\n  for (let b = 0; b < convInfo.batchSize; ++b) {\n    const xOffset1 = b * xBatchStride;\n    const yOffset1 = b * yBatchStride;\n    for (let yR = 0; yR < convInfo.outHeight; ++yR) {\n      const yOffset2 = yOffset1 + yR * yRowStride;\n      const xRCorner = yR * convInfo.strideHeight - padTop;\n      for (let wR = 0; wR < filterHeight; ++wR) {\n        const xR = xRCorner + wR * dilationHeight;\n        if (xR < 0 || xR >= convInfo.inHeight) {\n          continue;\n        }\n        const wOffset1 = wR * filterStrides[0];\n        const xOffset2 = xOffset1 + xR * xRowStride;\n        for (let yC = 0; yC < convInfo.outWidth; ++yC) {\n          const yOffset3 = yOffset2 + yC * yColStride;\n          const xCCorner = yC * convInfo.strideWidth - padLeft;\n          for (let wC = 0; wC < filterWidth; ++wC) {\n            const xC = xCCorner + wC * dilationWidth;\n            if (xC < 0 || xC >= convInfo.inWidth) {\n              continue;\n            }\n            const wOffset2 = wOffset1 + wC * filterStrides[1];\n            const xOffset3 = xOffset2 + xC * xColStride;\n            let wOffset3 = wOffset2;\n            for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n              const xVal = xVals[xOffset3 + d1 * xChannelStride];\n              for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n                yVals[yOffset3 + d2 * yChannelStride] +=\n                    xVal * wVals[wOffset3 + d2];\n              }\n              wOffset3 += convInfo.outChannels;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(y.shape, y.dtype, yVals);\n}\n\nexport const conv2DConfig: KernelConfig = {\n  kernelName: Conv2D,\n  backendName: 'cpu',\n  kernelFunc: conv2D as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Conv2DBackpropFilter, Conv2DBackpropFilterAttrs, Conv2DBackpropFilterInputs, KernelConfig, KernelFunc, TensorBuffer, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function conv2DBackpropFilter(args: {\n  inputs: Conv2DBackpropFilterInputs,\n  backend: MathBackendCPU,\n  attrs: Conv2DBackpropFilterAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, dy} = inputs;\n  const {strides, pad, dataFormat, dimRoundingMode, filterShape} = attrs;\n\n  assertNotComplex([x, dy], 'conv2dBackpropFilter');\n\n  const $dataFormat = backend_util.convertConv2DDataFormat(dataFormat);\n  const convInfo = backend_util.computeConv2DInfo(\n      x.shape as [number, number, number, number], filterShape, strides,\n      1 /* dilations */, pad, dimRoundingMode, false /* depthwise */,\n      $dataFormat);\n\n  const {strideHeight, strideWidth, filterHeight, filterWidth} = convInfo;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const dW = new TensorBuffer(convInfo.filterShape, 'float32');\n\n  const leftPad = convInfo.padInfo.left;\n  const topPad = convInfo.padInfo.top;\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const dyVals = backend.data.get(dy.dataId).values as TypedArray;\n\n  const xBuf = new TensorBuffer(x.shape, x.dtype, xVals);\n  const dyBuf = new TensorBuffer(dy.shape, dy.dtype, dyVals);\n\n  for (let wR = 0; wR < filterHeight; ++wR) {\n    const yRMin = Math.max(0, Math.ceil((topPad - wR) / strideHeight));\n    const yRMax = Math.min(\n        convInfo.outHeight, (convInfo.inHeight + topPad - wR) / strideHeight);\n\n    for (let wC = 0; wC < filterWidth; ++wC) {\n      const yCMin = Math.max(0, Math.ceil((leftPad - wC) / strideWidth));\n      const yCMax = Math.min(\n          convInfo.outWidth, (convInfo.inWidth + leftPad - wC) / strideWidth);\n\n      for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n        for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n          let dotProd = 0;\n          for (let b = 0; b < convInfo.batchSize; ++b) {\n            for (let yR = yRMin; yR < yRMax; ++yR) {\n              const xR = wR + yR * strideHeight - topPad;\n              for (let yC = yCMin; yC < yCMax; ++yC) {\n                const xC = wC + yC * strideWidth - leftPad;\n                if (isChannelsLast) {\n                  dotProd += (xBuf.get(b, xR, xC, d1) as number) *\n                      (dyBuf.get(b, yR, yC, d2) as number);\n                } else {\n                  dotProd += (xBuf.get(b, d1, xR, xC) as number) *\n                      (dyBuf.get(b, d2, yR, yC) as number);\n                }\n              }\n            }\n          }\n          dW.set(dotProd, wR, wC, d1, d2);\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(dW.shape, dW.dtype, dW.values);\n}\n\nexport const conv2DBackpropFilterConfig: KernelConfig = {\n  kernelName: Conv2DBackpropFilter,\n  backendName: 'cpu',\n  kernelFunc: conv2DBackpropFilter as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Conv2DBackpropInput, Conv2DBackpropInputAttrs, Conv2DBackpropInputInputs, KernelConfig, KernelFunc, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function conv2DBackpropInput(args: {\n  inputs: Conv2DBackpropInputInputs,\n  backend: MathBackendCPU,\n  attrs: Conv2DBackpropInputAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {dy, filter} = inputs;\n  const {inputShape, strides, pad, dataFormat, dimRoundingMode} = attrs;\n\n  assertNotComplex([dy, filter], 'conv2dBackpropInput');\n\n  const filterStrides = util.computeStrides(filter.shape);\n  const dyStrides = util.computeStrides(dy.shape);\n\n  let $dataFormat = backend_util.convertConv2DDataFormat(dataFormat);\n  const convInfo = backend_util.computeConv2DInfo(\n      inputShape, filter.shape as [number, number, number, number], strides,\n      1 /* dilations */, pad, dimRoundingMode, false, $dataFormat);\n\n  const dx = new TensorBuffer(convInfo.inShape, 'float32');\n  const dxValues = dx.values;\n  const dyValues = backend.data.get(dy.dataId).values as TypedArray;\n  const fltValues = backend.data.get(filter.dataId).values as TypedArray;\n  const [fltS0, fltS1, fltS2] = filterStrides;\n  const {\n    batchSize,\n    filterHeight,\n    filterWidth,\n    inChannels,\n    inHeight,\n    inWidth,\n    outChannels,\n    outHeight,\n    outWidth,\n    strideHeight,\n    strideWidth\n  } = convInfo;\n  $dataFormat = convInfo.dataFormat;\n  const topPad = filterHeight - 1 - convInfo.padInfo.top;\n  const leftPad = filterWidth - 1 - convInfo.padInfo.left;\n\n  const isChannelsLast = $dataFormat === 'channelsLast';\n  const xBatchStride = dx.strides[0];\n  const xRowStride = isChannelsLast ? dx.strides[1] : dx.strides[2];\n  const xColStride = isChannelsLast ? dx.strides[2] : 1;\n  const xChannelStride = isChannelsLast ? 1 : dx.strides[1];\n  const yBatchStride = dyStrides[0];\n  const yRowStride = isChannelsLast ? dyStrides[1] : dyStrides[2];\n  const yColStride = isChannelsLast ? dyStrides[2] : 1;\n  const yChannelStride = isChannelsLast ? 1 : dyStrides[1];\n\n  for (let b = 0; b < batchSize; ++b) {\n    for (let d1 = 0; d1 < inChannels; ++d1) {\n      for (let xR = 0; xR < inHeight; ++xR) {\n        const xRCorner = xR - topPad;\n        const xRMin = Math.max(0, Math.ceil(xRCorner / strideHeight));\n        const yRMax =\n            Math.min(outHeight, (filterHeight + xRCorner) / strideHeight);\n\n        for (let xC = 0; xC < inWidth; ++xC) {\n          const xCCorner = xC - leftPad;\n          const xCMin = Math.max(0, Math.ceil(xCCorner / strideWidth));\n          const yCMax =\n              Math.min(outWidth, (filterWidth + xCCorner) / strideWidth);\n\n          let dotProd = 0;\n          for (let yR = xRMin; yR < yRMax; ++yR) {\n            const wR = yR * strideHeight - xRCorner;\n\n            for (let yC = xCMin; yC < yCMax; ++yC) {\n              const wC = yC * strideWidth - xCCorner;\n              const dyOffset =\n                  yBatchStride * b + yRowStride * yR + yColStride * yC;\n              const fltOffset = fltS0 * (filterHeight - 1 - wR) +\n                  fltS1 * (filterWidth - 1 - wC) + fltS2 * d1;\n\n              for (let d2 = 0; d2 < outChannels; ++d2) {\n                const pixel = dyValues[dyOffset + yChannelStride * d2];\n                const weight = fltValues[fltOffset + d2];\n                dotProd += pixel * weight;\n              }\n            }\n          }\n          const dxOffset = xBatchStride * b + xRowStride * xR +\n              xColStride * xC + xChannelStride * d1;\n          dxValues[dxOffset] = dotProd;\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(dx.shape, dx.dtype, dx.values);\n}\n\nexport const conv2DBackpropInputConfig: KernelConfig = {\n  kernelName: Conv2DBackpropInput,\n  backendName: 'cpu',\n  kernelFunc: conv2DBackpropInput as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Conv3D, Conv3DAttrs, Conv3DInputs, KernelConfig, KernelFunc, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function conv3D(\n    args: {inputs: Conv3DInputs, backend: MathBackendCPU, attrs: Conv3DAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, filter} = inputs;\n  const {strides, pad, dilations} = attrs;\n\n  assertNotComplex([x, filter], 'conv3d');\n\n  const convInfo = backend_util.computeConv3DInfo(\n      x.shape as [number, number, number, number, number],\n      filter.shape as [number, number, number, number, number], strides,\n      dilations, pad);\n\n  const {\n    filterDepth,\n    filterHeight,\n    filterWidth,\n    dilationDepth,\n    dilationHeight,\n    dilationWidth,\n    padInfo\n  } = convInfo;\n  const padFront = padInfo.front;\n  const padLeft = padInfo.left;\n  const padTop = padInfo.top;\n  const y = new TensorBuffer(convInfo.outShape, x.dtype as 'float32');\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const wVals = backend.data.get(filter.dataId).values as TypedArray;\n  const yVals = y.values;\n\n  const xStrides = util.computeStrides(x.shape);\n  const filterStrides = util.computeStrides(filter.shape);\n\n  for (let b = 0; b < convInfo.batchSize; ++b) {\n    const xOffset1 = b * xStrides[0];\n    const yOffset1 = b * y.strides[0];\n    for (let yF = 0; yF < convInfo.outDepth; ++yF) {\n      const yOffset2 = yOffset1 + yF * y.strides[1];\n      const xFCorner = yF * convInfo.strideDepth - padFront;\n      for (let wF = 0; wF < filterDepth; ++wF) {\n        const xF = xFCorner + wF * dilationDepth;\n        if (xF < 0 || xF >= convInfo.inDepth) {\n          continue;\n        }\n        const wOffset1 = wF * filterStrides[0];\n        const xOffset2 = xOffset1 + xF * xStrides[1];\n\n        for (let yR = 0; yR < convInfo.outHeight; ++yR) {\n          const yOffset3 = yOffset2 + yR * y.strides[2];\n          const xRCorner = yR * convInfo.strideHeight - padTop;\n          for (let wR = 0; wR < filterHeight; ++wR) {\n            const xR = xRCorner + wR * dilationHeight;\n            if (xR < 0 || xR >= convInfo.inHeight) {\n              continue;\n            }\n            const wOffset2 = wOffset1 + wR * filterStrides[1];\n            const xOffset3 = xOffset2 + xR * xStrides[2];\n            for (let yC = 0; yC < convInfo.outWidth; ++yC) {\n              const yOffset4 = yOffset3 + yC * convInfo.outChannels;\n              const xCCorner = yC * convInfo.strideWidth - padLeft;\n              for (let wC = 0; wC < filterWidth; ++wC) {\n                const xC = xCCorner + wC * dilationWidth;\n                if (xC < 0 || xC >= convInfo.inWidth) {\n                  continue;\n                }\n                const wOffset3 = wOffset2 + wC * filterStrides[2];\n                const xOffset4 = xOffset3 + xC * convInfo.inChannels;\n                let wOffset4 = wOffset3;\n                for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n                  const xVal = xVals[xOffset4 + d1];\n                  for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n                    yVals[yOffset4 + d2] += xVal * wVals[wOffset4 + d2];\n                  }\n                  wOffset4 += convInfo.outChannels;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(y.shape, y.dtype, y.values);\n}\n\nexport const conv3DConfig: KernelConfig = {\n  kernelName: Conv3D,\n  backendName: 'cpu',\n  kernelFunc: conv3D as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Conv3DBackpropFilterV2, Conv3DBackpropFilterV2Attrs, Conv3DBackpropFilterV2Inputs, KernelConfig, KernelFunc, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function conv3DBackpropFilterV2(args: {\n  inputs: Conv3DBackpropFilterV2Inputs,\n  backend: MathBackendCPU,\n  attrs: Conv3DBackpropFilterV2Attrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, dy} = inputs;\n  const {strides, pad, filterShape} = attrs;\n\n  assertNotComplex([x, dy], 'conv3dBackpropFilterV2');\n\n  const xStrides = util.computeStrides(x.shape);\n  const dyStrides = util.computeStrides(dy.shape);\n\n  const convInfo = backend_util.computeConv3DInfo(\n      x.shape as [number, number, number, number, number], filterShape, strides,\n      1 /* dilations */, pad);\n\n  const strideDepth = convInfo.strideDepth;\n  const strideHeight = convInfo.strideHeight;\n  const strideWidth = convInfo.strideWidth;\n  const filterDepth = convInfo.filterDepth;\n  const filterHeight = convInfo.filterHeight;\n  const filterWidth = convInfo.filterWidth;\n\n  const dw = new TensorBuffer(convInfo.filterShape, 'float32');\n  const dwValues = dw.values;\n  const [dwS0, dwS1, dwS2, dwS3] = dw.strides;\n  const dyValues = backend.data.get(dy.dataId).values as TypedArray;\n  const [dyS0, dyS1, dyS2, dyS3] = dyStrides;\n  const xValues = backend.data.get(x.dataId).values as TypedArray;\n  const [xS0, xS1, xS2, xS3] = xStrides;\n\n  const frontPad = convInfo.padInfo.front;\n  const leftPad = convInfo.padInfo.left;\n  const topPad = convInfo.padInfo.top;\n\n  for (let wF = 0; wF < filterDepth; ++wF) {\n    const yFMin = Math.max(0, Math.ceil((frontPad - wF) / strideDepth));\n    const yFMax = Math.min(\n        convInfo.outDepth, (convInfo.inDepth + frontPad - wF) / strideDepth);\n    const wOffset1 = wF * dwS0;\n\n    for (let wR = 0; wR < filterHeight; ++wR) {\n      const yRMin = Math.max(0, Math.ceil((topPad - wR) / strideHeight));\n      const yRMax = Math.min(\n          convInfo.outHeight, (convInfo.inHeight + topPad - wR) / strideHeight);\n      const wOffset2 = wR * dwS1 + wOffset1;\n\n      for (let wC = 0; wC < filterWidth; ++wC) {\n        const yCMin = Math.max(0, Math.ceil((leftPad - wC) / strideWidth));\n        const yCMax = Math.min(\n            convInfo.outWidth, (convInfo.inWidth + leftPad - wC) / strideWidth);\n        const wOffset3 = wC * dwS2 + wOffset2;\n\n        for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n          const wOffset4 = d1 * dwS3 + wOffset3;\n\n          for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n            let dotProd = 0;\n            for (let b = 0; b < convInfo.batchSize; ++b) {\n              const xOffset1 = b * xS0;\n              const yOffset1 = b * dyS0;\n\n              for (let yF = yFMin; yF < yFMax; ++yF) {\n                const xF = wF + yF * strideDepth - frontPad;\n                const xOffset2 = xF * xS1 + xOffset1;\n                const yOffset2 = yF * dyS1 + yOffset1;\n\n                for (let yR = yRMin; yR < yRMax; ++yR) {\n                  const xR = wR + yR * strideHeight - topPad;\n                  const xOffset3 = xR * xS2 + xOffset2;\n                  const yOffset3 = yR * dyS2 + yOffset2;\n\n                  for (let yC = yCMin; yC < yCMax; ++yC) {\n                    const xC = wC + yC * strideWidth - leftPad;\n                    const xOffset4 = xC * xS3 + xOffset3;\n                    const yOffset4 = yC * dyS3 + yOffset3;\n\n                    dotProd += xValues[xOffset4 + d1] * dyValues[yOffset4 + d2];\n                  }\n                }\n              }\n            }\n            dwValues[wOffset4 + d2] = dotProd;\n          }\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(dw.shape, dw.dtype, dw.values);\n}\n\nexport const conv3DBackpropFilterV2Config: KernelConfig = {\n  kernelName: Conv3DBackpropFilterV2,\n  backendName: 'cpu',\n  kernelFunc: conv3DBackpropFilterV2 as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Conv3DBackpropInputV2, Conv3DBackpropInputV2Attrs, Conv3DBackpropInputV2Inputs, KernelConfig, KernelFunc, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function conv3DBackpropInputV2(args: {\n  inputs: Conv3DBackpropInputV2Inputs,\n  backend: MathBackendCPU,\n  attrs: Conv3DBackpropInputV2Attrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {dy, filter} = inputs;\n  const {pad, strides, inputShape} = attrs;\n\n  assertNotComplex([dy], 'conv3dBackpropInputV2');\n\n  const dyStrides = util.computeStrides(dy.shape);\n  const filterStrides = util.computeStrides(filter.shape);\n\n  const convInfo = backend_util.computeConv3DInfo(\n      inputShape, filter.shape as [number, number, number, number, number],\n      strides, 1 /* dilations */, pad);\n\n  const dx = new TensorBuffer(convInfo.inShape, 'float32');\n  const dxValues = dx.values;\n  const [dxS0, dxS1, dxS2, dxS3] = dx.strides;\n  const dyValues = backend.data.get(dy.dataId).values as TypedArray;\n  const [dyS0, dyS1, dyS2, dyS3] = dyStrides;\n  const fltValues = backend.data.get(filter.dataId).values as TypedArray;\n  const [fltS0, fltS1, fltS2, fltS3] = filterStrides;\n  const {\n    batchSize,\n    filterDepth,\n    filterHeight,\n    filterWidth,\n    inChannels,\n    inDepth,\n    inHeight,\n    inWidth,\n    outChannels,\n    outDepth,\n    outHeight,\n    outWidth,\n    strideDepth,\n    strideHeight,\n    strideWidth\n  } = convInfo;\n  const frontPad = filterDepth - 1 - convInfo.padInfo.front;\n  const topPad = filterHeight - 1 - convInfo.padInfo.top;\n  const leftPad = filterWidth - 1 - convInfo.padInfo.left;\n\n  for (let b = 0; b < batchSize; ++b) {\n    for (let d1 = 0; d1 < inChannels; ++d1) {\n      // Frames of depth\n      for (let xF = 0; xF < inDepth; ++xF) {\n        const xFCorner = xF - frontPad;\n        const xFMin = Math.max(0, Math.ceil(xFCorner / strideDepth));\n        const yFMax =\n            Math.min(outDepth, (filterDepth + xFCorner) / strideDepth);\n\n        // Rows as per standard 2d matrix notation\n        for (let xR = 0; xR < inHeight; ++xR) {\n          const xRCorner = xR - topPad;\n          const xRMin = Math.max(0, Math.ceil(xRCorner / strideHeight));\n          const yRMax =\n              Math.min(outHeight, (filterHeight + xRCorner) / strideHeight);\n          // Columns as per standard 2d matrix notation\n          for (let xC = 0; xC < inWidth; ++xC) {\n            const xCCorner = xC - leftPad;\n            const xCMin = Math.max(0, Math.ceil(xCCorner / strideWidth));\n            const yCMax =\n                Math.min(outWidth, (filterWidth + xCCorner) / strideWidth);\n\n            let dotProd = 0;\n            for (let yF = xFMin; yF < yFMax; ++yF) {\n              const wF = yF * strideDepth - xFCorner;\n\n              for (let yR = xRMin; yR < yRMax; ++yR) {\n                const wR = yR * strideHeight - xRCorner;\n\n                for (let yC = xCMin; yC < yCMax; ++yC) {\n                  const wC = yC * strideWidth - xCCorner;\n                  const dyOffset = dyS0 * b + dyS1 * yF + dyS2 * yR + dyS3 * yC;\n                  const fltOffset = fltS0 * (filterDepth - 1 - wF) +\n                      fltS1 * (filterHeight - 1 - wR) +\n                      fltS2 * (filterWidth - 1 - wC) + fltS3 * d1;\n\n                  for (let d2 = 0; d2 < outChannels; ++d2) {\n                    const pixel = dyValues[dyOffset + d2];\n                    const weight = fltValues[fltOffset + d2];\n                    dotProd += pixel * weight;\n                  }\n                }\n              }\n            }\n            dxValues[dxS0 * b + dxS1 * xF + dxS2 * xR + dxS3 * xC + d1] =\n                dotProd;\n          }\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(dx.shape, dx.dtype, dx.values);\n}\n\nexport const conv3DBackpropInputV2Config: KernelConfig = {\n  kernelName: Conv3DBackpropInputV2,\n  backendName: 'cpu',\n  kernelFunc: conv3DBackpropInputV2 as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Cos, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const cos = unaryKernelFunc(Cos, (xi) => Math.cos(xi));\n\nexport const cosConfig: KernelConfig = {\n  kernelName: Cos,\n  backendName: 'cpu',\n  kernelFunc: cos,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Cosh, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const cosh = unaryKernelFunc(Cosh, (xi) => Math.cosh(xi));\n\nexport const coshConfig: KernelConfig = {\n  kernelName: Cosh,\n  backendName: 'cpu',\n  kernelFunc: cosh,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {buffer, CropAndResize, CropAndResizeAttrs, CropAndResizeInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport function cropAndResize(args: {\n  inputs: CropAndResizeInputs,\n  backend: MathBackendCPU,\n  attrs: CropAndResizeAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {image, boxes, boxInd} = inputs;\n  const {cropSize, method, extrapolationValue} = attrs;\n\n  const [batch, imageHeight, imageWidth, numChannels] = image.shape;\n  const numBoxes = boxes.shape[0];\n\n  const [cropHeight, cropWidth] = cropSize;\n  const output =\n      buffer([numBoxes, cropHeight, cropWidth, numChannels], 'float32');\n\n  const boxVals = backend.data.get(boxes.dataId).values as TypedArray;\n  const boxIndVals = backend.data.get(boxInd.dataId).values as TypedArray;\n  const imageVals = backend.data.get(image.dataId).values as TypedArray;\n\n  const inStride =\n      util.computeStrides(image.shape);  // to calculate flat indexes into image\n  const outStride = util.computeStrides(\n      output.shape);  // to calculate flat indexes into output\n\n  // Reference implementation\n  // tslint:disable-next-line:max-line-length\n  // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/crop_and_resize_op.cc\n  for (let b = 0; b < numBoxes; b++) {\n    const startInd = b * 4;\n    const y1 = boxVals[startInd];\n    const x1 = boxVals[startInd + 1];\n    const y2 = boxVals[startInd + 2];\n    const x2 = boxVals[startInd + 3];\n\n    const bInd: number = boxIndVals[b];\n    if (bInd >= batch) {\n      continue;\n    }\n\n    const heightScale =\n        (cropHeight > 1) ? (y2 - y1) * (imageHeight - 1) / (cropHeight - 1) : 0;\n    const widthScale =\n        (cropWidth > 1) ? (x2 - x1) * (imageWidth - 1) / (cropWidth - 1) : 0;\n\n    for (let y = 0; y < cropHeight; y++) {\n      const yInd: number = (cropHeight > 1) ?\n          y1 * (imageHeight - 1) + y * (heightScale) :\n          0.5 * (y1 + y2) * (imageHeight - 1);\n\n      if (yInd < 0 || yInd > imageHeight - 1) {\n        for (let x = 0; x < cropWidth; x++) {\n          for (let c = 0; c < numChannels; c++) {\n            const ind =\n                c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n            output.values[ind] = extrapolationValue;\n          }\n        }\n        continue;\n      }\n\n      if (method === 'bilinear') {\n        const topInd = Math.floor(yInd);\n        const bottomInd = Math.ceil(yInd);\n        const yLerp = yInd - topInd;\n\n        for (let x = 0; x < cropWidth; x++) {\n          const xInd = (cropWidth > 1) ?\n              x1 * (imageWidth - 1) + x * widthScale :\n              0.5 * (x1 + x2) * (imageWidth - 1);\n\n          if (xInd < 0 || xInd > imageWidth - 1) {\n            for (let c = 0; c < numChannels; c++) {\n              const ind =\n                  c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n              output.values[ind] = extrapolationValue;\n            }\n            continue;\n          }\n\n          const leftInd = Math.floor(xInd);\n          const rightInd = Math.ceil(xInd);\n          const xLerp = xInd - leftInd;\n\n          for (let c = 0; c < numChannels; c++) {\n            let ind = c + leftInd * inStride[2] + topInd * inStride[1] +\n                bInd * inStride[0];\n            const topLeft = imageVals[ind];\n\n            ind = c + rightInd * inStride[2] + topInd * inStride[1] +\n                bInd * inStride[0];\n            const topRight = imageVals[ind];\n\n            ind = c + leftInd * inStride[2] + bottomInd * inStride[1] +\n                bInd * inStride[0];\n            const bottomLeft = imageVals[ind];\n\n            ind = c + rightInd * inStride[2] + bottomInd * inStride[1] +\n                bInd * inStride[0];\n            const bottomRight = imageVals[ind];\n\n            const top = topLeft + (topRight - topLeft) * xLerp;\n            const bottom = bottomLeft + (bottomRight - bottomLeft) * xLerp;\n\n            ind = c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n            output.values[ind] = top + ((bottom - top) * yLerp);\n          }\n        }\n      } else {  // method == \"nearest\"\n        for (let x = 0; x < cropWidth; ++x) {\n          const xInd = (cropWidth > 1) ?\n              x1 * (imageWidth - 1) + x * widthScale :\n              0.5 * (x1 + x2) * (imageWidth - 1);\n\n          if (xInd < 0 || xInd > imageWidth - 1) {\n            for (let c = 0; c < numChannels; c++) {\n              const ind =\n                  c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n              output.values[ind] = extrapolationValue;\n            }\n            continue;\n          }\n\n          const closestX = Math.round(xInd);\n          const closestY = Math.round(yInd);\n          for (let c = 0; c < numChannels; c++) {\n            const inInd = c + closestX * inStride[2] + closestY * inStride[1] +\n                bInd * inStride[0];\n            const outInd =\n                c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n            output.values[outInd] = imageVals[inInd];\n          }\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(output.shape, output.dtype, output.values);\n}\n\nexport const cropAndResizeConfig: KernelConfig = {\n  kernelName: CropAndResize,\n  backendName: 'cpu',\n  kernelFunc: cropAndResize as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Cumprod, CumprodAttrs, CumprodInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, upcastType, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {transpose} from './Transpose';\n\nexport function cumprod(\n    args: {inputs: CumprodInputs, backend: MathBackendCPU,\n           attrs: CumprodAttrs}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, exclusive, reverse} = attrs;\n\n  assertNotComplex(x, 'cumprod');\n\n  const permutation = backend_util.getAxesPermutation([axis], x.shape.length);\n  let $x = x;\n  if (permutation != null) {\n    $x = transpose({inputs: {x}, backend, attrs: {perm: permutation}});\n  }\n  const permutedAxis = backend_util.getInnerMostAxes(1, x.shape.length)[0];\n\n  if (permutedAxis !== $x.shape.length - 1) {\n    throw new Error(\n        `backend.cumprod in CPU expects an inner-most ` +\n        `axis=${$x.shape.length - 1} but got axis=${permutedAxis}`);\n  }\n\n  const resultDtype = upcastType($x.dtype, 'int32');\n  const vals = util.makeOnesTypedArray(\n                   util.sizeFromShape($x.shape), resultDtype) as TypedArray;\n\n  const aVals = backend.data.get($x.dataId).values as TypedArray;\n  const finalDim = $x.shape[$x.shape.length - 1];\n  const indexAdjuster = reverse ?\n      (i: number, j: number) => i + finalDim - j - 1 :\n      (i: number, j: number) => i + j;\n  for (let i = 0; i < aVals.length; i += finalDim) {\n    for (let j = 0; j < finalDim; j++) {\n      const idx = indexAdjuster(i, j);\n      if (j === 0) {\n        vals[idx] = exclusive ? 1 : aVals[idx];\n      } else {\n        const prevIdx = indexAdjuster(i, j - 1);\n        vals[idx] = exclusive ? aVals[prevIdx] * vals[prevIdx] :\n                                aVals[idx] * vals[prevIdx];\n      }\n    }\n  }\n\n  const result = backend.makeTensorInfo($x.shape, resultDtype, vals);\n\n  if (permutation != null) {\n    const reversePermutation = backend_util.getUndoAxesPermutation(permutation);\n    const reverseTransposedResult = transpose(\n        {inputs: {x: result}, backend, attrs: {perm: reversePermutation}});\n\n    backend.disposeIntermediateTensorInfo(result);\n    backend.disposeIntermediateTensorInfo($x);\n\n    return reverseTransposedResult;\n  }\n\n  return result;\n}\n\nexport const cumprodConfig: KernelConfig = {\n  kernelName: Cumprod,\n  backendName: 'cpu',\n  kernelFunc: cumprod as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Cumsum, CumsumAttrs, CumsumInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, upcastType, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {transpose} from './Transpose';\n\nexport function cumsum(\n    args: {inputs: CumsumInputs, backend: MathBackendCPU, attrs: CumsumAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, exclusive, reverse} = attrs;\n\n  assertNotComplex(x, 'cumsum');\n\n  const permutation = backend_util.getAxesPermutation([axis], x.shape.length);\n  let $x = x;\n  if (permutation != null) {\n    $x = transpose({inputs: {x}, backend, attrs: {perm: permutation}});\n  }\n  const permutedAxis = backend_util.getInnerMostAxes(1, x.shape.length)[0];\n\n  if (permutedAxis !== $x.shape.length - 1) {\n    throw new Error(\n        `backend.cumsum in CPU expects an inner-most ` +\n        `axis=${$x.shape.length - 1} but got axis=${permutedAxis}`);\n  }\n\n  const resultDtype = upcastType($x.dtype, 'int32');\n  const vals = util.makeZerosTypedArray(\n                   util.sizeFromShape($x.shape), resultDtype) as TypedArray;\n\n  const aVals = backend.data.get($x.dataId).values as TypedArray;\n  const finalDim = $x.shape[$x.shape.length - 1];\n  const indexAdjuster = reverse ?\n      (i: number, j: number) => i + finalDim - j - 1 :\n      (i: number, j: number) => i + j;\n  for (let i = 0; i < aVals.length; i += finalDim) {\n    for (let j = 0; j < finalDim; j++) {\n      const idx = indexAdjuster(i, j);\n      if (j === 0) {\n        vals[idx] = exclusive ? 0 : aVals[idx];\n      } else {\n        const prevIdx = indexAdjuster(i, j - 1);\n        vals[idx] = exclusive ? aVals[prevIdx] + vals[prevIdx] :\n                                aVals[idx] + vals[prevIdx];\n      }\n    }\n  }\n\n  const result = backend.makeTensorInfo($x.shape, resultDtype, vals);\n\n  if (permutation != null) {\n    const reversePermutation = backend_util.getUndoAxesPermutation(permutation);\n    const reverseTransposedResult = transpose(\n        {inputs: {x: result}, backend, attrs: {perm: reversePermutation}});\n\n    backend.disposeIntermediateTensorInfo(result);\n    backend.disposeIntermediateTensorInfo($x);\n\n    return reverseTransposedResult;\n  }\n\n  return result;\n}\n\nexport const cumsumConfig: KernelConfig = {\n  kernelName: Cumsum,\n  backendName: 'cpu',\n  kernelFunc: cumsum as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DenseBincount, DenseBincountAttrs, DenseBincountInputs, KernelConfig, KernelFunc, Rank, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {bincountImpl, bincountReduceImpl} from './Bincount_impl';\n\nexport function denseBincount(args: {\n  inputs: DenseBincountInputs,\n  backend: MathBackendCPU,\n  attrs: DenseBincountAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, weights} = inputs;\n  const {size, binaryOutput} = attrs;\n\n  if (x.shape.length === 1) {\n    const xVals = backend.data.get(x.dataId).values as TypedArray;\n    const weightsVals = backend.data.get(weights.dataId).values as TypedArray;\n\n    const outVals =\n        bincountImpl(xVals, weightsVals, weights.dtype, weights.shape, size);\n\n    return backend.makeTensorInfo([size], weights.dtype, outVals);\n  } else if (x.shape.length === 2) {\n    const xBuf = backend.bufferSync<Rank, 'float32'>(x);\n    const weightsBuf = backend.bufferSync<Rank, 'float32'>(weights);\n\n    const outBuf = bincountReduceImpl(xBuf, weightsBuf, size, binaryOutput);\n\n    return backend.makeTensorInfo(outBuf.shape, weights.dtype, outBuf.values);\n  }\n\n  throw new Error(\n      `Error in denseBincount: input must be at most rank 2, but got rank` +\n      `${x.shape.length}.`);\n}\n\nexport const denseBincountConfig: KernelConfig = {\n  kernelName: DenseBincount,\n  backendName: 'cpu',\n  kernelFunc: denseBincount as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DepthToSpace, DepthToSpaceAttrs, DepthToSpaceInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport function depthToSpace(args: {\n  inputs: DepthToSpaceInputs,\n  backend: MathBackendCPU,\n  attrs: DepthToSpaceAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {blockSize, dataFormat} = attrs;\n\n  util.assert(\n      dataFormat === 'NHWC',\n      () => `Only NHWC dataFormat supported on CPU for depthToSpace. Got ${\n          dataFormat}`);\n\n  const batchSize = x.shape[0];\n  const inputHeight = x.shape[1];\n  const inputWidth = x.shape[2];\n  const inputDepth = x.shape[3];\n\n  const outputHeight = inputHeight * blockSize;\n  const outputWidth = inputWidth * blockSize;\n  const outputDepth = inputDepth / (blockSize * blockSize);\n\n  const xValues = backend.data.get(x.dataId).values as TypedArray;\n  const result =\n      new Float32Array(batchSize * outputHeight * outputWidth * outputDepth);\n\n  let outputIdx = 0;\n  for (let b = 0; b < batchSize; ++b) {\n    for (let h = 0; h < outputHeight; ++h) {\n      const inH = Math.floor(h / blockSize);\n      const offsetH = (h % blockSize);\n      for (let w = 0; w < outputWidth; ++w) {\n        const inW = Math.floor(w / blockSize);\n        const offsetW = (w % blockSize);\n        const offsetD = (offsetH * blockSize + offsetW) * outputDepth;\n        for (let d = 0; d < outputDepth; ++d) {\n          const inD = d + offsetD;\n          const inputIdx =\n              inD + inputDepth * (inW + inputWidth * (inH + inputHeight * b));\n          result[outputIdx++] = xValues[inputIdx];\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(\n      [batchSize, outputHeight, outputWidth, outputDepth], x.dtype, result);\n}\n\nexport const depthToSpaceConfig: KernelConfig = {\n  kernelName: DepthToSpace,\n  backendName: 'cpu',\n  kernelFunc: depthToSpace as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, DepthwiseConv2dNative, DepthwiseConv2dNativeAttrs, DepthwiseConv2dNativeInputs, KernelConfig, KernelFunc, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function depthwiseConv2dNative(args: {\n  inputs: DepthwiseConv2dNativeInputs,\n  backend: MathBackendCPU,\n  attrs: DepthwiseConv2dNativeAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, filter} = inputs;\n  const {strides, pad, dilations, dimRoundingMode} = attrs;\n\n  assertNotComplex([x, filter], 'depthwiseConv2DNative');\n\n  const xStrides = util.computeStrides(x.shape);\n  const filterStrides = util.computeStrides(filter.shape);\n\n  let $dilations = dilations;\n  if ($dilations == null) {\n    $dilations = [1, 1];\n  }\n\n  util.assert(\n      backend_util.eitherStridesOrDilationsAreOne(strides, $dilations),\n      () => 'Error in depthwiseConv2d: Either strides or dilations must be ' +\n          `1. Got strides ${strides} and dilations '${$dilations}'`);\n\n  const convInfo = backend_util.computeConv2DInfo(\n      x.shape as [number, number, number, number],\n      filter.shape as [number, number, number, number], strides, $dilations,\n      pad, dimRoundingMode, true /* depthwise */);\n\n  const {filterHeight, filterWidth, dilationHeight, dilationWidth, padInfo} =\n      convInfo;\n  const padLeft = padInfo.left;\n  const padTop = padInfo.top;\n  const chMul = convInfo.outChannels / convInfo.inChannels;\n  const y = new TensorBuffer(convInfo.outShape, x.dtype as 'float32');\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const wVals = backend.data.get(filter.dataId).values as TypedArray;\n  const yVals = y.values;\n\n  for (let b = 0; b < convInfo.batchSize; ++b) {\n    const xOffset1 = b * xStrides[0];\n    const yOffset1 = b * y.strides[0];\n    for (let yR = 0; yR < convInfo.outHeight; ++yR) {\n      const yOffset2 = yOffset1 + yR * y.strides[1];\n      const xRCorner = yR * convInfo.strideHeight - padTop;\n      for (let wR = 0; wR < filterHeight; ++wR) {\n        const xR = xRCorner + wR * dilationHeight;\n        if (xR < 0 || xR >= convInfo.inHeight) {\n          continue;\n        }\n        const wOffset1 = wR * filterStrides[0];\n        const xOffset2 = xOffset1 + xR * xStrides[1];\n        for (let yC = 0; yC < convInfo.outWidth; ++yC) {\n          const yOffset3 = yOffset2 + yC * y.strides[2];\n          const xCCorner = yC * convInfo.strideWidth - padLeft;\n          for (let wC = 0; wC < filterWidth; ++wC) {\n            const xC = xCCorner + wC * dilationWidth;\n            if (xC < 0 || xC >= convInfo.inWidth) {\n              continue;\n            }\n            const wOffset2 = wOffset1 + wC * filterStrides[1];\n            const xOffset3 = xOffset2 + xC * convInfo.inChannels;\n            let yOffset4 = yOffset3;\n            let wOffset3 = wOffset2;\n            for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n              const xVal = xVals[xOffset3 + d1];\n              for (let q = 0; q < chMul; ++q) {\n                yVals[yOffset4 + q] += xVal * wVals[wOffset3 + q];\n              }\n              yOffset4 += chMul;\n              wOffset3 += chMul;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(y.shape, y.dtype, y.values);\n}\n\nexport const depthwiseConv2dNativeConfig: KernelConfig = {\n  kernelName: DepthwiseConv2dNative,\n  backendName: 'cpu',\n  kernelFunc: depthwiseConv2dNative as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, DepthwiseConv2dNativeBackpropFilter, DepthwiseConv2dNativeBackpropFilterAttrs, DepthwiseConv2dNativeBackpropFilterInputs, KernelConfig, KernelFunc, TensorBuffer, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function depthwiseConv2dNativeBackpropFilter(args: {\n  inputs: DepthwiseConv2dNativeBackpropFilterInputs,\n  backend: MathBackendCPU,\n  attrs: DepthwiseConv2dNativeBackpropFilterAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, dy} = inputs;\n  const {strides, dilations, pad, dimRoundingMode, filterShape} = attrs;\n\n  assertNotComplex([x, dy], 'depthwiseConv2dNativeBackpropFilter');\n\n  const convInfo = backend_util.computeConv2DInfo(\n      x.shape as [number, number, number, number], filterShape, strides,\n      dilations, pad, dimRoundingMode, true /* depthwise */);\n\n  const {strideHeight, strideWidth, filterHeight, filterWidth} = convInfo;\n\n  const dW = new TensorBuffer(convInfo.filterShape, 'float32');\n\n  const leftPad = convInfo.padInfo.left;\n  const topPad = convInfo.padInfo.top;\n  const chMul = convInfo.outChannels / convInfo.inChannels;\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const xBuf = new TensorBuffer(x.shape, x.dtype, xVals);\n  const dyVals = backend.data.get(dy.dataId).values as TypedArray;\n  const dyBuf = new TensorBuffer(dy.shape, dy.dtype, dyVals);\n  for (let wR = 0; wR < filterHeight; ++wR) {\n    const yRMin = Math.max(0, Math.ceil((topPad - wR) / strideHeight));\n    const yRMax = Math.min(\n        convInfo.outHeight, (convInfo.inHeight + topPad - wR) / strideHeight);\n\n    for (let wC = 0; wC < filterWidth; ++wC) {\n      const yCMin = Math.max(0, Math.ceil((leftPad - wC) / strideWidth));\n      const yCMax = Math.min(\n          convInfo.outWidth, (convInfo.inWidth + leftPad - wC) / strideWidth);\n\n      for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n        const d1 = Math.trunc(d2 / chMul);\n        const dm = d2 % chMul;\n\n        let dotProd = 0;\n        for (let b = 0; b < convInfo.batchSize; ++b) {\n          for (let yR = yRMin; yR < yRMax; ++yR) {\n            const xR = wR + yR * strideHeight - topPad;\n            for (let yC = yCMin; yC < yCMax; ++yC) {\n              const xC = wC + yC * strideWidth - leftPad;\n              dotProd += (xBuf.get(b, xR, xC, d1) as number) *\n                  (dyBuf.get(b, yR, yC, d2) as number);\n            }\n          }\n        }\n        dW.set(dotProd, wR, wC, d1, dm);\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(dW.shape, dW.dtype, dW.values);\n}\n\nexport const depthwiseConv2dNativeBackpropFilterConfig: KernelConfig = {\n  kernelName: DepthwiseConv2dNativeBackpropFilter,\n  backendName: 'cpu',\n  kernelFunc: depthwiseConv2dNativeBackpropFilter as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, DepthwiseConv2dNativeBackpropInput, DepthwiseConv2dNativeBackpropInputAttrs, DepthwiseConv2dNativeBackpropInputInputs, KernelConfig, KernelFunc, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function depthwiseConv2dNativeBackpropInput(args: {\n  inputs: DepthwiseConv2dNativeBackpropInputInputs,\n  backend: MathBackendCPU,\n  attrs: DepthwiseConv2dNativeBackpropInputAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {dy, filter} = inputs;\n  const {strides, dilations, pad, dimRoundingMode, inputShape} = attrs;\n\n  assertNotComplex([dy, filter], 'depthwiseConv2DNativeBackpropInput');\n\n  const dyStrides = util.computeStrides(dy.shape);\n  const filterStrides = util.computeStrides(filter.shape);\n\n  const convInfo = backend_util.computeConv2DInfo(\n      inputShape, filter.shape as [number, number, number, number], strides,\n      dilations, pad, dimRoundingMode, true /* depthwise */);\n\n  const dx = new TensorBuffer(convInfo.inShape, 'float32');\n  const dxValues = dx.values;\n  const [dxS0, dxS1, dxS2] = dx.strides;\n  const dyValues = backend.data.get(dy.dataId).values as TypedArray;\n  const [dyS0, dyS1, dyS2] = dyStrides;\n  const fltValues = backend.data.get(filter.dataId).values as TypedArray;\n  const [fltS0, fltS1, fltS2] = filterStrides;\n  const {\n    batchSize,\n    filterHeight,\n    filterWidth,\n    inChannels,\n    inHeight,\n    inWidth,\n    outChannels,\n    outHeight,\n    outWidth,\n    strideHeight,\n    strideWidth\n  } = convInfo;\n  const topPad = filterHeight - 1 - convInfo.padInfo.top;\n  const leftPad = filterWidth - 1 - convInfo.padInfo.left;\n  const chMul = outChannels / inChannels;\n\n  for (let b = 0; b < batchSize; ++b) {\n    for (let d1 = 0; d1 < inChannels; ++d1) {\n      for (let xR = 0; xR < inHeight; ++xR) {\n        const xRCorner = xR - topPad;\n        const xRMin = Math.max(0, Math.ceil(xRCorner / strideHeight));\n        const yRMax =\n            Math.min(outHeight, (filterHeight + xRCorner) / strideHeight);\n\n        for (let xC = 0; xC < inWidth; ++xC) {\n          const xCCorner = xC - leftPad;\n          const xCMin = Math.max(0, Math.ceil(xCCorner / strideWidth));\n          const yCMax =\n              Math.min(outWidth, (filterWidth + xCCorner) / strideWidth);\n\n          let dotProd = 0;\n          for (let yR = xRMin; yR < yRMax; ++yR) {\n            const wR = yR * strideHeight - xRCorner;\n\n            for (let yC = xCMin; yC < yCMax; ++yC) {\n              const wC = yC * strideWidth - xCCorner;\n              const dyOffset = dyS0 * b + dyS1 * yR + dyS2 * yC;\n              const fltOffset = fltS0 * (filterHeight - 1 - wR) +\n                  fltS1 * (filterWidth - 1 - wC) + fltS2 * d1;\n\n              for (let dm = 0; dm < chMul; ++dm) {\n                const d2 = d1 * chMul + dm;\n                const pixel = dyValues[dyOffset + d2];\n                const weight = fltValues[fltOffset + dm];\n                dotProd += pixel * weight;\n              }\n            }\n          }\n          dxValues[dxS0 * b + dxS1 * xR + dxS2 * xC + d1] = dotProd;\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(dx.shape, dx.dtype, dx.values);\n}\n\nexport const depthwiseConv2dNativeBackpropInputConfig: KernelConfig = {\n  kernelName: DepthwiseConv2dNativeBackpropInput,\n  backendName: 'cpu',\n  kernelFunc: depthwiseConv2dNativeBackpropInput as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {buffer, Diag, DiagInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport function diag(args: {inputs: DiagInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {x} = inputs;\n\n  const xSize = util.sizeFromShape(x.shape);\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const outBuf = buffer([xSize, xSize], x.dtype);\n  const vals = outBuf.values;\n  for (let i = 0; i < xVals.length; i++) {\n    vals[i * xSize + i] = xVals[i];\n  }\n\n  const outShape = [...x.shape, ...x.shape];\n\n  return backend.makeTensorInfo(outShape, outBuf.dtype, outBuf.values);\n}\n\nexport const diagConfig: KernelConfig = {\n  kernelName: Diag,\n  backendName: 'cpu',\n  kernelFunc: diag as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Dilation2D, Dilation2DAttrs, Dilation2DInputs, KernelConfig, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport const dilation2DConfig: KernelConfig = {\n  kernelName: Dilation2D,\n  backendName: 'cpu',\n  kernelFunc: ({inputs, backend, attrs}) => {\n    const {x, filter} = inputs as Dilation2DInputs;\n    const {strides, pad, dilations} = attrs as {} as Dilation2DAttrs;\n    const cpuBackend = backend as MathBackendCPU;\n\n    const xVals = cpuBackend.data.get(x.dataId).values as TypedArray;\n    const xRank = x.shape.length;\n\n    const filterVals = cpuBackend.data.get(filter.dataId).values as TypedArray;\n    const filterRank = filter.shape.length;\n\n    const {\n      batchSize,\n      inHeight,\n      inWidth,\n      inChannels,\n      outHeight,\n      outWidth,\n      padInfo,\n      strideHeight,\n      strideWidth,\n      filterHeight,\n      filterWidth,\n      dilationHeight,\n      dilationWidth,\n      outShape\n    } =\n        backend_util.computeDilation2DInfo(\n            x.shape as [number, number, number, number],\n            filter.shape as [number, number, number], strides, pad,\n            'NHWC' /* dataFormat */, dilations);\n\n    const outSize = util.sizeFromShape(outShape);\n    const outRank = outShape.length;\n    const outputVals = util.getArrayFromDType(x.dtype, outSize);\n\n    // Upsampling the input by fill in `dilation size - 1` values between each\n    // input value.\n    // This implementation follows the TF c++ implementation:\n    // https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/dilation_ops.cc\n    for (let b = 0; b < batchSize; ++b) {\n      for (let hOut = 0; hOut < outHeight; ++hOut) {\n        const hBeg = hOut * strideHeight - padInfo.top;\n        for (let wOut = 0; wOut < outWidth; ++wOut) {\n          const wBeg = wOut * strideWidth - padInfo.left;\n          for (let d = 0; d < inChannels; ++d) {\n            let curVal = Number.MIN_SAFE_INTEGER;\n            for (let h = 0; h < filterHeight; ++h) {\n              const hIn = hBeg + h * dilationHeight;\n              if (hIn >= 0 && hIn < inHeight) {\n                for (let w = 0; w < filterWidth; ++w) {\n                  const wIn = wBeg + w * dilationWidth;\n                  if (wIn >= 0 && wIn < inWidth) {\n                    const xIndex = util.locToIndex(\n                        [b, hIn, wIn, d], xRank, util.computeStrides(x.shape));\n                    const filterIndex = util.locToIndex(\n                        [h, w, d], filterRank,\n                        util.computeStrides(filter.shape));\n                    const val = xVals[xIndex] + filterVals[filterIndex];\n                    if (val > curVal) {\n                      curVal = val;\n                    }\n                  }\n                }\n              }\n            }\n            const outputIndex = util.locToIndex(\n                [b, hOut, wOut, d], outRank, util.computeStrides(outShape));\n            outputVals[outputIndex] = curVal;\n          }\n        }\n      }\n    }\n\n    const dataId = cpuBackend.write(\n        util.toTypedArray(outputVals, x.dtype), outShape, x.dtype);\n\n    return {dataId, shape: outShape, dtype: x.dtype};\n  }\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Dilation2DAttrs, Dilation2DBackpropFilter, Tensor3D, Tensor4D, TypedArray, util} from '@tensorflow/tfjs-core';\nimport {KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport const dilation2DBackpropFilterConfig: KernelConfig = {\n  kernelName: Dilation2DBackpropFilter,\n  backendName: 'cpu',\n  kernelFunc: ({inputs, backend, attrs}) => {\n    const {x, filter, dy} =\n        inputs as {x: Tensor4D, filter: Tensor3D, dy: Tensor4D};\n    const {strides, pad, dilations} = attrs as {} as Dilation2DAttrs;\n    const cpuBackend = backend as MathBackendCPU;\n\n    const $x =\n        util.toNestedArray(\n            x.shape, cpuBackend.data.get(x.dataId).values as TypedArray) as\n        number[][][][];\n\n    const $filter = util.toNestedArray(\n                        filter.shape,\n                        cpuBackend.data.get(filter.dataId).values as\n                            TypedArray) as number[][][];\n\n    const {\n      batchSize,\n      inHeight,\n      inWidth,\n      inChannels,\n      outHeight,\n      outWidth,\n      padInfo,\n      strideHeight,\n      strideWidth,\n      filterHeight,\n      filterWidth,\n      dilationHeight,\n      dilationWidth,\n      outShape\n    } =\n        backend_util.computeDilation2DInfo(\n            x.shape as [number, number, number, number],\n            filter.shape as [number, number, number], strides, pad,\n            'NHWC' /* dataFormat */, dilations);\n\n    util.assert(\n        dy.rank === outShape.length,\n        () => `Error in ${Dilation2DBackpropFilter}, dy ` +\n            `must have the same rank as output ${outShape.length}, but got ` +\n            `${dy.rank}`);\n\n    const $dy =\n        util.toNestedArray(\n            outShape, cpuBackend.data.get(dy.dataId).values as TypedArray) as\n        number[][][][];\n\n    // The computed filter gradients has the same dimensions as the filter:\n    // [filterHeight, filterWidth, depth]\n    const gradients = util.makeZerosNestedTypedArray(\n                          filter.shape, filter.dtype) as number[][][];\n\n    // In the case of multiple argmax branches, we only back-propagate along the\n    // last branch, i.e., the one with largest value of `h * filter_cols + w`,\n    // similarly to the max-pooling backward routines.\n    // This implementation follows the TF c++ implementation:\n    // https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/dilation_ops.cc\n    for (let b = 0; b < batchSize; ++b) {\n      for (let hOut = 0; hOut < outHeight; ++hOut) {\n        const hBeg = hOut * strideHeight - padInfo.top;\n        for (let wOut = 0; wOut < outWidth; ++wOut) {\n          const wBeg = wOut * strideWidth - padInfo.left;\n          for (let d = 0; d < inChannels; ++d) {\n            let curVal = Number.MIN_SAFE_INTEGER;\n            let hMax = 0;\n            let wMax = 0;\n            for (let h = 0; h < filterHeight; ++h) {\n              const hIn = hBeg + h * dilationHeight;\n              if (hIn >= 0 && hIn < inHeight) {\n                for (let w = 0; w < filterWidth; ++w) {\n                  const wIn = wBeg + w * dilationWidth;\n                  if (wIn >= 0 && wIn < inWidth) {\n                    const val = $x[b][hIn][wIn][d] + $filter[h][w][d];\n                    if (val > curVal) {\n                      curVal = val;\n                      hMax = h;\n                      wMax = w;\n                    }\n                  }\n                }\n              }\n            }\n            gradients[hMax][wMax][d] += $dy[b][hOut][wOut][d];\n          }\n        }\n      }\n    }\n\n    const dataId = cpuBackend.write(\n        util.toTypedArray(gradients, x.dtype), filter.shape, filter.dtype);\n\n    return {dataId, shape: filter.shape, dtype: filter.dtype};\n  }\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Dilation2DAttrs, Dilation2DBackpropInput, Tensor3D, Tensor4D, TypedArray, util} from '@tensorflow/tfjs-core';\nimport {KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport const dilation2DBackpropInputConfig: KernelConfig = {\n  kernelName: Dilation2DBackpropInput,\n  backendName: 'cpu',\n  kernelFunc: ({inputs, backend, attrs}) => {\n    const {x, filter, dy} =\n        inputs as {x: Tensor4D, filter: Tensor3D, dy: Tensor4D};\n    const {strides, pad, dilations} = attrs as {} as Dilation2DAttrs;\n    const cpuBackend = backend as MathBackendCPU;\n\n    const $x =\n        util.toNestedArray(\n            x.shape, cpuBackend.data.get(x.dataId).values as TypedArray) as\n        number[][][][];\n\n    const $filter = util.toNestedArray(\n                        filter.shape,\n                        cpuBackend.data.get(filter.dataId).values as\n                            TypedArray) as number[][][];\n\n    const {\n      batchSize,\n      inHeight,\n      inWidth,\n      inChannels,\n      outHeight,\n      outWidth,\n      padInfo,\n      strideHeight,\n      strideWidth,\n      filterHeight,\n      filterWidth,\n      dilationHeight,\n      dilationWidth,\n      outShape\n    } =\n        backend_util.computeDilation2DInfo(\n            x.shape as [number, number, number, number],\n            filter.shape as [number, number, number], strides, pad,\n            'NHWC' /* dataFormat */, dilations);\n\n    util.assert(\n        dy.rank === outShape.length,\n        () => `Error in ${Dilation2DBackpropInput}, dy ` +\n            `must have the same rank as output ${outShape.length}, but got ` +\n            `${dy.rank}`);\n\n    const $dy =\n        util.toNestedArray(\n            outShape, cpuBackend.data.get(dy.dataId).values as TypedArray) as\n        number[][][][];\n\n    // The computed gradients has the same dimensions as the input:\n    // [batch, inputHeight, inputCols, inChannel]\n    const gradients =\n        util.makeZerosNestedTypedArray(x.shape, x.dtype) as number[][][][];\n\n    // In the case of multiple argmax branches, we only back-propagate along the\n    // last branch, i.e., the one with largest value of `h * filter_cols + w`,\n    // similarly to the max-pooling backward routines.\n    // This implementation follows the TF c++ implementation:\n    // https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/dilation_ops.cc\n    for (let b = 0; b < batchSize; ++b) {\n      for (let hOut = 0; hOut < outHeight; ++hOut) {\n        const hBeg = hOut * strideHeight - padInfo.top;\n        for (let wOut = 0; wOut < outWidth; ++wOut) {\n          const wBeg = wOut * strideWidth - padInfo.left;\n          for (let d = 0; d < inChannels; ++d) {\n            let curVal = Number.MIN_SAFE_INTEGER;\n            let hInMax = (hBeg < 0) ? 0 : hBeg;\n            let wInMax = (wBeg < 0) ? 0 : wBeg;\n            for (let h = 0; h < filterHeight; ++h) {\n              const hIn = hBeg + h * dilationHeight;\n              if (hIn >= 0 && hIn < inHeight) {\n                for (let w = 0; w < filterWidth; ++w) {\n                  const wIn = wBeg + w * dilationWidth;\n                  if (wIn >= 0 && wIn < inWidth) {\n                    const val = $x[b][hIn][wIn][d] + $filter[h][w][d];\n                    if (val > curVal) {\n                      curVal = val;\n                      hInMax = hIn;\n                      wInMax = wIn;\n                    }\n                  }\n                }\n              }\n            }\n            gradients[b][hInMax][wInMax][d] += $dy[b][hOut][wOut][d];\n          }\n        }\n      }\n    }\n\n    const dataId = cpuBackend.write(\n        util.toTypedArray(gradients, x.dtype), x.shape, x.dtype);\n\n    return {dataId, shape: x.shape, dtype: x.dtype};\n  }\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, Sum, SumAttrs, SumInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {zeros} from '../utils/zeros_impl';\nimport {cast} from './Cast';\nimport {identity} from './Identity';\nimport {reshape} from './Reshape';\nimport {transpose} from './Transpose';\n\nexport function sum(\n    args: {inputs: SumInputs, backend: MathBackendCPU, attrs: SumAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, keepDims} = attrs;\n\n  assertNotComplex(x, 'sum');\n\n  let $x;\n  if (x.dtype === 'bool') {\n    $x = cast({inputs: {x}, backend, attrs: {dtype: 'int32'}});\n  } else {\n    $x = identity({inputs: {x}, backend});\n  }\n\n  const xRank = $x.shape.length;\n  const axes = util.parseAxisParam(axis, $x.shape);\n  const permutation = backend_util.getAxesPermutation(axes, xRank);\n\n  let reductionAxes = axes;\n  let permutedX = $x;\n  if (permutation != null) {\n    permutedX =\n        transpose({inputs: {x: $x}, backend, attrs: {perm: permutation}});\n    reductionAxes = backend_util.getInnerMostAxes(reductionAxes.length, xRank);\n  }\n\n  backend_util.assertAxesAreInnerMostDims(\n      'sum', reductionAxes, permutedX.shape.length);\n\n  const [outShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes(permutedX.shape, reductionAxes);\n  const resultDtype = backend_util.upcastType(permutedX.dtype, 'int32');\n  let result = zeros(backend, outShape, resultDtype);\n  const reduceSize = util.sizeFromShape(reduceShape);\n  const vals = backend.data.get(result.dataId).values as TypedArray;\n\n  const aVals = backend.data.get(permutedX.dataId).values as TypedArray;\n  for (let i = 0; i < vals.length; ++i) {\n    const offset = i * reduceSize;\n    let sum = 0;\n    for (let j = 0; j < reduceSize; ++j) {\n      sum += aVals[offset + j];\n    }\n    vals[i] = sum;\n  }\n\n  if (keepDims) {\n    const newShape = backend_util.expandShapeToKeepDim(result.shape, axes);\n    const oldResult = result;\n    result = reshape({inputs: {x: result}, backend, attrs: {shape: newShape}});\n    backend.disposeIntermediateTensorInfo(oldResult);\n  }\n\n  backend.disposeIntermediateTensorInfo($x);\n\n  if (permutation != null) {\n    backend.disposeIntermediateTensorInfo(permutedX);\n  }\n\n  return result;\n}\n\nexport const sumConfig: KernelConfig = {\n  kernelName: Sum,\n  backendName: 'cpu',\n  kernelFunc: sum as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Einsum, EinsumAttrs, EinsumInputs, KernelConfig, KernelFunc, Tensor, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {multiply} from './Multiply';\nimport {reshape} from './Reshape';\nimport {sum} from './Sum';\nimport {transpose} from './Transpose';\n\nexport function einsum(\n    args: {inputs: EinsumInputs, backend: MathBackendCPU, attrs: EinsumAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {equation} = attrs;\n  const tensors = inputs as Tensor[];\n\n  const {allDims, summedDims, idDims} =\n      backend_util.decodeEinsumEquation(equation, tensors.length);\n  backend_util.checkEinsumDimSizes(allDims.length, idDims, tensors);\n  const {path, steps} = backend_util.getEinsumComputePath(summedDims, idDims);\n\n  const nSteps = steps.length;\n  let out: TensorInfo|null = null;\n  let numDimsRemaining = allDims.length;\n  const tensorsToDispose: TensorInfo[] = [];\n  for (let i = 0; i < nSteps; ++i) {\n    for (const idTerm of steps[i]) {\n      const {permutationIndices: perm, expandDims: dimsToExpand} =\n          backend_util.getEinsumPermutation(numDimsRemaining, idDims[idTerm]);\n      let x: TensorInfo;\n      if (backend_util.isIdentityPermutation(perm)) {\n        x = tensors[idTerm];\n      } else {\n        x = transpose({inputs: {x: tensors[idTerm]}, backend, attrs: {perm}});\n        tensorsToDispose.push(x);\n      }\n      const targetShape: number[] = x.shape.slice();\n      for (let k = 0; k < dimsToExpand.length; ++k) {\n        targetShape.splice(dimsToExpand[k], 0, 1);\n      }\n\n      if (!util.arraysEqual(x.shape, targetShape)) {\n        x = reshape({inputs: {x}, backend, attrs: {shape: targetShape}});\n        tensorsToDispose.push(x);\n      }\n      if (out === null) {\n        out = x;\n      } else {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        out = multiply({inputs: {a: x, b: out}, backend}) as TensorInfo;\n        tensorsToDispose.push(out);\n      }\n    }\n    if (i < nSteps - 1) {\n      if (path[i] >= 0) {\n        out = sum({\n          inputs: {x: out},\n          backend,\n          attrs: {\n            axis: path[i] - (allDims.length - numDimsRemaining),\n            keepDims: false\n          }\n        });\n        tensorsToDispose.push(out);\n      }\n      numDimsRemaining--;\n    }\n  }\n\n  // Clean up intermediate tensors.\n  for (const tensorInfo of tensorsToDispose) {\n    if (tensorInfo === out) {\n      continue;\n    }\n    backend.disposeIntermediateTensorInfo(tensorInfo);\n  }\n\n  return out;\n}\n\nexport const einsumConfig: KernelConfig = {\n  kernelName: Einsum,\n  backendName: 'cpu',\n  kernelFunc: einsum as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {EluGrad, EluGradInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function eluGrad(args: {inputs: EluGradInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {dy, y} = inputs;\n\n  assertNotComplex([dy, y], 'eluGrad');\n\n  const resultValues = new Float32Array(util.sizeFromShape(y.shape));\n  const values = backend.data.get(y.dataId).values as TypedArray;\n  const dyValues = backend.data.get(dy.dataId).values as TypedArray;\n  for (let i = 0; i < values.length; ++i) {\n    const v = values[i];\n    if (v >= 1) {\n      resultValues[i] = dyValues[i];\n    } else {\n      resultValues[i] = dyValues[i] * (v + 1);\n    }\n  }\n\n  return backend.makeTensorInfo(y.shape, 'float32', resultValues);\n}\n\nexport const eluGradConfig: KernelConfig = {\n  kernelName: EluGrad,\n  backendName: 'cpu',\n  kernelFunc: eluGrad as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Erf, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nconst p = backend_util.ERF_P;\nconst a1 = backend_util.ERF_A1;\nconst a2 = backend_util.ERF_A2;\nconst a3 = backend_util.ERF_A3;\nconst a4 = backend_util.ERF_A4;\nconst a5 = backend_util.ERF_A5;\n\nexport const erf = unaryKernelFunc(\n    Erf,\n    (xi) => {\n      const sign = Math.sign(xi);\n      const v = Math.abs(xi);\n      const t = 1.0 / (1.0 + p * v);\n      return sign *\n          (1.0 -\n           (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t *\n               Math.exp(-v * v));\n    },\n);\n\nexport const erfConfig: KernelConfig = {\n  kernelName: Erf,\n  backendName: 'cpu',\n  kernelFunc: erf,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ExpandDims, ExpandDimsAttrs, ExpandDimsInputs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {reshape} from './Reshape';\n\nexport function expandDims(args: {\n  inputs: ExpandDimsInputs,\n  backend: MathBackendCPU,\n  attrs: ExpandDimsAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {input} = inputs;\n  const {dim} = attrs;\n\n  const inputRank = input.shape.length;\n  const newShape = input.shape.slice();\n  let $dim = dim;\n  if (dim < 0) {\n    // Negative value is counted from the tail of rank.\n    util.assert(\n        -(inputRank + 1) <= dim,\n        () => `Axis must be in the interval [${- (inputRank + 1)}, ${\n            inputRank}]`);\n    $dim = inputRank + dim + 1;\n  }\n  newShape.splice($dim, 0, 1);\n\n  return reshape({inputs: {x: input}, backend, attrs: {shape: newShape}});\n}\n\nexport const expandDimsConfig: KernelConfig = {\n  kernelName: ExpandDims,\n  backendName: 'cpu',\n  kernelFunc: expandDims as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, RealDiv} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const realDivImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => a / b);\nexport const div = binaryKernelFunc(RealDiv, realDivImpl);\n\nexport const realDivConfig: KernelConfig = {\n  kernelName: RealDiv,\n  backendName: 'cpu',\n  kernelFunc: div\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Tensor, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {add} from '../kernels/Add';\nimport {complex} from '../kernels/Complex';\nimport {concat} from '../kernels/Concat';\nimport {identity} from '../kernels/Identity';\nimport {imag} from '../kernels/Imag';\nimport {multiply} from '../kernels/Multiply';\nimport {real} from '../kernels/Real';\nimport {realDivConfig} from '../kernels/RealDiv';\nimport {slice} from '../kernels/Slice';\nimport {sub} from '../kernels/Sub';\n\n/**\n * Calculate FFT of inner most elements of batch tensor.\n */\nexport function fftBatch(\n    input: TensorInfo, inverse: boolean,\n    cpuBackend: MathBackendCPU): TensorInfo {\n  const inputShape = input.shape;\n  const batch = inputShape[0];\n  const innerDim = inputShape[1];\n\n  const inputVals = cpuBackend.data.get(input.dataId);\n\n  const real2D = inputVals.complexTensorInfos.real;\n  const imag2D = inputVals.complexTensorInfos.imag;\n\n  // Collects real and imaginary values separately.\n  const resultShape = [batch, innerDim];\n  const resultSize = util.sizeFromShape(resultShape);\n  const resultReal = util.getTypedArrayFromDType('float32', resultSize);\n  const resultImag = util.getTypedArrayFromDType('float32', resultSize);\n\n  for (let b = 0; b < batch; b++) {\n    // TODO: Support slice ops for complex type.\n    const r = slice({\n      inputs: {x: real2D},\n      backend: cpuBackend,\n      attrs: {begin: [b, 0], size: [1, innerDim]}\n    });\n    const i = slice({\n      inputs: {x: imag2D},\n      backend: cpuBackend,\n      attrs: {begin: [b, 0], size: [1, innerDim]}\n    });\n\n    const input = complex({inputs: {real: r, imag: i}, backend: cpuBackend});\n\n    // Run FFT by batch element.\n    const {real, imag} = fftImpl(input, inverse, cpuBackend);\n    const res = backend_util.mergeRealAndImagArrays(real, imag);\n\n    for (let d = 0; d < innerDim; d++) {\n      const c = backend_util.getComplexWithIndex(res, d);\n      resultReal[b * innerDim + d] = c.real;\n      resultImag[b * innerDim + d] = c.imag;\n    }\n\n    cpuBackend.disposeIntermediateTensorInfo(r);\n    cpuBackend.disposeIntermediateTensorInfo(i);\n    cpuBackend.disposeIntermediateTensorInfo(input);\n  }\n\n  const $realInfo: TensorInfo =\n      cpuBackend.makeTensorInfo(resultShape, 'float32', resultReal);\n  const $imagInfo: TensorInfo =\n      cpuBackend.makeTensorInfo(resultShape, 'float32', resultImag);\n\n  const result = complex(\n      {inputs: {real: $realInfo, imag: $imagInfo}, backend: cpuBackend});\n\n  cpuBackend.disposeIntermediateTensorInfo($realInfo);\n  cpuBackend.disposeIntermediateTensorInfo($imagInfo);\n\n  return result;\n}\n\nexport function fftImpl(\n    input: TensorInfo, inverse: boolean,\n    cpuBackend: MathBackendCPU): {real: Float32Array, imag: Float32Array} {\n  const inputSize = util.sizeFromShape(input.shape);\n\n  const inputVals = cpuBackend.data.get(input.dataId);\n\n  const realVals =\n      cpuBackend.data.get(inputVals.complexTensorInfos.real.dataId).values as\n      Float32Array;\n\n  const imagVals =\n      cpuBackend.data.get(inputVals.complexTensorInfos.imag.dataId).values as\n      Float32Array;\n\n  if (isExponentOf2(inputSize)) {\n    const result =\n        fftRadix2(realVals, imagVals, inputSize, inverse, cpuBackend);\n\n    const resultShape = [input.shape[0], input.shape[1]];\n\n    if (inverse) {\n      const realInfo: TensorInfo =\n          cpuBackend.makeTensorInfo(resultShape, 'float32', result.real);\n      const imagInfo: TensorInfo =\n          cpuBackend.makeTensorInfo(resultShape, 'float32', result.imag);\n\n      const sizeInfo: TensorInfo = cpuBackend.makeTensorInfo(\n          [], 'float32',\n          util.createScalarValue(inputSize as {} as 'float32', 'float32'));\n      const sizeInfoCopy =\n          identity({inputs: {x: sizeInfo}, backend: cpuBackend});\n\n      const divRealInfo =\n          realDivConfig.kernelFunc(\n              {inputs: {a: realInfo, b: sizeInfo}, backend: cpuBackend}) as\n          TensorInfo;\n      const divImagInfo =\n          realDivConfig.kernelFunc(\n              {inputs: {a: imagInfo, b: sizeInfoCopy}, backend: cpuBackend}) as\n          TensorInfo;\n\n      const divRealVals =\n          cpuBackend.data.get(divRealInfo.dataId).values as Float32Array;\n      const divImagVals =\n          cpuBackend.data.get(divImagInfo.dataId).values as Float32Array;\n\n      cpuBackend.disposeIntermediateTensorInfo(realInfo);\n      cpuBackend.disposeIntermediateTensorInfo(imagInfo);\n      cpuBackend.disposeIntermediateTensorInfo(sizeInfo);\n      cpuBackend.disposeIntermediateTensorInfo(sizeInfoCopy);\n      cpuBackend.disposeIntermediateTensorInfo(divRealInfo);\n      cpuBackend.disposeIntermediateTensorInfo(divImagInfo);\n\n      return {real: divRealVals, imag: divImagVals};\n    }\n\n    return result;\n  } else {\n    const data = backend_util.mergeRealAndImagArrays(realVals, imagVals);\n\n    const rawOutput =\n        fourierTransformByMatmul(data, inputSize, inverse) as Float32Array;\n\n    return backend_util.splitRealAndImagArrays(rawOutput);\n  }\n}\n\nfunction isExponentOf2(size: number): boolean {\n  return (size & size - 1) === 0;\n}\n\n// FFT using Cooley-Tukey algorithm on radix 2 dimensional input.\nfunction fftRadix2(\n    realVals: Float32Array, imagVals: Float32Array, size: number,\n    inverse: boolean,\n    cpuBackend: MathBackendCPU): {real: Float32Array, imag: Float32Array} {\n  if (size === 1) {\n    return {real: realVals, imag: imagVals};\n  }\n\n  const data = backend_util.mergeRealAndImagArrays(realVals, imagVals);\n\n  const half = size / 2;\n\n  const evenComplex = backend_util.complexWithEvenIndex(data);\n\n  const evenRealVals = evenComplex.real;\n  const evenImagVals = evenComplex.imag;\n\n  const evenShape = [evenRealVals.length];\n\n  const evenRealInfo =\n      cpuBackend.makeTensorInfo(evenShape, 'float32', evenRealVals);\n  const evenImagInfo =\n      cpuBackend.makeTensorInfo(evenShape, 'float32', evenImagVals);\n\n  const evenTensorInfo = complex(\n      {inputs: {real: evenRealInfo, imag: evenImagInfo}, backend: cpuBackend});\n\n  const oddComplex = backend_util.complexWithOddIndex(data);\n\n  const oddRealVals = oddComplex.real;\n  const oddImagVals = oddComplex.imag;\n\n  const oddShape = [oddRealVals.length];\n\n  const oddRealInfo =\n      cpuBackend.makeTensorInfo(oddShape, 'float32', oddRealVals);\n  const oddImagInfo =\n      cpuBackend.makeTensorInfo(oddShape, 'float32', oddImagVals);\n\n  const oddTensorInfo = complex(\n      {inputs: {real: oddRealInfo, imag: oddImagInfo}, backend: cpuBackend});\n\n  // Recursive call for half part of original input.\n  const $evenComplex =\n      fftRadix2(evenRealVals, evenImagVals, half, inverse, cpuBackend);\n\n  const $evenRealVals = $evenComplex.real;\n  const $evenImagVals = $evenComplex.imag;\n\n  const $evenShape = [$evenRealVals.length];\n\n  const $evenRealInfo =\n      cpuBackend.makeTensorInfo($evenShape, 'float32', $evenRealVals);\n  const $evenImagInfo =\n      cpuBackend.makeTensorInfo($evenShape, 'float32', $evenImagVals);\n\n  const $evenTensorInfo = complex({\n    inputs: {real: $evenRealInfo, imag: $evenImagInfo},\n    backend: cpuBackend\n  });\n\n  const $oddComplex =\n      fftRadix2(oddRealVals, oddImagVals, half, inverse, cpuBackend);\n\n  const $oddRealVals = $oddComplex.real;\n  const $oddImagVals = $oddComplex.imag;\n\n  const $oddShape = [$oddRealVals.length];\n\n  const $oddRealInfo =\n      cpuBackend.makeTensorInfo($oddShape, 'float32', $oddRealVals);\n  const $oddImagInfo =\n      cpuBackend.makeTensorInfo($oddShape, 'float32', $oddImagVals);\n\n  const $oddTensorInfo = complex(\n      {inputs: {real: $oddRealInfo, imag: $oddImagInfo}, backend: cpuBackend});\n\n  const e = backend_util.exponents(size, inverse);\n  const eShape = [e.real.length];\n\n  const eRealInfo = cpuBackend.makeTensorInfo(eShape, 'float32', e.real);\n  const eImagInfo = cpuBackend.makeTensorInfo(eShape, 'float32', e.imag);\n\n  const complexInfo = complex(\n      {inputs: {real: eRealInfo, imag: eImagInfo}, backend: cpuBackend});\n\n  const exponentInfo =\n      multiply(\n          {inputs: {a: complexInfo, b: $oddTensorInfo}, backend: cpuBackend}) as\n      TensorInfo;\n\n  const addPart = add({\n                    inputs: {a: $evenTensorInfo, b: exponentInfo},\n                    backend: cpuBackend\n                  }) as TensorInfo;\n  const subPart = sub({\n                    inputs: {a: $evenTensorInfo, b: exponentInfo},\n                    backend: cpuBackend\n                  }) as TensorInfo;\n\n  const addPartReal = real({inputs: {input: addPart}, backend: cpuBackend});\n  const subPartReal = real({inputs: {input: subPart}, backend: cpuBackend});\n\n  const addPartImag = imag({inputs: {input: addPart}, backend: cpuBackend});\n  const subPartImag = imag({inputs: {input: subPart}, backend: cpuBackend});\n\n  const $real = concat({\n    inputs: [addPartReal as Tensor, subPartReal as Tensor],\n    backend: cpuBackend,\n    attrs: {axis: 0}\n  });\n  const $imag = concat({\n    inputs: [addPartImag as Tensor, subPartImag as Tensor],\n    backend: cpuBackend,\n    attrs: {axis: 0}\n  });\n\n  const $realVals = cpuBackend.data.get($real.dataId).values as Float32Array;\n  const $imagVals = cpuBackend.data.get($imag.dataId).values as Float32Array;\n\n  cpuBackend.disposeIntermediateTensorInfo(evenRealInfo);\n  cpuBackend.disposeIntermediateTensorInfo(evenImagInfo);\n  cpuBackend.disposeIntermediateTensorInfo(evenTensorInfo);\n  cpuBackend.disposeIntermediateTensorInfo(oddRealInfo);\n  cpuBackend.disposeIntermediateTensorInfo(oddImagInfo);\n  cpuBackend.disposeIntermediateTensorInfo(oddTensorInfo);\n  cpuBackend.disposeIntermediateTensorInfo($evenRealInfo);\n  cpuBackend.disposeIntermediateTensorInfo($evenImagInfo);\n  cpuBackend.disposeIntermediateTensorInfo($evenTensorInfo);\n  cpuBackend.disposeIntermediateTensorInfo($oddRealInfo);\n  cpuBackend.disposeIntermediateTensorInfo($oddImagInfo);\n  cpuBackend.disposeIntermediateTensorInfo($oddTensorInfo);\n  cpuBackend.disposeIntermediateTensorInfo(eRealInfo);\n  cpuBackend.disposeIntermediateTensorInfo(eImagInfo);\n  cpuBackend.disposeIntermediateTensorInfo(complexInfo);\n  cpuBackend.disposeIntermediateTensorInfo(exponentInfo);\n  cpuBackend.disposeIntermediateTensorInfo(addPart);\n  cpuBackend.disposeIntermediateTensorInfo(subPart);\n  cpuBackend.disposeIntermediateTensorInfo(addPartReal);\n  cpuBackend.disposeIntermediateTensorInfo(addPartImag);\n  cpuBackend.disposeIntermediateTensorInfo(subPartReal);\n  cpuBackend.disposeIntermediateTensorInfo(subPartImag);\n  cpuBackend.disposeIntermediateTensorInfo($real);\n  cpuBackend.disposeIntermediateTensorInfo($imag);\n\n  return {real: $realVals, imag: $imagVals};\n}\n\n// Calculate fourier transform by multplying sinusoid matrix.\nfunction fourierTransformByMatmul(\n    data: TypedArray, size: number, inverse: boolean): TypedArray {\n  const ret = new Float32Array(size * 2);\n  // TODO: Use matmul instead once it supports complex64 type.\n  for (let r = 0; r < size; r++) {\n    let real = 0.0;\n    let imag = 0.0;\n    for (let c = 0; c < size; c++) {\n      const e = backend_util.exponent(r * c, size, inverse);\n      const term = backend_util.getComplexWithIndex(data as Float32Array, c);\n      real += term.real * e.real - term.imag * e.imag;\n      imag += term.real * e.imag + term.imag * e.real;\n    }\n    if (inverse) {\n      real /= size;\n      imag /= size;\n    }\n    backend_util.assignToTypedArray(ret, real, imag, r);\n  }\n  return ret;\n}\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {FFT, FFTInputs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {fftBatch} from '../utils/fft_utils';\nimport {reshape} from './Reshape';\n\nexport function fft(args: {inputs: FFTInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {input} = inputs;\n\n  const inputSize = util.sizeFromShape(input.shape);\n\n  // Collapse all outer dimensions to a single batch dimension.\n  const innerDimensionSize = input.shape[input.shape.length - 1];\n  const batch = inputSize / innerDimensionSize;\n\n  const input2D = reshape({\n    inputs: {x: input},\n    backend,\n    attrs: {shape: [batch, innerDimensionSize]}\n  });\n\n  const result = fftBatch(input2D, false, backend);\n\n  const resultReshaped =\n      reshape({inputs: {x: result}, backend, attrs: {shape: input.shape}});\n\n  backend.disposeIntermediateTensorInfo(input2D);\n  backend.disposeIntermediateTensorInfo(result);\n\n  return resultReshaped;\n}\n\nexport const fftConfig: KernelConfig = {\n  kernelName: FFT,\n  backendName: 'cpu',\n  kernelFunc: fft as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType, DataValues, Fill, FillAttrs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport function fill(args: {backend: MathBackendCPU, attrs: FillAttrs}):\n    TensorInfo {\n  const {backend, attrs} = args;\n  const {shape, value, dtype} = attrs;\n\n  const $dtype = dtype || util.inferDtype(value);\n  const values = util.getArrayFromDType($dtype, util.sizeFromShape(shape));\n  fillValues(values, value, $dtype);\n\n  return backend.makeTensorInfo(shape, $dtype, values);\n}\n\nexport const fillConfig: KernelConfig = {\n  kernelName: Fill,\n  backendName: 'cpu',\n  kernelFunc: fill as {} as KernelFunc\n};\n\nfunction fillValues(\n    values: DataValues, value: string|number, dtype: DataType): void {\n  if (dtype === 'string') {\n    (values as string[]).fill(value as string);\n  } else {\n    (values as TypedArray).fill(value as number);\n  }\n}\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, NumericDataType, TypedArray} from '@tensorflow/tfjs-core';\nimport {FlipLeftRight, FlipLeftRightInputs, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport const flipLeftRightConfig: KernelConfig = {\n  kernelName: FlipLeftRight,\n  backendName: 'cpu',\n  kernelFunc: ({inputs, attrs, backend}) => {\n    const {image} = inputs as FlipLeftRightInputs;\n    const cpuBackend = backend as MathBackendCPU;\n\n    const output = util.getTypedArrayFromDType(\n        image.dtype as NumericDataType, util.sizeFromShape(image.shape));\n    const [batch, imageHeight, imageWidth, numChannels] = image.shape;\n\n    const imageVals = cpuBackend.data.get(image.dataId).values as TypedArray;\n\n    for (let batchIdx = 0; batchIdx < batch; batchIdx++) {\n      const batchOffset = batchIdx * imageWidth * imageHeight * numChannels;\n\n      for (let row = 0; row < imageHeight; row++) {\n        const rowOffset = row * (imageWidth * numChannels);\n\n        for (let col = 0; col < imageWidth; col++) {\n          const colOffset = col * numChannels;\n\n          for (let channel = 0; channel < numChannels; channel++) {\n            const coordX = Math.round(imageWidth - col - 1);\n            const outIdx = batchOffset + rowOffset + colOffset + channel;\n\n            let outputValue = imageVals[outIdx];\n            // If the coordinate position falls within the image boundaries...\n            if (coordX >= 0 && coordX < imageWidth) {\n              // set the output to the image value at the coordinate position.\n              const rotatedColOffset = coordX * numChannels;\n              const imageIdx =\n                  batchOffset + rowOffset + rotatedColOffset + channel;\n              outputValue = imageVals[imageIdx];\n            }\n            output[outIdx] = outputValue;\n          }\n        }\n      }\n    }\n\n    const dataId = cpuBackend.write(output, image.shape, image.dtype);\n    return {dataId, shape: image.shape, dtype: image.dtype};\n  }\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {FloorDiv, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const floorDivImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => Math.floor(a / b));\nexport const floorDiv =\n    binaryKernelFunc(FloorDiv, floorDivImpl, null /* complexImpl */, 'int32');\n\nexport const floorDivConfig: KernelConfig = {\n  kernelName: FloorDiv,\n  backendName: 'cpu',\n  kernelFunc: floorDiv\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {FusedConv2D, FusedConv2DAttrs, FusedConv2DInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {applyActivation} from '../utils/fused_utils';\nimport {add} from './Add';\nimport {conv2D} from './Conv2D';\nimport {reshape} from './Reshape';\n\nexport function fusedConv2D(args: {\n  inputs: FusedConv2DInputs,\n  backend: MathBackendCPU,\n  attrs: FusedConv2DAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, filter, bias, preluActivationWeights} = inputs;\n  const {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  } = attrs;\n\n  let result = conv2D({\n    inputs: {x, filter},\n    backend,\n    attrs: {strides, pad, dataFormat, dilations, dimRoundingMode}\n  });\n\n  if (bias) {\n    const resultOld = result;\n    // For NCHW format, if bias is a 1-D tensor, it is supposed to be aligned\n    // to the channel of the conv2d's result; if the bias is a scalar, the\n    // bias_add is computed as if the bias was broadcasted to the shape of the\n    // conv2d's result.\n    if (dataFormat === 'NCHW' && bias.shape.length === 1 &&\n        bias.shape[0] !== 1) {\n      const reshapedBias = reshape(\n          {inputs: {x: bias}, backend, attrs: {shape: [bias.shape[0], 1, 1]}});\n      result =\n          add({inputs: {a: result, b: reshapedBias}, backend}) as TensorInfo;\n      backend.disposeIntermediateTensorInfo(reshapedBias);\n    } else {\n      // This condition handles NHWC and NCHW (scalar case). The only other case\n      // for NCHW (1D case) is handled above.\n      result = add({inputs: {a: result, b: bias}, backend}) as TensorInfo;\n    }\n    backend.disposeIntermediateTensorInfo(resultOld);\n  }\n\n  if (activation) {\n    const resultOld = result;\n    // For NCHW format, if PReLu activation weights is a 1-D tensor, it is\n    // supposed to be aligned with the channel of the conv2d's result. For other\n    // cases, whether NCHW or NHWC data format, the conv2d result is\n    // already aligned with the activation weights.\n    if (dataFormat === 'NCHW' && activation === 'prelu' &&\n        preluActivationWeights.shape.length === 1 &&\n        preluActivationWeights.shape[0] !== 1) {\n      const reshapedAlpha = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: [preluActivationWeights.shape[0], 1, 1]}\n      });\n      result = applyActivation(\n          backend, result, activation, reshapedAlpha, leakyreluAlpha);\n      backend.disposeIntermediateTensorInfo(reshapedAlpha);\n    } else {\n      result = applyActivation(\n          backend, result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    backend.disposeIntermediateTensorInfo(resultOld);\n  }\n\n  return result;\n}\n\nexport const fusedConv2DConfig: KernelConfig = {\n  kernelName: FusedConv2D,\n  backendName: 'cpu',\n  kernelFunc: fusedConv2D as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {FusedDepthwiseConv2D, FusedDepthwiseConv2DAttrs, FusedDepthwiseConv2DInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {applyActivation} from '../utils/fused_utils';\nimport {add} from './Add';\nimport {depthwiseConv2dNative} from './DepthwiseConv2dNative';\n\nexport function fusedDepthwiseConv2D(args: {\n  inputs: FusedDepthwiseConv2DInputs,\n  backend: MathBackendCPU,\n  attrs: FusedDepthwiseConv2DAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, filter, bias, preluActivationWeights} = inputs;\n  const {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  } = attrs;\n\n  let result = depthwiseConv2dNative({\n    inputs: {x, filter},\n    backend,\n    attrs: {strides, pad, dataFormat, dilations, dimRoundingMode}\n  });\n\n  if (bias) {\n    const oldResult = result;\n    result = add({inputs: {a: result, b: bias}, backend}) as TensorInfo;\n    backend.disposeIntermediateTensorInfo(oldResult);\n  }\n  if (activation) {\n    const oldResult = result;\n    result = applyActivation(\n        backend, result, activation, preluActivationWeights, leakyreluAlpha);\n    backend.disposeIntermediateTensorInfo(oldResult);\n  }\n\n  return result;\n}\n\nexport const fusedDepthwiseConv2DConfig: KernelConfig = {\n  kernelName: FusedDepthwiseConv2D,\n  backendName: 'cpu',\n  kernelFunc: fusedDepthwiseConv2D as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, GatherNd, GatherNdInputs, KernelConfig, KernelFunc, Rank, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {gatherNdImpl} from './GatherNd_Impl';\n\nexport function gatherNd(\n    args: {inputs: GatherNdInputs, backend: MathBackendCPU}): TensorInfo {\n  const {inputs, backend} = args;\n  const {params, indices} = inputs;\n\n  const paramsSize = util.sizeFromShape(params.shape);\n\n  const indicesShape = indices.shape;\n  const sliceRank = indicesShape[indicesShape.length - 1];\n\n  const [resultShape, numSlices, sliceSize, strides] =\n      backend_util.prepareAndValidate(params, indices);\n  if (numSlices === 0) {\n    return backend.makeTensorInfo(resultShape, params.dtype, []);\n  }\n\n  const indicesData = backend.data.get(indices.dataId).values as TypedArray;\n  const paramsBuf = backend.bufferSync<Rank, 'float32'>(params);\n  const outBuf = gatherNdImpl(\n      indicesData, paramsBuf, params.dtype, numSlices, sliceRank, sliceSize,\n      strides, params.shape, paramsSize);\n\n  return backend.makeTensorInfo(resultShape, params.dtype, outBuf.values);\n}\n\nexport const gatherNdConfig: KernelConfig = {\n  kernelName: GatherNd,\n  backendName: 'cpu',\n  kernelFunc: gatherNd as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, GatherV2, GatherV2Attrs, GatherV2Inputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {gatherV2Impl} from './GatherV2_impl';\nimport {reshape} from './Reshape';\n\nexport function gatherV2(args: {\n  inputs: GatherV2Inputs,\n  backend: MathBackendCPU,\n  attrs: GatherV2Attrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, indices} = inputs;\n  const {axis, batchDims} = attrs;\n\n  assertNotComplex([x, indices], 'gatherV2');\n\n  // Throw error when any index is out of bound.\n  const parsedAxis = util.parseAxisParam(axis, x.shape)[0];\n  const indicesVals = backend.data.get(indices.dataId).values as TypedArray;\n  const axisDim = x.shape[parsedAxis];\n  for (let i = 0; i < indicesVals.length; ++i) {\n    const index = indicesVals[i];\n    util.assert(\n        index <= axisDim - 1 && index >= 0,\n        () =>\n            `GatherV2: the index value ${index} is not in [0, ${axisDim - 1}]`);\n  }\n\n  let $batchDims = batchDims;\n\n  if (batchDims == null) {\n    $batchDims = 0;\n  }\n\n  const indicesSize = util.sizeFromShape(indices.shape);\n\n  const shapeInfo = backend_util.segment_util.collectGatherOpShapeInfo(\n      x, indices, parsedAxis, $batchDims);\n\n  const flattenX = reshape({\n    inputs: {x},\n    backend,\n    attrs: {\n      shape: [\n        shapeInfo.batchSize, shapeInfo.outerSize, shapeInfo.dimSize,\n        shapeInfo.sliceSize\n      ]\n    }\n  });\n\n  const flattenIndex = reshape({\n    inputs: {x: indices},\n    backend,\n    attrs: {shape: [shapeInfo.batchSize, indicesSize / shapeInfo.batchSize]}\n  });\n\n  const flattenOutputShape = [\n    shapeInfo.batchSize, shapeInfo.outerSize, indicesSize / shapeInfo.batchSize,\n    shapeInfo.sliceSize\n  ];\n\n  const indicesBuf = backend.bufferSync(flattenIndex);\n  const xBuf = backend.bufferSync(flattenX);\n  const outBuf = gatherV2Impl(xBuf, indicesBuf, flattenOutputShape);\n\n  backend.disposeIntermediateTensorInfo(flattenX);\n  backend.disposeIntermediateTensorInfo(flattenIndex);\n\n  return backend.makeTensorInfo(\n      shapeInfo.outputShape, outBuf.dtype, outBuf.values);\n}\n\nexport const gatherV2Config: KernelConfig = {\n  kernelName: GatherV2,\n  backendName: 'cpu',\n  kernelFunc: gatherV2 as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {IFFT, IFFTInputs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {fftBatch} from '../utils/fft_utils';\nimport {reshape} from './Reshape';\n\nexport function ifft(args: {inputs: IFFTInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {input} = inputs;\n\n  const inputSize = util.sizeFromShape(input.shape);\n\n  // Collapse all outer dimensions to a single batch dimension.\n  const innerDimensionSize = input.shape[input.shape.length - 1];\n  const batch = inputSize / innerDimensionSize;\n\n  const input2D = reshape({\n    inputs: {x: input},\n    backend,\n    attrs: {shape: [batch, innerDimensionSize]}\n  });\n\n  const result = fftBatch(input2D, true, backend);\n\n  const resultReshaped =\n      reshape({inputs: {x: result}, backend, attrs: {shape: input.shape}});\n\n  backend.disposeIntermediateTensorInfo(input2D);\n  backend.disposeIntermediateTensorInfo(result);\n\n  return resultReshaped;\n}\n\nexport const ifftConfig: KernelConfig = {\n  kernelName: IFFT,\n  backendName: 'cpu',\n  kernelFunc: ifft as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {IsFinite, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const isFinite =\n    unaryKernelFunc(IsFinite, (xi) => Number.isFinite(xi) ? 1 : 0, 'bool');\n\nexport const isFiniteConfig: KernelConfig = {\n  kernelName: IsFinite,\n  backendName: 'cpu',\n  kernelFunc: isFinite,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {IsInf, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const isInf =\n    unaryKernelFunc(IsInf, (xi) => Math.abs(xi) === Infinity ? 1 : 0, 'bool');\n\nexport const isInfConfig: KernelConfig = {\n  kernelName: IsInf,\n  backendName: 'cpu',\n  kernelFunc: isInf,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {IsNan, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const isNaN =\n    unaryKernelFunc(IsNan, (xi) => Number.isNaN(xi) ? 1 : 0, 'bool');\n\nexport const isNaNConfig: KernelConfig = {\n  kernelName: IsNan,\n  backendName: 'cpu',\n  kernelFunc: isNaN,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, LinSpace, LinSpaceAttrs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {linSpaceImpl} from './LinSpace_impl';\n\nexport function linSpace(args: {backend: MathBackendCPU, attrs: LinSpaceAttrs}):\n    TensorInfo {\n  const {backend, attrs} = args;\n  const {start, stop, num} = attrs;\n\n  const outVals = linSpaceImpl(start, stop, num);\n\n  return backend.makeTensorInfo([outVals.length], 'float32', outVals);\n}\n\nexport const linSpaceConfig: KernelConfig = {\n  kernelName: LinSpace,\n  backendName: 'cpu',\n  kernelFunc: linSpace as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Log1p} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const log1p = unaryKernelFunc(Log1p, (xi) => Math.log1p(xi));\n\nexport const log1pConfig: KernelConfig = {\n  kernelName: Log1p,\n  backendName: 'cpu',\n  kernelFunc: log1p,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, LogicalAnd} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const logicalAndImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => a && b);\nexport const logicalAnd = binaryKernelFunc(\n    LogicalAnd, logicalAndImpl, null /* complexImpl */, 'bool');\n\nexport const logicalAndConfig: KernelConfig = {\n  kernelName: LogicalAnd,\n  backendName: 'cpu',\n  kernelFunc: logicalAnd\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, LogicalNot} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const logicalNot =\n    unaryKernelFunc(LogicalNot, (xi) => xi ? 0 : 1, 'bool');\n\nexport const logicalNotConfig: KernelConfig = {\n  kernelName: LogicalNot,\n  backendName: 'cpu',\n  kernelFunc: logicalNot,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, LogicalOr} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const logicalOrImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => a || b);\nexport const logicalOr =\n    binaryKernelFunc(LogicalOr, logicalOrImpl, null /* complexImpl */, 'bool');\n\nexport const logicalOrConfig: KernelConfig = {\n  kernelName: LogicalOr,\n  backendName: 'cpu',\n  kernelFunc: logicalOr\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, LRN, LRNAttrs, LRNInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function lRN(\n    args: {inputs: LRNInputs, backend: MathBackendCPU, attrs: LRNAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {depthRadius, bias, alpha, beta} = attrs;\n\n  assertNotComplex(x, 'LRN');\n\n  const channels = x.shape[3];\n  const maxD = channels - 1;\n  const xValues = backend.data.get(x.dataId).values as TypedArray;\n  const size = util.sizeFromShape(x.shape);\n  const result = new Float32Array(size);\n\n  function sumAcrossChannels(offset: number) {\n    const currentChannel = offset % channels;\n    let beginSumOffset =\n        offset - currentChannel + Math.max(0, currentChannel - depthRadius);\n    const endSumOffset =\n        offset - currentChannel + Math.min(currentChannel + depthRadius, maxD);\n\n    let sum = 0.0;\n    for (; beginSumOffset <= endSumOffset; beginSumOffset++) {\n      const z = xValues[beginSumOffset];\n      sum += z * z;\n    }\n    return sum;\n  }\n\n  for (let offset = 0; offset < size; offset++) {\n    const sum = sumAcrossChannels(offset);\n    const val = xValues[offset] * Math.pow(bias + alpha * sum, -beta);\n    result[offset] = val;\n  }\n\n  return backend.makeTensorInfo(x.shape, x.dtype, result);\n}\n\n// tslint:disable-next-line: variable-name\nexport const LRNConfig: KernelConfig = {\n  kernelName: LRN,\n  backendName: 'cpu',\n  kernelFunc: lRN as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, LRNGrad, LRNGradAttrs, LRNGradInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function lRNGrad(\n    args:\n        {inputs: LRNGradInputs, backend: MathBackendCPU, attrs: LRNGradAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, y, dy} = inputs;\n  const {depthRadius, bias, alpha, beta} = attrs;\n\n  assertNotComplex(dy, 'LRNGrad');\n\n  const dySize = util.sizeFromShape(dy.shape);\n\n  const channels = dy.shape[3];\n  const dyValues = backend.data.get(dy.dataId).values as TypedArray;\n  const xValues = backend.data.get(x.dataId).values as TypedArray;\n  const yValues = backend.data.get(y.dataId).values as TypedArray;\n  const result = new Float32Array(dySize);\n  const size = dySize;\n\n  for (let offset = 0; offset < size; offset++) {\n    const currentChannel = offset % channels;\n    const depthBegin =\n        (offset - currentChannel) + Math.max(0, currentChannel - depthRadius);\n    const depthEnd = (offset - currentChannel) +\n        Math.min(channels, currentChannel + depthRadius + 1);\n\n    let norm = 0;\n    for (let k = depthBegin; k < depthEnd; k++) {\n      norm += Math.pow(xValues[k], 2);\n    }\n    norm = alpha * norm + bias;\n\n    for (let k = depthBegin; k < depthEnd; k++) {\n      let dyi = -2 * alpha * beta * xValues[k] * yValues[offset] / norm;\n      if (offset === k) {\n        dyi += Math.pow(norm, -beta);\n      }\n      dyi *= dyValues[offset];\n      result[k] += dyi;\n    }\n  }\n\n  return backend.makeTensorInfo(dy.shape, x.dtype, result);\n}\n\n// tslint:disable-next-line: variable-name\nexport const LRNGradConfig: KernelConfig = {\n  kernelName: LRNGrad,\n  backendName: 'cpu',\n  kernelFunc: lRNGrad as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelFunc, Max, MaxAttrs, MaxInputs, TensorInfo} from '@tensorflow/tfjs-core';\nimport {backend_util, KernelConfig} from '@tensorflow/tfjs-core';\nimport {TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nimport {maxImpl} from './Max_impl';\nimport {transposeImpl} from './Transpose_impl';\n\nexport function max(\n    args: {inputs: MaxInputs, backend: MathBackendCPU, attrs: MaxAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {reductionIndices, keepDims} = attrs;\n  const cpuBackend = backend;\n  let xShape = x.shape;\n  const xRank = xShape.length;\n\n  const origAxes = util.parseAxisParam(reductionIndices, xShape);\n  let axes = origAxes;\n  const permutedAxes = backend_util.getAxesPermutation(axes, xRank);\n  let xVals = cpuBackend.data.get(x.dataId).values as TypedArray;\n  if (permutedAxes != null) {\n    const newShape: number[] = new Array(xRank);\n    for (let i = 0; i < newShape.length; i++) {\n      newShape[i] = xShape[permutedAxes[i]];\n    }\n\n    xVals = transposeImpl(xVals, xShape, x.dtype, permutedAxes, newShape);\n    axes = backend_util.getInnerMostAxes(axes.length, xRank);\n\n    xShape = newShape;\n  }\n\n  assertNotComplex(x, 'max');\n  backend_util.assertAxesAreInnerMostDims('max', axes, xRank);\n  const [maxOutShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes(xShape, axes);\n\n  const reduceSize = util.sizeFromShape(reduceShape);\n\n  const result = maxImpl(xVals, reduceSize, maxOutShape, x.dtype);\n  const dataId = cpuBackend.write(result, maxOutShape, x.dtype);\n\n  let outShape = maxOutShape;\n  if (keepDims) {\n    // reshape\n    const newShape = backend_util.expandShapeToKeepDim(maxOutShape, origAxes);\n    outShape = newShape;\n  }\n\n  return {dataId, shape: outShape, dtype: x.dtype};\n}\n\nexport const maxConfig: KernelConfig = {\n  kernelName: Max,\n  backendName: 'cpu',\n  kernelFunc: max as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {backend_util, KernelConfig, KernelFunc, MaxPool, MaxPoolAttrs, MaxPoolInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {pool} from '../utils/pool_utils';\nimport {identity} from './Identity';\n\nexport function maxPool(\n    args:\n        {inputs: MaxPoolInputs, backend: MathBackendCPU, attrs: MaxPoolAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  assertNotComplex(x, 'maxPool');\n  const {filterSize, strides, pad, dimRoundingMode} = attrs;\n  const dilations = 1;\n\n  util.assert(\n      backend_util.eitherStridesOrDilationsAreOne(strides, dilations),\n      () => 'Error in maxPool: Either strides or dilations must be 1. ' +\n          `Got strides ${strides} and dilations '${dilations}'`);\n\n  const convInfo = backend_util.computePool2DInfo(\n      x.shape as [number, number, number, number], filterSize, strides,\n      dilations, pad, dimRoundingMode);\n  let res: TensorInfo;\n\n  if (convInfo.filterWidth === 1 && convInfo.filterHeight === 1 &&\n      util.arraysEqual(convInfo.inShape, convInfo.outShape)) {\n    res = identity({inputs: {x}, backend});\n  } else {\n    const xValues = backend.data.get(x.dataId).values as TypedArray;\n    const strides = util.computeStrides(x.shape);\n    const buffer = pool(xValues, x.shape, x.dtype, strides, convInfo, 'max');\n    res = backend.makeTensorInfo(\n        convInfo.outShape, x.dtype, buffer.values as TypedArray);\n  }\n  return res;\n}\n\nexport const maxPoolConfig: KernelConfig = {\n  kernelName: MaxPool,\n  backendName: 'cpu',\n  kernelFunc: maxPool as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, MaxPool3D, MaxPool3DAttrs, MaxPool3DInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {pool3d} from '../utils/pool_utils';\n\nexport function maxPool3D(args: {\n  inputs: MaxPool3DInputs,\n  backend: MathBackendCPU,\n  attrs: MaxPool3DAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {filterSize, strides, pad, dimRoundingMode, dataFormat} = attrs;\n\n  assertNotComplex(x, 'maxPool3d');\n\n  const convInfo = backend_util.computePool3DInfo(\n      x.shape as [number, number, number, number, number], filterSize, strides,\n      1 /* dilations */, pad, dimRoundingMode, dataFormat);\n\n  const xValues = backend.data.get(x.dataId).values as TypedArray;\n  const outBuf = pool3d(\n      xValues, x.shape, x.dtype, util.computeStrides(x.shape), convInfo, 'max');\n\n  return backend.makeTensorInfo(outBuf.shape, 'float32', outBuf.values);\n}\n\nexport const maxPool3DConfig: KernelConfig = {\n  kernelName: MaxPool3D,\n  backendName: 'cpu',\n  kernelFunc: maxPool3D as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, buffer, KernelConfig, KernelFunc, MaxPool3DGrad, MaxPool3DGradAttrs, MaxPool3DGradInputs, Rank, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {maxPool3dPositions} from '../utils/pool_utils';\n\nexport function maxPool3DGrad(args: {\n  inputs: MaxPool3DGradInputs,\n  backend: MathBackendCPU,\n  attrs: MaxPool3DGradAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {dy, input} = inputs;\n  const {filterSize, strides, pad, dimRoundingMode} = attrs;\n\n  assertNotComplex([dy, input], 'maxPool3DGrad');\n\n  const convInfo = backend_util.computePool3DInfo(\n      input.shape as [number, number, number, number, number], filterSize,\n      strides, 1 /* dilations */, pad, dimRoundingMode);\n\n  const inputBuf = backend.bufferSync(input);\n  const maxPosBuf = maxPool3dPositions(inputBuf, convInfo);\n  const strideDepth = convInfo.strideDepth;\n  const strideHeight = convInfo.strideHeight;\n  const strideWidth = convInfo.strideWidth;\n  const dilationDepth = convInfo.dilationDepth;\n  const dilationHeight = convInfo.dilationHeight;\n  const dilationWidth = convInfo.dilationWidth;\n  const effectiveFilterDepth = convInfo.effectiveFilterDepth;\n  const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n  const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n  const padFront = effectiveFilterDepth - 1 - convInfo.padInfo.front;\n  const padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n  const padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n  const dx = buffer(input.shape, 'float32');\n\n  const dyBuf = backend.bufferSync<Rank, 'float32'>(dy);\n\n  for (let batch = 0; batch < convInfo.batchSize; ++batch) {\n    for (let channel = 0; channel < convInfo.inChannels; ++channel) {\n      for (let dxDepth = 0; dxDepth < convInfo.inDepth; ++dxDepth) {\n        for (let dxRow = 0; dxRow < convInfo.inHeight; ++dxRow) {\n          for (let dxCol = 0; dxCol < convInfo.inWidth; ++dxCol) {\n            // Shader code begins\n            const dyDepthCorner = dxDepth - padFront;\n            const dyRowCorner = dxRow - padTop;\n            const dyColCorner = dxCol - padLeft;\n            let dotProd = 0;\n            for (let wDepth = 0; wDepth < effectiveFilterDepth;\n                 wDepth += dilationDepth) {\n              const dyDepth = (dyDepthCorner + wDepth) / strideDepth;\n              if (dyDepth < 0 || dyDepth >= convInfo.outDepth ||\n                  Math.floor(dyDepth) !== dyDepth) {\n                continue;\n              }\n              for (let wRow = 0; wRow < effectiveFilterHeight;\n                   wRow += dilationHeight) {\n                const dyRow = (dyRowCorner + wRow) / strideHeight;\n                if (dyRow < 0 || dyRow >= convInfo.outHeight ||\n                    Math.floor(dyRow) !== dyRow) {\n                  continue;\n                }\n                for (let wCol = 0; wCol < effectiveFilterWidth;\n                     wCol += dilationWidth) {\n                  const dyCol = (dyColCorner + wCol) / strideWidth;\n                  if (dyCol < 0 || dyCol >= convInfo.outWidth ||\n                      Math.floor(dyCol) !== dyCol) {\n                    continue;\n                  }\n\n                  const maxPos = effectiveFilterDepth * effectiveFilterHeight *\n                          effectiveFilterWidth -\n                      1 -\n                      (maxPosBuf.get(batch, dyDepth, dyRow, dyCol, channel) as\n                       number);\n                  const curPos =\n                      wDepth * effectiveFilterHeight * effectiveFilterWidth +\n                      wRow * effectiveFilterWidth + wCol;\n\n                  const mask = maxPos === curPos ? 1 : 0;\n                  if (mask === 0) {\n                    continue;\n                  }\n\n                  const pixel =\n                      dyBuf.get(batch, dyDepth, dyRow, dyCol, channel);\n                  dotProd += pixel * mask;\n                }\n              }\n            }\n            dx.set(dotProd, batch, dxDepth, dxRow, dxCol, channel);\n          }\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(dx.shape, dx.dtype, dx.values);\n}\n\nexport const maxPool3DGradConfig: KernelConfig = {\n  kernelName: MaxPool3DGrad,\n  backendName: 'cpu',\n  kernelFunc: maxPool3DGrad as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {backend_util, buffer, KernelConfig, KernelFunc, MaxPoolGrad, MaxPoolGradAttrs, MaxPoolGradInputs, Rank, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {maxPoolPositions} from '../utils/pool_utils';\n\nexport function maxPoolGrad(args: {\n  inputs: MaxPoolGradInputs,\n  backend: MathBackendCPU,\n  attrs: MaxPoolGradAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {dy, input, output} = inputs;\n  const x = input;\n  assertNotComplex([input, output], 'maxPoolGrad');\n  const {filterSize, strides, pad, dimRoundingMode} = attrs;\n\n  const convInfo = backend_util.computePool2DInfo(\n      x.shape as [number, number, number, number], filterSize, strides,\n      1 /* dilations */, pad, dimRoundingMode);\n  const xValues = backend.data.get(x.dataId).values as TypedArray;\n  const maxPosBuf = buffer(\n      convInfo.outShape, x.dtype,\n      maxPoolPositions(xValues, x.shape, x.dtype, convInfo).values);\n  const strideHeight = convInfo.strideHeight;\n  const strideWidth = convInfo.strideWidth;\n  const dilationHeight = convInfo.dilationHeight;\n  const dilationWidth = convInfo.dilationWidth;\n  const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n  const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n  const padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n  const padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n  const dx =\n      buffer<Rank.R4>(x.shape as [number, number, number, number], 'float32');\n\n  const dyData = backend.data.get(dy.dataId).values as Float32Array;\n  const dyBuf = buffer<Rank.R4>(\n      dy.shape as [number, number, number, number], 'float32', dyData);\n\n  for (let b = 0; b < convInfo.batchSize; ++b) {\n    for (let d = 0; d < convInfo.inChannels; ++d) {\n      for (let dxR = 0; dxR < convInfo.inHeight; ++dxR) {\n        for (let dxC = 0; dxC < convInfo.inWidth; ++dxC) {\n          // Shader code begins.\n          const dyRCorner = dxR - padTop;\n          const dyCCorner = dxC - padLeft;\n          let dotProd = 0;\n          for (let wR = 0; wR < effectiveFilterHeight; wR += dilationHeight) {\n            const dyR = (dyRCorner + wR) / strideHeight;\n            if (dyR < 0 || dyR >= convInfo.outHeight ||\n                Math.floor(dyR) !== dyR) {\n              continue;\n            }\n            for (let wC = 0; wC < effectiveFilterWidth; wC += dilationWidth) {\n              const dyC = (dyCCorner + wC) / strideWidth;\n              if (dyC < 0 || dyC >= convInfo.outWidth ||\n                  Math.floor(dyC) !== dyC) {\n                continue;\n              }\n              const maxPos = effectiveFilterHeight * effectiveFilterWidth - 1 -\n                  (maxPosBuf.get(b, dyR, dyC, d) as number);\n              const curPos = wR * effectiveFilterWidth + wC;\n\n              const mask = maxPos === curPos ? 1 : 0;\n              if (mask === 0) {\n                continue;\n              }\n\n              const pixel = dyBuf.get(b, dyR, dyC, d);\n              dotProd += pixel * mask;\n            }\n          }\n          dx.set(dotProd, b, dxR, dxC, d);\n        }\n      }\n    }\n  }\n  return backend.makeTensorInfo(dx.shape, dx.dtype, dx.values);\n}\n\nexport const maxPoolGradConfig: KernelConfig = {\n  kernelName: MaxPoolGrad,\n  backendName: 'cpu',\n  kernelFunc: maxPoolGrad as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {backend_util, DataType, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {maxPoolPositions, pool} from '../utils/pool_utils';\nexport function maxPoolWithArgmaxImpl(\n    xValues: TypedArray, xShape: number[], dtype: DataType,\n    includeBatchInIndex: boolean, convInfo: backend_util.Conv2DInfo) {\n  const strides = util.computeStrides(xShape);\n  const maxPools = pool(xValues, xShape, dtype, strides, convInfo, 'max');\n  const maxPositions = maxPoolPositions(\n      xValues, xShape, dtype, convInfo, true, includeBatchInIndex);\n\n  return [maxPools.values, maxPositions.values];\n}\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {MaxPoolWithArgmax, MaxPoolWithArgmaxAttrs, MaxPoolWithArgmaxInputs} from '@tensorflow/tfjs-core';\nimport {backend_util, KernelConfig, TypedArray} from '@tensorflow/tfjs-core';\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nimport {maxPoolWithArgmaxImpl} from './MaxPoolWithArgmax_impl';\n\nexport const maxPoolWithArgmaxConfig: KernelConfig = {\n  kernelName: MaxPoolWithArgmax,\n  backendName: 'cpu',\n  kernelFunc: ({inputs, attrs, backend}) => {\n    const {x} = inputs as MaxPoolWithArgmaxInputs;\n    const {filterSize, strides, pad, includeBatchInIndex} =\n        attrs as {} as MaxPoolWithArgmaxAttrs;\n    const cpuBackend = backend as MathBackendCPU;\n    assertNotComplex(x, 'MaxPoolWithArgmax');\n\n    const values = cpuBackend.data.get(x.dataId).values as TypedArray;\n    const convInfo = backend_util.computePool2DInfo(\n        x.shape as [number, number, number, number], filterSize, strides,\n        [1, 1], pad);\n    const [pooled, indexes] = maxPoolWithArgmaxImpl(\n        values, x.shape, x.dtype, includeBatchInIndex, convInfo);\n\n    const pooledDataId =\n        cpuBackend.write(pooled as Float32Array, convInfo.outShape, x.dtype);\n    const indexesDataId =\n        cpuBackend.write(indexes as Int32Array, convInfo.outShape, x.dtype);\n    return [\n      {dataId: pooledDataId, shape: convInfo.outShape, dtype: x.dtype},\n      {dataId: indexesDataId, shape: convInfo.outShape, dtype: 'int32'}\n    ];\n  }\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, Mean, MeanAttrs, MeanInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {cast} from './Cast';\nimport {div} from './RealDiv';\nimport {sum} from './Sum';\n\nexport function mean(\n    args: {inputs: MeanInputs, backend: MathBackendCPU, attrs: MeanAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, keepDims} = attrs;\n\n  const axes = util.parseAxisParam(axis, x.shape);\n  const shapes = backend_util.computeOutAndReduceShapes(x.shape, axes);\n  const reduceShape = shapes[1];\n  const reduceSize = util.sizeFromShape(reduceShape);\n  const toDispose = [];\n  const reduceSizeScalar =\n      backend.makeTensorInfo([], 'float32', new Float32Array([reduceSize]));\n  toDispose.push(reduceSizeScalar);\n\n  const $x = cast({inputs: {x}, backend, attrs: {dtype: 'float32'}});\n  toDispose.push($x);\n\n  const res =\n      div({inputs: {a: $x, b: reduceSizeScalar}, backend}) as TensorInfo;\n  toDispose.push(res);\n\n  const result = sum({inputs: {x: res}, backend, attrs: {axis, keepDims}});\n\n  toDispose.forEach(t => backend.disposeIntermediateTensorInfo(t));\n\n  return result;\n}\n\nexport const meanConfig: KernelConfig = {\n  kernelName: Mean,\n  backendName: 'cpu',\n  kernelFunc: mean as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, Min, MinAttrs, MinInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {reshape} from './Reshape';\nimport {transpose} from './Transpose';\n\nexport function min(\n    args: {inputs: MinInputs, backend: MathBackendCPU, attrs: MinAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, keepDims} = attrs;\n\n  assertNotComplex(x, 'min');\n\n  const origAxes = util.parseAxisParam(axis, x.shape);\n  let axes = origAxes;\n  const permutedAxes = backend_util.getAxesPermutation(axes, x.shape.length);\n  let $x = x;\n  if (permutedAxes != null) {\n    $x = transpose({inputs: {x}, backend, attrs: {perm: permutedAxes}});\n    axes = backend_util.getInnerMostAxes(axes.length, x.shape.length);\n  }\n\n  backend_util.assertAxesAreInnerMostDims('min', axes, $x.shape.length);\n  const [outShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes($x.shape, axes);\n  const reduceSize = util.sizeFromShape(reduceShape);\n  const vals = util.makeZerosTypedArray(util.sizeFromShape(outShape), $x.dtype);\n\n  const aVals = backend.data.get($x.dataId).values as TypedArray;\n  for (let i = 0; i < vals.length; ++i) {\n    const offset = i * reduceSize;\n    let min = aVals[offset];\n    for (let j = 0; j < reduceSize; ++j) {\n      const value = aVals[offset + j];\n      if (Number.isNaN(value) ||\n          value < min) {  // comparison with NaN always return false\n        min = value;\n      }\n    }\n    vals[i] = min;\n  }\n\n  if (permutedAxes != null) {\n    backend.disposeIntermediateTensorInfo($x);\n  }\n\n  const result = backend.makeTensorInfo(outShape, $x.dtype, vals);\n\n  if (keepDims) {\n    const expandedShape = backend_util.expandShapeToKeepDim(outShape, origAxes);\n    const reshapedResult =\n        reshape({inputs: {x: result}, backend, attrs: {shape: expandedShape}});\n\n    backend.disposeIntermediateTensorInfo(result);\n\n    return reshapedResult;\n  }\n\n  return result;\n}\n\nexport const minConfig: KernelConfig = {\n  kernelName: Min,\n  backendName: 'cpu',\n  kernelFunc: min as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, MirrorPad, MirrorPadAttrs, MirrorPadInputs, NumericDataType, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function mirrorPad(args: {\n  inputs: MirrorPadInputs,\n  backend: MathBackendCPU,\n  attrs: MirrorPadAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {paddings, mode} = attrs;\n\n  assertNotComplex(x, 'mirrorPad');\n\n  const outShape = paddings.map(\n      (p, i) => p[0] /* beforePad */ + x.shape[i] + p[1] /* afterPad */);\n\n  const start = paddings.map(p => p[0]);\n  const end = paddings.map((p, i) => p[0] + x.shape[i]);\n  const offset = mode === 'reflect' ? 0 : 1;\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const xRank = x.shape.length;\n  const xStrides = util.computeStrides(x.shape);\n\n  const resultSize = util.sizeFromShape(outShape);\n  const resultRank = outShape.length;\n  const resultStrides = util.computeStrides(outShape);\n  const resVals =\n      util.getTypedArrayFromDType(x.dtype as NumericDataType, resultSize);\n\n  for (let i = 0; i < resultSize; i++) {\n    let coords = util.indexToLoc(i, resultRank, resultStrides);\n    for (let i = 0; i < resultRank; i++) {\n      if (coords[i] < start[i]) {\n        coords[i] = start[i] * 2 - coords[i] - offset;\n      } else if (coords[i] >= end[i]) {\n        coords[i] = (end[i] - 1) * 2 - coords[i] + offset;\n      }\n    }\n    coords = coords.map((c, i) => c - start[i]);\n\n    const inIndex = util.locToIndex(coords, xRank, xStrides);\n\n    resVals[i] = xVals[inIndex];\n  }\n\n  const outId = backend.write(resVals, outShape, x.dtype);\n\n  return {dataId: outId, shape: outShape, dtype: x.dtype};\n}\n\nexport const mirrorPadConfig: KernelConfig = {\n  kernelName: MirrorPad,\n  backendName: 'cpu',\n  kernelFunc: mirrorPad as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Mod} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const modImpl =\n    createSimpleBinaryKernelImpl(((aValue: number, bValue: number) => {\n      const rem = aValue % bValue;\n      if ((aValue < 0 && bValue < 0) || (aValue >= 0 && bValue >= 0)) {\n        return rem;\n      } else {\n        return (rem + bValue) % bValue;\n      }\n    }));\n\nexport const mod = binaryKernelFunc(Mod, modImpl);\n\nexport const modConfig: KernelConfig = {\n  kernelName: Mod,\n  backendName: 'cpu',\n  kernelFunc: mod\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Multinomial, MultinomialAttrs, MultinomialInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nimport {softmax} from './Softmax';\n\nexport function multinomial(args: {\n  inputs: MultinomialInputs,\n  backend: MathBackendCPU,\n  attrs: MultinomialAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {logits} = inputs;\n  const {numSamples, seed, normalized} = attrs;\n\n  assertNotComplex(logits, 'multinomial');\n\n  const probabilities = normalized ?\n      logits :\n      softmax({inputs: {logits}, backend, attrs: {dim: -1}});\n\n  const batchSize = probabilities.shape[0];\n  const numEvents = probabilities.shape[1];\n  const probVals = backend.data.get(probabilities.dataId).values as TypedArray;\n  const resShape = [batchSize, numSamples];\n  const resVals =\n      util.makeZerosTypedArray(util.sizeFromShape(resShape), 'int32');\n\n  for (let b = 0; b < batchSize; ++b) {\n    const offset = b * numEvents;\n    // The cdf won't include the last event. It will be implicit if no other\n    // event happened.\n    const cdf = new Float32Array(numEvents - 1);\n    cdf[0] = probVals[offset];\n    for (let event = 1; event < cdf.length; ++event) {\n      cdf[event] = cdf[event - 1] + probVals[offset + event];\n    }\n\n    const random = seedrandom.alea(seed.toString());\n    const outOffset = b * numSamples;\n    for (let sampleId = 0; sampleId < numSamples; ++sampleId) {\n      const r = random();\n\n      // Assume last event happened by default.\n      resVals[outOffset + sampleId] = cdf.length;\n\n      for (let event = 0; event < cdf.length; event++) {\n        if (r < cdf[event]) {\n          resVals[outOffset + sampleId] = event;\n          break;\n        }\n      }\n    }\n  }\n\n  if (!normalized) {\n    backend.disposeIntermediateTensorInfo(probabilities);\n  }\n\n  return backend.makeTensorInfo(resShape, 'int32', resVals);\n}\n\nexport const multinomialConfig: KernelConfig = {\n  kernelName: Multinomial,\n  backendName: 'cpu',\n  kernelFunc: multinomial as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, Softmax, SoftmaxAttrs, SoftmaxInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {exp} from './Exp';\nimport {max} from './Max';\nimport {div} from './RealDiv';\nimport {reshape} from './Reshape';\nimport {sub} from './Sub';\nimport {sum} from './Sum';\n\nexport function softmax(\n    args:\n        {inputs: SoftmaxInputs, backend: MathBackendCPU, attrs: SoftmaxAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {logits} = inputs;\n  const {dim} = attrs;\n\n  const logitsRank = logits.shape.length;\n\n  let $dim = dim;\n  if ($dim === -1) {\n    $dim = logitsRank - 1;\n  }\n  if ($dim !== logitsRank - 1) {\n    throw Error(\n        'Softmax along a non-last dimension is not yet supported. ' +\n        `Logits was rank ${logitsRank} and dim was ${$dim}`);\n  }\n\n  const axes = util.parseAxisParam([$dim], logits.shape);\n  const maxLogit = max({\n    inputs: {x: logits},\n    backend,\n    attrs: {reductionIndices: axes, keepDims: false}\n  });\n  const expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n\n  const maxLogitReshaped =\n      reshape({inputs: {x: maxLogit}, backend, attrs: {shape: expandedShape}});\n  const a =\n      sub({inputs: {a: logits, b: maxLogitReshaped}, backend}) as TensorInfo;\n  const b = exp({inputs: {x: a}, backend}) as TensorInfo;\n  const sumExp =\n      sum({inputs: {x: b}, backend, attrs: {axis: axes, keepDims: false}});\n  const sumReshaped =\n      reshape({inputs: {x: sumExp}, backend, attrs: {shape: expandedShape}});\n\n  const result = div({inputs: {a: b, b: sumReshaped}, backend}) as TensorInfo;\n\n  backend.disposeIntermediateTensorInfo(maxLogit);\n  backend.disposeIntermediateTensorInfo(maxLogitReshaped);\n  backend.disposeIntermediateTensorInfo(a);\n  backend.disposeIntermediateTensorInfo(b);\n  backend.disposeIntermediateTensorInfo(sumExp);\n  backend.disposeIntermediateTensorInfo(sumReshaped);\n\n  return result;\n}\n\nexport const softmaxConfig: KernelConfig = {\n  kernelName: Softmax,\n  backendName: 'cpu',\n  kernelFunc: softmax as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {kernel_impls, KernelConfig, KernelFunc, NonMaxSuppressionV3, NonMaxSuppressionV3Attrs, NonMaxSuppressionV3Inputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nconst nonMaxSuppressionV3Impl = kernel_impls.nonMaxSuppressionV3Impl;\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function nonMaxSuppressionV3(args: {\n  inputs: NonMaxSuppressionV3Inputs,\n  backend: MathBackendCPU,\n  attrs: NonMaxSuppressionV3Attrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {boxes, scores} = inputs;\n  const {maxOutputSize, iouThreshold, scoreThreshold} = attrs;\n\n  assertNotComplex(boxes, 'NonMaxSuppression');\n\n  const boxesVals = backend.data.get(boxes.dataId).values as TypedArray;\n  const scoresVals = backend.data.get(scores.dataId).values as TypedArray;\n\n  const {selectedIndices} = nonMaxSuppressionV3Impl(\n      boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);\n\n  return backend.makeTensorInfo(\n      [selectedIndices.length], 'int32', new Int32Array(selectedIndices));\n}\n\nexport const nonMaxSuppressionV3Config: KernelConfig = {\n  kernelName: NonMaxSuppressionV3,\n  backendName: 'cpu',\n  kernelFunc: nonMaxSuppressionV3 as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {kernel_impls, KernelConfig, KernelFunc, NonMaxSuppressionV4, NonMaxSuppressionV4Attrs, NonMaxSuppressionV4Inputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nconst nonMaxSuppressionV4Impl = kernel_impls.nonMaxSuppressionV4Impl;\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function nonMaxSuppressionV4(args: {\n  inputs: NonMaxSuppressionV4Inputs,\n  backend: MathBackendCPU,\n  attrs: NonMaxSuppressionV4Attrs\n}): [TensorInfo, TensorInfo] {\n  const {inputs, backend, attrs} = args;\n  const {boxes, scores} = inputs;\n  const {maxOutputSize, iouThreshold, scoreThreshold, padToMaxOutputSize} =\n      attrs;\n\n  assertNotComplex(boxes, 'NonMaxSuppressionPadded');\n\n  const boxesVals = backend.data.get(boxes.dataId).values as TypedArray;\n  const scoresVals = backend.data.get(scores.dataId).values as TypedArray;\n\n  const {selectedIndices, validOutputs} = nonMaxSuppressionV4Impl(\n      boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold,\n      padToMaxOutputSize);\n\n  return [\n    backend.makeTensorInfo(\n        [selectedIndices.length], 'int32', new Int32Array(selectedIndices)),\n    backend.makeTensorInfo([], 'int32', new Int32Array([validOutputs]))\n  ];\n}\nexport const nonMaxSuppressionV4Config: KernelConfig = {\n  kernelName: NonMaxSuppressionV4,\n  backendName: 'cpu',\n  kernelFunc: nonMaxSuppressionV4 as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {kernel_impls, KernelConfig, KernelFunc, NonMaxSuppressionV5, NonMaxSuppressionV5Attrs, NonMaxSuppressionV5Inputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nconst nonMaxSuppressionV5Impl = kernel_impls.nonMaxSuppressionV5Impl;\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function nonMaxSuppressionV5(args: {\n  inputs: NonMaxSuppressionV5Inputs,\n  backend: MathBackendCPU,\n  attrs: NonMaxSuppressionV5Attrs\n}): [TensorInfo, TensorInfo] {\n  const {inputs, backend, attrs} = args;\n  const {boxes, scores} = inputs;\n  const {maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma} = attrs;\n\n  assertNotComplex(boxes, 'NonMaxSuppressionWithScore');\n\n  const boxesVals = backend.data.get(boxes.dataId).values as TypedArray;\n  const scoresVals = backend.data.get(scores.dataId).values as TypedArray;\n\n  const maxOutputSizeVal = maxOutputSize;\n  const iouThresholdVal = iouThreshold;\n  const scoreThresholdVal = scoreThreshold;\n  const softNmsSigmaVal = softNmsSigma;\n\n  const {selectedIndices, selectedScores} = nonMaxSuppressionV5Impl(\n      boxesVals, scoresVals, maxOutputSizeVal, iouThresholdVal,\n      scoreThresholdVal, softNmsSigmaVal);\n\n  return [\n    backend.makeTensorInfo(\n        [selectedIndices.length], 'int32', new Int32Array(selectedIndices)),\n    backend.makeTensorInfo(\n        [selectedScores.length], 'float32', new Float32Array(selectedScores))\n  ];\n}\n\nexport const nonMaxSuppressionV5Config: KernelConfig = {\n  kernelName: NonMaxSuppressionV5,\n  backendName: 'cpu',\n  kernelFunc: nonMaxSuppressionV5 as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, OneHot, OneHotAttrs, OneHotInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function oneHot(\n    args: {inputs: OneHotInputs, backend: MathBackendCPU, attrs: OneHotAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {indices} = inputs;\n  const {dtype, depth, onValue, offValue} = attrs;\n\n  assertNotComplex(indices, 'oneHot');\n\n  const indicesSize = util.sizeFromShape(indices.shape);\n\n  const res = new Float32Array(indicesSize * depth);\n  res.fill(offValue);\n  const indicesVal = backend.data.get(indices.dataId).values as TypedArray;\n\n  for (let event = 0; event < indicesSize; ++event) {\n    if (indicesVal[event] >= 0 && indicesVal[event] < depth) {\n      res[event * depth + indicesVal[event]] = onValue;\n    }\n  }\n\n  return backend.makeTensorInfo([...indices.shape, depth], dtype, res);\n}\n\nexport const oneHotConfig: KernelConfig = {\n  kernelName: OneHot,\n  backendName: 'cpu',\n  kernelFunc: oneHot as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, TensorInfo, ZerosLike, ZerosLikeInputs} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {complex} from './Complex';\nimport {fill} from './Fill';\nimport {imag} from './Imag';\nimport {real} from './Real';\n\nexport function zerosLike(\n    args: {inputs: ZerosLikeInputs, backend: MathBackendCPU}): TensorInfo {\n  const {inputs, backend} = args;\n  const {x} = inputs;\n\n  if (x.dtype === 'string') {\n    throw new Error('zerosLike is not supported for string tensors');\n  } else if (x.dtype === 'complex64') {\n    const realPart = real({inputs: {input: x}, backend});\n    const r = zerosLike({inputs: {x: realPart}, backend});\n    const imagPart = imag({inputs: {input: x}, backend});\n    const i = zerosLike({inputs: {x: imagPart}, backend});\n\n    const result = complex({inputs: {real: r, imag: i}, backend});\n\n    backend.disposeIntermediateTensorInfo(realPart);\n    backend.disposeIntermediateTensorInfo(r);\n    backend.disposeIntermediateTensorInfo(imagPart);\n    backend.disposeIntermediateTensorInfo(i);\n\n    return result;\n  } else {\n    return fill({backend, attrs: {shape: x.shape, value: 0, dtype: x.dtype}});\n  }\n}\n\nexport const zerosLikeConfig: KernelConfig = {\n  kernelName: ZerosLike,\n  backendName: 'cpu',\n  kernelFunc: zerosLike as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, OnesLike, OnesLikeInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {complex} from './Complex';\nimport {fill} from './Fill';\nimport {imag} from './Imag';\nimport {real} from './Real';\nimport {zerosLike} from './ZerosLike';\n\nexport function onesLike(\n    args: {inputs: OnesLikeInputs, backend: MathBackendCPU}): TensorInfo {\n  const {inputs, backend} = args;\n  const {x} = inputs;\n\n  if (x.dtype === 'string') {\n    throw new Error('onesLike is not supported for string tensors');\n  } else if (x.dtype === 'complex64') {\n    const realPart = real({inputs: {input: x}, backend});\n    const r = onesLike({inputs: {x: realPart}, backend});\n    const imagPart = imag({inputs: {input: x}, backend});\n    const i = zerosLike({inputs: {x: imagPart}, backend});\n\n    const result = complex({inputs: {real: r, imag: i}, backend});\n\n    backend.disposeIntermediateTensorInfo(realPart);\n    backend.disposeIntermediateTensorInfo(r);\n    backend.disposeIntermediateTensorInfo(imagPart);\n    backend.disposeIntermediateTensorInfo(i);\n\n    return result;\n  } else {\n    return fill({backend, attrs: {shape: x.shape, value: 1, dtype: x.dtype}});\n  }\n}\n\nexport const onesLikeConfig: KernelConfig = {\n  kernelName: OnesLike,\n  backendName: 'cpu',\n  kernelFunc: onesLike as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Pack, PackAttrs, PackInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {concat} from './Concat';\nimport {expandDims} from './ExpandDims';\n\nexport function pack(\n    args: {inputs: PackInputs, backend: MathBackendCPU, attrs: PackAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {axis} = attrs;\n\n  if (inputs.length === 1) {\n    return expandDims(\n        {inputs: {input: inputs[0]}, backend, attrs: {dim: axis}});\n  }\n\n  const shape = inputs[0].shape;\n  const dtype = inputs[0].dtype;\n\n  inputs.forEach(t => {\n    util.assertShapesMatch(\n        shape, t.shape,\n        'All tensors passed to stack must have matching shapes');\n    util.assert(\n        dtype === t.dtype,\n        () => 'All tensors passed to stack must have matching dtypes');\n  });\n\n  const intermediateTensorInfos: TensorInfo[] = [];\n  const expandedTensors = inputs.map(t => {\n    const expandedT =\n        expandDims({inputs: {input: t}, backend, attrs: {dim: axis}});\n    intermediateTensorInfos.push(expandedT);\n    return expandedT;\n  });\n\n  const result = concat({inputs: expandedTensors, backend, attrs: {axis}});\n\n  intermediateTensorInfos.forEach(\n      t => backend.disposeIntermediateTensorInfo(t));\n\n  return result;\n}\n\nexport const packConfig: KernelConfig = {\n  kernelName: Pack,\n  backendName: 'cpu',\n  kernelFunc: pack as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, NumericDataType, PadV2, PadV2Attrs, PadV2Inputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function padV2(\n    args: {inputs: PadV2Inputs, backend: MathBackendCPU, attrs: PadV2Attrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {paddings, constantValue} = attrs;\n\n  assertNotComplex(x, 'pad');\n\n  const outShape = paddings.map(\n      (p, i) => p[0] /* beforePad */ + x.shape[i] + p[1] /* afterPad */);\n\n  const start = paddings.map(p => p[0]);\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const xSize = util.sizeFromShape(x.shape);\n  const xRank = x.shape.length;\n  const xStrides = util.computeStrides(x.shape);\n\n  const resultSize = util.sizeFromShape(outShape);\n  const resultRank = outShape.length;\n  const resultStrides = util.computeStrides(outShape);\n  const resVals =\n      util.getTypedArrayFromDType(x.dtype as NumericDataType, resultSize);\n\n  if (constantValue !== 0) {\n    resVals.fill(constantValue);\n  }\n\n  for (let i = 0; i < xSize; i++) {\n    const coords = util.indexToLoc(i, xRank, xStrides);\n    const outCoords = coords.map((c, i) => c + start[i]);\n    const outIndex = util.locToIndex(outCoords, resultRank, resultStrides);\n\n    resVals[outIndex] = xVals[i];\n  }\n\n  const outId = backend.write(resVals, outShape, x.dtype);\n\n  return {dataId: outId, shape: outShape, dtype: x.dtype};\n}\n\nexport const padV2Config: KernelConfig = {\n  kernelName: PadV2,\n  backendName: 'cpu',\n  kernelFunc: padV2 as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Pow} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const powImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => Math.pow(a, b));\nexport const pow = binaryKernelFunc(Pow, powImpl);\n\nexport const powConfig: KernelConfig = {\n  kernelName: Pow,\n  backendName: 'cpu',\n  kernelFunc: pow\n};\n", "/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, RaggedGather, RaggedGatherAttrs, RaggedGatherInputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {raggedGatherImpl} from './RaggedGather_impl';\n\nexport function raggedGather(args: {\n  inputs: RaggedGatherInputs,\n  backend: MathBackendCPU,\n  attrs: RaggedGatherAttrs\n}): TensorInfo[] {\n  const {inputs, backend, attrs} = args;\n  const {paramsNestedSplits, paramsDenseValues, indices} = inputs;\n  const {outputRaggedRank} = attrs;\n\n  const $paramsNestedSplits = paramsNestedSplits.map(\n      t => backend.data.get(t.dataId).values as TypedArray);\n  const $paramsNestedSplitsShapes = paramsNestedSplits.map(t => t.shape);\n  const $paramsDenseValues =\n      backend.data.get(paramsDenseValues.dataId).values as TypedArray;\n  const $indices = backend.data.get(indices.dataId).values as TypedArray;\n\n  const [outputNestedSplits, outputDenseValues, outputDenseValuesShape] =\n      raggedGatherImpl(\n          $paramsNestedSplits, $paramsNestedSplitsShapes, $paramsDenseValues,\n          paramsDenseValues.shape, paramsDenseValues.dtype, $indices,\n          indices.shape, outputRaggedRank);\n\n  const outputNestedSplitsTensors = outputNestedSplits.map(\n      (splits) => backend.makeTensorInfo([splits.length], 'int32', splits));\n\n  const outputDenseValuesTensor = backend.makeTensorInfo(\n      outputDenseValuesShape, paramsDenseValues.dtype, outputDenseValues);\n\n  return outputNestedSplitsTensors.concat([outputDenseValuesTensor]);\n}\n\nexport const raggedGatherConfig: KernelConfig = {\n  kernelName: RaggedGather,\n  backendName: 'cpu',\n  kernelFunc: raggedGather as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, RaggedTensorToTensor, RaggedTensorToTensorAttrs, RaggedTensorToTensorInputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {raggedTensorToTensorImpl} from './RaggedTensorToTensor_impl';\n\nexport function raggedTensorToTensor(args: {\n  inputs: RaggedTensorToTensorInputs,\n  backend: MathBackendCPU,\n  attrs: RaggedTensorToTensorAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {shape, values, defaultValue, rowPartitionTensors} = inputs;\n  const {rowPartitionTypes} = attrs;\n\n  const $shape = backend.data.get(shape.dataId).values as TypedArray;\n  const $values = backend.data.get(values.dataId).values as TypedArray;\n  const $defaultValue =\n      backend.data.get(defaultValue.dataId).values as TypedArray;\n  const $rowPartitionValues = rowPartitionTensors.map(\n      t => backend.data.get(t.dataId).values as TypedArray);\n  const rowPartitionValuesShapes = rowPartitionTensors.map(t => t.shape);\n\n  const [outputShape, output] = raggedTensorToTensorImpl(\n      $shape, shape.shape, $values, values.shape, values.dtype, $defaultValue,\n      defaultValue.shape, $rowPartitionValues, rowPartitionValuesShapes,\n      rowPartitionTypes);\n  return backend.makeTensorInfo(outputShape, values.dtype, output);\n}\n\nexport const raggedTensorToTensorConfig: KernelConfig = {\n  kernelName: RaggedTensorToTensor,\n  backendName: 'cpu',\n  kernelFunc: raggedTensorToTensor as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Range, RangeAttrs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {rangeImpl} from './Range_impl';\n\nexport function range(args: {backend: MathBackendCPU, attrs: RangeAttrs}):\n    TensorInfo {\n  const {backend, attrs} = args;\n  const {start, stop, dtype, step} = attrs;\n\n  const values = rangeImpl(start, stop, step, dtype);\n  return backend.makeTensorInfo([values.length], dtype, values);\n}\n\nexport const rangeConfig: KernelConfig = {\n  kernelName: Range,\n  backendName: 'cpu',\n  kernelFunc: range as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Reciprocal} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const reciprocal = unaryKernelFunc(Reciprocal, (xi) => 1 / xi);\n\nexport const reciprocalConfig: KernelConfig = {\n  kernelName: Reciprocal,\n  backendName: 'cpu',\n  kernelFunc: reciprocal,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, ResizeBilinear, ResizeBilinearAttrs, ResizeBilinearInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function resizeBilinear(args: {\n  inputs: ResizeBilinearInputs,\n  backend: MathBackendCPU,\n  attrs: ResizeBilinearAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {images} = inputs;\n  const {alignCorners, halfPixelCenters, size} = attrs;\n\n  assertNotComplex(images, 'resizeBilinear');\n\n  const imagesStrides = util.computeStrides(images.shape);\n  const [newHeight, newWidth] = size;\n\n  const [batch, oldHeight, oldWidth, numChannels] = images.shape;\n  const xValues = backend.data.get(images.dataId).values as TypedArray;\n  const result = new Float32Array(\n      util.sizeFromShape([batch, newHeight, newWidth, numChannels]));\n\n  const effectiveInputSize: [number, number] = [\n    (alignCorners && newHeight > 1) ? oldHeight - 1 : oldHeight,\n    (alignCorners && newWidth > 1) ? oldWidth - 1 : oldWidth\n  ];\n\n  const effectiveOutputSize: [number, number] = [\n    (alignCorners && newHeight > 1) ? newHeight - 1 : newHeight,\n    (alignCorners && newWidth > 1) ? newWidth - 1 : newWidth\n  ];\n  let outputIdx = 0;\n  const effectiveRowSizeRatio = effectiveInputSize[0] / effectiveOutputSize[0];\n  const effectiveColSizeRatio = effectiveInputSize[1] / effectiveOutputSize[1];\n  for (let b = 0; b < batch; b++) {\n    for (let r = 0; r < newHeight; r++) {\n      let sourceFracRow: number;\n      if (halfPixelCenters) {\n        sourceFracRow = effectiveRowSizeRatio * (r + 0.5) - 0.5;\n      } else {\n        sourceFracRow = effectiveRowSizeRatio * r;\n      }\n\n      const sourceRowFloor = Math.max(0, Math.floor(sourceFracRow));\n      const rowFrac = sourceFracRow - sourceRowFloor;\n      const sourceRowCeil = Math.min(oldHeight - 1, Math.ceil(sourceFracRow));\n      const topRowOffset =\n          b * imagesStrides[0] + sourceRowFloor * imagesStrides[1];\n      const botRowOffset =\n          b * imagesStrides[0] + sourceRowCeil * imagesStrides[1];\n      for (let c = 0; c < newWidth; c++) {\n        let sourceFracCol: number;\n        if (halfPixelCenters) {\n          sourceFracCol = effectiveColSizeRatio * (c + 0.5) - 0.5;\n        } else {\n          sourceFracCol = effectiveColSizeRatio * c;\n        }\n        const sourceColFloor = Math.max(0, Math.floor(sourceFracCol));\n        const colFrac = sourceFracCol - sourceColFloor;\n        const sourceColCeil = Math.min(oldWidth - 1, Math.ceil(sourceFracCol));\n        const topLeftOffest = topRowOffset + sourceColFloor * imagesStrides[2];\n        const botLeftOffset = botRowOffset + sourceColFloor * imagesStrides[2];\n        const topRightOffset = topRowOffset + sourceColCeil * imagesStrides[2];\n        const botRightOffest = botRowOffset + sourceColCeil * imagesStrides[2];\n        for (let d = 0; d < numChannels; d++) {\n          // Begin shader.\n\n          // Compute the fractional index of the source.\n          const topLeft = xValues[topLeftOffest + d];\n          const bottomLeft = xValues[botLeftOffset + d];\n          const topRight = xValues[topRightOffset + d];\n          const bottomRight = xValues[botRightOffest + d];\n\n          const top = topLeft + (topRight - topLeft) * colFrac;\n          const bottom = bottomLeft + (bottomRight - bottomLeft) * colFrac;\n          const newValue = top + (bottom - top) * rowFrac;\n\n          result[outputIdx++] = newValue;\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(\n      [batch, newHeight, newWidth, numChannels], 'float32', result);\n}\n\nexport const resizeBilinearConfig: KernelConfig = {\n  kernelName: ResizeBilinear,\n  backendName: 'cpu',\n  kernelFunc: resizeBilinear as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, ResizeBilinearGrad, ResizeBilinearGradAttrs, ResizeBilinearGradInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function resizeBilinearGrad(args: {\n  inputs: ResizeBilinearGradInputs,\n  backend: MathBackendCPU,\n  attrs: ResizeBilinearGradAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {images, dy} = inputs;\n  const {alignCorners} = attrs;\n\n  assertNotComplex([dy, images], 'resizeBilinearGrad');\n\n  const imagesStrides = util.computeStrides(images.shape);\n\n  const [batch, xHeight, xWidth, depth] = images.shape;\n  const [, yHeight, yWidth] = dy.shape;\n\n  const output = new Float32Array(batch * xHeight * xWidth * depth);\n\n  // In the backwards pass, we want to find the pixels that were generated\n  // for each pixel in the input image the forward pass and add the\n  // corresponding coefficient from dy to the gradient (with some\n  // interpolation).\n\n  const effectiveXSize: [number, number] = [\n    (alignCorners && yHeight > 1) ? xHeight - 1 : xHeight,\n    (alignCorners && yWidth > 1) ? xWidth - 1 : xWidth\n  ];\n\n  const effectiveYSize: [number, number] = [\n    (alignCorners && yHeight > 1) ? yHeight - 1 : yHeight,\n    (alignCorners && yWidth > 1) ? yWidth - 1 : yWidth\n  ];\n\n  const heightScale = effectiveXSize[0] / effectiveYSize[0];\n  const widthScale = effectiveXSize[1] / effectiveYSize[1];\n\n  // Reference implementation\n  // tslint:disable-next-line:max-line-length\n  // https://github.com/tensorflow/tensorflow/blob/3039375c86a5bbc9610c7725dcaa95d635f87ba2/tensorflow/core/kernels/resize_bilinear_op.cc#L275\n  const dyValues = backend.data.get(dy.dataId).values as TypedArray;\n  let offset = 0;\n  for (let b = 0; b < batch; b++) {\n    const bOffset = b * imagesStrides[0];\n    for (let r = 0; r < yHeight; r++) {\n      const dxR = r * heightScale;\n      const topDxRIndex = Math.floor(dxR);\n      const bottomDxRIndex = Math.min(Math.ceil(dxR), xHeight - 1);\n\n      const topDxROffset = bOffset + topDxRIndex * imagesStrides[1];\n      const bottomDxROffset = bOffset + bottomDxRIndex * imagesStrides[1];\n\n      const dxRLerp = dxR - topDxRIndex;\n      const inverseDxRLerp = 1.0 - dxRLerp;\n      for (let c = 0; c < yWidth; c++) {\n        const dxC = c * widthScale;\n        const leftDxCIndex = Math.floor(dxC);\n        const rightDxCIndex = Math.min(Math.ceil(dxC), xWidth - 1);\n        const dxCLerp = dxC - leftDxCIndex;\n        const inverseDxCLerp = 1.0 - dxCLerp;\n\n        const topLeftRCOffset = topDxROffset + leftDxCIndex * imagesStrides[2];\n        const topRightRCOffset =\n            topDxROffset + rightDxCIndex * imagesStrides[2];\n        const bottomLeftRCOffset =\n            bottomDxROffset + leftDxCIndex * imagesStrides[2];\n        const bottomRightRCOffset =\n            bottomDxROffset + rightDxCIndex * imagesStrides[2];\n\n        const inverseDxRLerpTimesInverseDxCLerp =\n            inverseDxRLerp * inverseDxCLerp;\n        const inverseDxRLerpTimesDxCLerp = inverseDxRLerp * dxCLerp;\n        const dxRLerpTimesInverseDxCLerp = dxRLerp * inverseDxCLerp;\n        const dxRLerpTimesDxCLerp = dxRLerp * dxCLerp;\n        for (let d = 0; d < depth; d++) {\n          const dyVal = dyValues[offset++];\n          output[topLeftRCOffset + d] +=\n              dyVal * inverseDxRLerpTimesInverseDxCLerp;\n          output[topRightRCOffset + d] += dyVal * inverseDxRLerpTimesDxCLerp;\n          output[bottomLeftRCOffset + d] += dyVal * dxRLerpTimesInverseDxCLerp;\n          output[bottomRightRCOffset + d] += dyVal * dxRLerpTimesDxCLerp;\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(\n      [batch, xWidth, xHeight, depth], 'float32', output);\n}\n\nexport const resizeBilinearGradConfig: KernelConfig = {\n  kernelName: ResizeBilinearGrad,\n  backendName: 'cpu',\n  kernelFunc: resizeBilinearGrad as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, ResizeNearestNeighbor, ResizeNearestNeighborAttrs, ResizeNearestNeighborInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function resizeNearestNeighbor(args: {\n  inputs: ResizeNearestNeighborInputs,\n  backend: MathBackendCPU,\n  attrs: ResizeNearestNeighborAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {images} = inputs;\n  const {alignCorners, halfPixelCenters, size} = attrs;\n\n  assertNotComplex(images, 'resizeNearestNeighbor');\n\n  const imagesStrides = util.computeStrides(images.shape);\n  const [newHeight, newWidth] = size;\n\n  const [batch, oldHeight, oldWidth, numChannels] = images.shape;\n  const xValues = backend.data.get(images.dataId).values as TypedArray;\n  const output = new Float32Array(batch * newHeight * newWidth * numChannels);\n\n  const effectiveInputSize: [number, number] = [\n    (alignCorners && newHeight > 1) ? oldHeight - 1 : oldHeight,\n    (alignCorners && newWidth > 1) ? oldWidth - 1 : oldWidth\n  ];\n\n  const effectiveOutputSize: [number, number] = [\n    (alignCorners && newHeight > 1) ? newHeight - 1 : newHeight,\n    (alignCorners && newWidth > 1) ? newWidth - 1 : newWidth\n  ];\n\n  const effectiveRowSizeRatio = effectiveInputSize[0] / effectiveOutputSize[0];\n  const effectiveColSizeRatio = effectiveInputSize[1] / effectiveOutputSize[1];\n\n  let outputOffset = 0;\n  for (let b = 0; b < batch; b++) {\n    const batchOffset = b * imagesStrides[0];\n    for (let r = 0; r < newHeight; r++) {\n      const sourceFracRow = halfPixelCenters ?\n          effectiveRowSizeRatio * (r + 0.5) :\n          effectiveRowSizeRatio * r;\n      let sourceNearestRow = Math.min(\n          oldHeight - 1,\n          alignCorners ? Math.round(sourceFracRow) : Math.floor(sourceFracRow));\n      if (halfPixelCenters) {\n        sourceNearestRow = Math.max(0, sourceNearestRow);\n      }\n      const rowOffset = batchOffset + sourceNearestRow * imagesStrides[1];\n      for (let c = 0; c < newWidth; c++) {\n        const sourceFracCol = halfPixelCenters ?\n            effectiveColSizeRatio * (c + 0.5) :\n            effectiveColSizeRatio * c;\n        let sourceNearestCol = Math.min(\n            oldWidth - 1,\n            alignCorners ? Math.round(sourceFracCol) :\n                           Math.floor(sourceFracCol));\n        if (halfPixelCenters) {\n          sourceNearestCol = Math.max(0, sourceNearestCol);\n        }\n        const colOffset = rowOffset + sourceNearestCol * imagesStrides[2];\n        for (let d = 0; d < numChannels; d++) {\n          // Begin shader.\n          // Compute the fractional index of the source.\n          const newVal = xValues[colOffset + d];\n          output[outputOffset++] = newVal;\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(\n      [batch, newHeight, newWidth, numChannels], images.dtype, output);\n}\n\nexport const resizeNearestNeighborConfig: KernelConfig = {\n  kernelName: ResizeNearestNeighbor,\n  backendName: 'cpu',\n  kernelFunc: resizeNearestNeighbor as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, ResizeNearestNeighborGrad, ResizeNearestNeighborGradAttrs, ResizeNearestNeighborGradInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function resizeNearestNeighborGrad(args: {\n  inputs: ResizeNearestNeighborGradInputs,\n  backend: MathBackendCPU,\n  attrs: ResizeNearestNeighborGradAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {images, dy} = inputs;\n  const {alignCorners} = attrs;\n\n  assertNotComplex([dy, images], 'resizeNearestNeighborGrad');\n\n  const imagesStrides = util.computeStrides(images.shape);\n  const dyStrides = util.computeStrides(dy.shape);\n  const [batch, xHeight, xWidth, depth] = images.shape;\n  const [, yHeight, yWidth] = dy.shape;\n\n  const output = new Float32Array(batch * xHeight * xWidth * depth);\n  const dyValues = backend.data.get(dy.dataId).values as TypedArray;\n\n  // In the backwards pass, we want to find the pixels that were generated\n  // for each pixel in the input image the forward pass\n\n  const effectiveXSize: [number, number] = [\n    (alignCorners && yHeight > 1) ? xHeight - 1 : xHeight,\n    (alignCorners && yWidth > 1) ? xWidth - 1 : xWidth\n  ];\n\n  const effectiveYSize: [number, number] = [\n    (alignCorners && yHeight > 1) ? yHeight - 1 : yHeight,\n    (alignCorners && yWidth > 1) ? yWidth - 1 : yWidth\n  ];\n\n  const heightScale = effectiveXSize[0] / effectiveYSize[0];\n  const widthScale = effectiveXSize[1] / effectiveYSize[1];\n\n  const invHeightScale = 1 / heightScale;\n  const invWidthScale = 1 / widthScale;\n\n  // This defines the size of the window of values around a particular\n  // index in dy that we want to search for contributions to dx.\n  const winHeight = (Math.ceil(invHeightScale) * 2) + 2;\n  const winWidth = (Math.ceil(invWidthScale) * 2) + 2;\n\n  // Loop over the output space.\n  for (let b = 0; b < batch; b++) {\n    const batchOffset = b * imagesStrides[0];\n    for (let r = 0; r < xHeight; r++) {\n      const rowOffset = batchOffset + r * imagesStrides[1];\n\n      // Compute bounds for where in dy we will look\n      const startRLerp = Math.floor(r * invHeightScale);\n      const startDyR = Math.floor(startRLerp - (winHeight / 2));\n      for (let c = 0; c < xWidth; c++) {\n        const colOffset = rowOffset + c * imagesStrides[2];\n\n        // Compute bounds for where in dy we will look\n        const startCLerp = Math.floor(c * invWidthScale);\n        const startDyC = Math.floor(startCLerp - (winWidth / 2));\n\n        for (let d = 0; d < depth; d++) {\n          let accum = 0;\n          // loop over dy\n\n          for (let dyRIndex = 0; dyRIndex < winHeight; dyRIndex++) {\n            const dyR = dyRIndex + startDyR;\n            // Guard against the window exceeding the bounds of dy\n            if (dyR < 0 || dyR >= yHeight) {\n              continue;\n            }\n\n            const dyROffset = batchOffset + dyR * dyStrides[1];\n            const sourceFracRow = dyR * heightScale;\n            const sourceNearestRow = Math.min(\n                xHeight - 1,\n                alignCorners ? Math.round(sourceFracRow) :\n                               Math.floor(sourceFracRow));\n            if (r !== sourceNearestRow) {\n              continue;\n            }\n            for (let dyCIndex = 0; dyCIndex < winWidth; dyCIndex++) {\n              const dyC = dyCIndex + startDyC;\n              // Guard against the window exceeding the bounds of dy\n              if (dyC < 0 || dyC >= yWidth) {\n                continue;\n              }\n\n              const dyCOffset = dyROffset + dyC * dyStrides[2];\n              const sourceFracCol = dyC * widthScale;\n              const sourceNearestCol = Math.min(\n                  xWidth - 1,\n                  alignCorners ? Math.round(sourceFracCol) :\n                                 Math.floor(sourceFracCol));\n\n              if (c === sourceNearestCol) {\n                accum += dyValues[dyCOffset + d];\n              }\n            }\n          }\n          output[colOffset + d] = accum;\n        }\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(images.shape, images.dtype, output);\n}\n\nexport const resizeNearestNeighborGradConfig: KernelConfig = {\n  kernelName: ResizeNearestNeighborGrad,\n  backendName: 'cpu',\n  kernelFunc: resizeNearestNeighborGrad as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Reverse, ReverseAttrs, ReverseInputs, TensorBuffer, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {identity} from './Identity';\n\nexport function reverse(\n    args:\n        {inputs: ReverseInputs, backend: MathBackendCPU, attrs: ReverseAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {dims} = attrs;\n\n  assertNotComplex(x, 'reverse');\n\n  const xRank = x.shape.length;\n\n  const $dims = util.parseAxisParam(dims, x.shape);\n  if (xRank === 0) {\n    return identity({inputs: {x}, backend});\n  }\n\n  const outBuf = new TensorBuffer(x.shape, x.dtype);\n  const xBuf = backend.bufferSync(x);\n\n  for (let i = 0; i < outBuf.size; i++) {\n    const outLoc = outBuf.indexToLoc(i);\n    const inLoc = outLoc.slice();\n    $dims.forEach(d => inLoc[d] = x.shape[d] - 1 - inLoc[d]);\n    outBuf.set(xBuf.get(...inLoc), ...outLoc);\n  }\n\n  return backend.makeTensorInfo(outBuf.shape, outBuf.dtype, outBuf.values);\n}\n\nexport const reverseConfig: KernelConfig = {\n  kernelName: Reverse,\n  backendName: 'cpu',\n  kernelFunc: reverse as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, NumericDataType, TypedArray} from '@tensorflow/tfjs-core';\nimport {backend_util, RotateWithOffset, RotateWithOffsetAttrs, RotateWithOffsetInputs, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport const rotateWithOffsetConfig: KernelConfig = {\n  kernelName: RotateWithOffset,\n  backendName: 'cpu',\n  kernelFunc: ({inputs, attrs, backend}) => {\n    const {image} = inputs as RotateWithOffsetInputs;\n    const {radians, fillValue, center} = attrs as {} as RotateWithOffsetAttrs;\n    const cpuBackend = backend as MathBackendCPU;\n\n    const output = util.getTypedArrayFromDType(\n        image.dtype as NumericDataType, util.sizeFromShape(image.shape));\n    const [batch, imageHeight, imageWidth, numChannels] = image.shape;\n\n    const [centerX, centerY] =\n        backend_util.getImageCenter(center, imageHeight, imageWidth);\n    const fullOpacityValue = 255;\n\n    const sinFactor = Math.sin(radians);\n    const cosFactor = Math.cos(radians);\n    const imageVals = cpuBackend.data.get(image.dataId).values as TypedArray;\n\n    for (let batchIdx = 0; batchIdx < batch; batchIdx++) {\n      const batchOffset = batchIdx * imageWidth * imageHeight * numChannels;\n\n      for (let row = 0; row < imageHeight; row++) {\n        const rowOffset = row * (imageWidth * numChannels);\n\n        for (let col = 0; col < imageWidth; col++) {\n          const colOffset = col * numChannels;\n\n          for (let channel = 0; channel < numChannels; channel++) {\n            const coords = [batch, row, col, channel];\n\n            const x = coords[2];\n            const y = coords[1];\n\n            // coordX/coordY are the result of rotating and translating x/y.\n            let coordX = (x - centerX) * cosFactor - (y - centerY) * sinFactor;\n            let coordY = (x - centerX) * sinFactor + (y - centerY) * cosFactor;\n            coordX = Math.round(coordX + centerX);\n            coordY = Math.round(coordY + centerY);\n\n            let outputValue = fillValue;\n            if (typeof fillValue !== 'number') {\n              if (channel === 3) {\n                outputValue = fullOpacityValue;\n              } else {\n                outputValue = fillValue[channel];\n              }\n            }\n\n            // If the coordinate position falls within the image boundaries...\n            if (coordX >= 0 && coordX < imageWidth && coordY >= 0 &&\n                coordY < imageHeight) {\n              // set the output to the image value at the coordinate position.\n              const rotatedRowOffset = coordY * (imageWidth * numChannels);\n              const rotatedColOffset = coordX * numChannels;\n              const imageIdx =\n                  batchOffset + rotatedRowOffset + rotatedColOffset + channel;\n              outputValue = imageVals[imageIdx];\n            }\n\n            const outIdx = batchOffset + rowOffset + colOffset + channel;\n            output[outIdx] = outputValue as number;\n          }\n        }\n      }\n    }\n\n    const dataId = cpuBackend.write(output, image.shape, image.dtype);\n    return {dataId, shape: image.shape, dtype: image.dtype};\n  }\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Round} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const round = unaryKernelFunc(Round, (xi) => {\n  // The algorithm is based on banker's rounding.\n  const base = Math.floor(xi);\n  if (xi - base < 0.5) {\n    return Math.floor(xi);\n  } else if (xi - base > 0.5) {\n    return Math.ceil(xi);\n  } else {\n    if (base % 2.0 === 0.0) {\n      return base;\n    } else {\n      return base + 1.0;\n    }\n  }\n});\n\nexport const roundConfig: KernelConfig = {\n  kernelName: Round,\n  backendName: 'cpu',\n  kernelFunc: round,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, Rank, ScatterNd, ScatterNdAttrs, ScatterNdInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {scatterImpl} from './Scatter_impl';\n\nexport function scatterNd(args: {\n  inputs: ScatterNdInputs,\n  backend: MathBackendCPU,\n  attrs: ScatterNdAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {indices, updates} = inputs;\n  const {shape} = attrs;\n\n  const {sliceRank, numUpdates, sliceSize, strides, outputSize} =\n      backend_util.calculateShapes(updates, indices, shape);\n  const sumDupeIndices = true;\n\n  const indicesBuf = backend.bufferSync<Rank, 'int32'>(indices);\n  const updatesBuf = backend.bufferSync<Rank, 'int32'|'float32'>(updates);\n\n  const outBuf = scatterImpl(\n      indicesBuf, updatesBuf, shape, outputSize, sliceSize, numUpdates,\n      sliceRank, strides, 0 /* defaultValue */, sumDupeIndices);\n\n  return backend.makeTensorInfo(shape, outBuf.dtype, outBuf.values);\n}\n\nexport const scatterNdConfig: KernelConfig = {\n  kernelName: ScatterNd,\n  backendName: 'cpu',\n  kernelFunc: scatterNd as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {TypedArray, util} from '@tensorflow/tfjs-core';\n\nfunction lowerBound(array: TypedArray, value: number) {\n  let left = 0;\n  let right = array.length;\n  let mid = 0;\n  while (left < right) {\n    mid = Math.floor((left + right) / 2);\n    if (array[mid] < value) {\n      left = mid + 1;\n    } else {\n      right = mid;\n    }\n  }\n  return right;\n}\n\nfunction upperBound(array: TypedArray, value: number) {\n  let left = 0;\n  let right = array.length;\n  let mid = 0;\n  while (left < right) {\n    mid = Math.floor((left + right) / 2);\n    if (array[mid] <= value) {\n      left = mid + 1;\n    } else {\n      right = mid;\n    }\n  }\n  return right;\n}\n\nexport function searchSortedImpl(\n    sortedInputs: TypedArray, values: TypedArray, batchSize: number,\n    numInputs: number, numValues: number, side: 'left'|'right'): TypedArray {\n  const output =\n      util.getArrayFromDType('int32', batchSize * numValues) as TypedArray;\n  for (let b = 0; b < batchSize; ++b) {\n    const sortedInputsSlice =\n        sortedInputs.slice(b * numInputs, (b + 1) * numInputs);\n    const outputOffset = b * numValues;\n    for (let i = 0; i < numValues; ++i) {\n      output[outputOffset + i] = side === 'left' ?\n          lowerBound(sortedInputsSlice, values[i + outputOffset]) :\n          upperBound(sortedInputsSlice, values[i + outputOffset]);\n    }\n  }\n  return output;\n}\n", "/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, SearchSorted, SearchSortedAttrs, SearchSortedInputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {searchSortedImpl} from './SearchSorted_impl';\n\nexport function searchSorted(args: {\n  inputs: SearchSortedInputs,\n  backend: MathBackendCPU,\n  attrs: SearchSortedAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {sortedSequence, values} = inputs;\n  const {side} = attrs;\n\n  const $sortedSequence =\n      backend.data.get(sortedSequence.dataId).values as TypedArray;\n  const $values = backend.data.get(values.dataId).values as TypedArray;\n\n  const output = searchSortedImpl(\n      $sortedSequence, $values, sortedSequence.shape[0],\n      sortedSequence.shape[1], values.shape[1], side);\n  return backend.makeTensorInfo(values.shape, 'int32', output);\n}\n\nexport const searchSortedConfig: KernelConfig = {\n  kernelName: SearchSorted,\n  backendName: 'cpu',\n  kernelFunc: searchSorted as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Select, SelectInputs, TensorInfo, TypedArray, upcastType, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function select(args: {inputs: SelectInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {condition, t, e} = inputs;\n\n  assertNotComplex([condition, t, e], 'select');\n  const conditionRank = condition.shape.length;\n\n  const values = backend.data.get(condition.dataId).values as TypedArray;\n  const tValues = backend.data.get(t.dataId).values as TypedArray;\n  const eValues = backend.data.get(e.dataId).values as TypedArray;\n  const resultDtype = upcastType(t.dtype, e.dtype);\n  const newValues =\n      util.makeZerosTypedArray(util.sizeFromShape(t.shape), resultDtype);\n\n  let index = 0;\n  const offset =\n      conditionRank === 0 || conditionRank > 1 || t.shape.length === 1 ?\n      1 :\n      util.sizeFromShape(t.shape.slice(1));\n\n  for (let i = 0; i < values.length; i++) {\n    for (let j = 0; j < offset; j++) {\n      if (values[i] === 1) {\n        newValues[index++] = tValues[i];\n      } else {\n        newValues[index++] = eValues[i];\n      }\n    }\n  }\n\n  return backend.makeTensorInfo(t.shape, resultDtype, newValues);\n}\n\nexport const selectConfig: KernelConfig = {\n  kernelName: Select,\n  backendName: 'cpu',\n  kernelFunc: select as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, Selu} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nconst scaleAlpha = backend_util.SELU_SCALEALPHA;\nconst scale = backend_util.SELU_SCALE;\n\nexport const selu = unaryKernelFunc(Selu, (xi) => {\n  if (xi >= 0) {\n    return scale * xi;\n  } else {\n    return scaleAlpha * (Math.exp(xi) - 1);\n  }\n});\n\nexport const seluConfig: KernelConfig = {\n  kernelName: Selu,\n  backendName: 'cpu',\n  kernelFunc: selu,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Sign} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const sign = unaryKernelFunc(Sign, (xi) => {\n  if (xi < 0) {\n    return -1;\n  } else if (xi > 0) {\n    return 1;\n  } else {\n    return 0;\n  }\n});\n\nexport const signConfig: KernelConfig = {\n  kernelName: Sign,\n  backendName: 'cpu',\n  kernelFunc: sign,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Sin} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const sin = unaryKernelFunc(Sin, (xi) => Math.sin(xi));\n\nexport const sinConfig: KernelConfig = {\n  kernelName: Sin,\n  backendName: 'cpu',\n  kernelFunc: sin,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Sinh} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const sinh = unaryKernelFunc(Sinh, (xi) => Math.sinh(xi));\n\nexport const sinhConfig: KernelConfig = {\n  kernelName: Sinh,\n  backendName: 'cpu',\n  kernelFunc: sinh,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Softplus} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\n// mirrors the implementation of tf.nn.softplus: https://goo.gl/vkcvwX\n\n// epsilon is the difference between 1.0 and the next representable float.\n// For a single precision 32 bit float this should be 2^-23, see:\n// https://math.byu.edu/~schow/work/IEEEFloatingPoint.htm\nconst epsilon = 1.1920928955078125e-7;\nconst threshold = Math.log(epsilon) + 2.0;\n\nexport const softplus = unaryKernelFunc(Softplus, (xi) => {\n  // Value above which exp(x) may overflow, but softplus(x) == x\n  // is within machine epsilon.\n  const tooLarge = xi > -threshold;\n\n  // Value below which exp(x) may underflow, but softplus(x) == exp(x)\n  // is within machine epsilon.\n  const tooSmall = xi < threshold;\n\n  const expX = Math.exp(xi);\n  let result;\n\n  if (tooSmall) {\n    result = expX;\n  } else if (tooLarge) {\n    result = xi;\n  } else {\n    result = Math.log(1.0 + expX);\n  }\n  return result;\n});\n\nexport const softplusConfig: KernelConfig = {\n  kernelName: Softplus,\n  backendName: 'cpu',\n  kernelFunc: softplus,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, ReshapeAttrs, ReshapeInputs, SpaceToBatchND, SpaceToBatchNDAttrs, SpaceToBatchNDInputs, TensorInfo, TransposeAttrs, TransposeInputs, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nimport {padV2Config} from './PadV2';\nimport {reshape} from './Reshape';\nimport {transpose} from './Transpose';\n\nexport function spaceToBatchND(args: {\n  inputs: SpaceToBatchNDInputs,\n  backend: MathBackendCPU,\n  attrs: SpaceToBatchNDAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {blockShape, paddings} = attrs;\n\n  assertNotComplex([x], 'spaceToBatchND');\n\n  const prod = util.sizeFromShape(blockShape);\n\n  const completePaddings: Array<[number, number]> = [[0, 0]];\n  completePaddings.push(...(paddings as Array<[number, number]>));\n\n  for (let i = 1 + blockShape.length; i < x.shape.length; ++i) {\n    completePaddings.push([0, 0]);\n  }\n\n  const paddedX = padV2Config.kernelFunc({\n    inputs: {x},\n    backend,\n    attrs: {paddings: completePaddings, constantValue: 0}\n  }) as TensorInfo;\n\n  const reshapedPaddedShape =\n      backend_util.getReshaped(paddedX.shape, blockShape, prod, false);\n\n  const permutedReshapedPaddedPermutation = backend_util.getPermuted(\n      reshapedPaddedShape.length, blockShape.length, false);\n\n  const flattenShape =\n      backend_util.getReshapedPermuted(paddedX.shape, blockShape, prod, false);\n\n  const reshapeInputs: ReshapeInputs = {x: paddedX};\n  const reshapeAttrs: ReshapeAttrs = {shape: reshapedPaddedShape};\n  const paddedXReshaped =\n      reshape({inputs: reshapeInputs, backend, attrs: reshapeAttrs});\n\n  const transposeInputs: TransposeInputs = {x: paddedXReshaped};\n  const transposeAttrs:\n      TransposeAttrs = {perm: permutedReshapedPaddedPermutation};\n  const paddedXT =\n      transpose({inputs: transposeInputs, backend, attrs: transposeAttrs});\n\n  const resultReshapeInputs: ReshapeInputs = {x: paddedXT};\n  const resultReshapeAttrs: ReshapeAttrs = {shape: flattenShape};\n  const result = reshape(\n      {inputs: resultReshapeInputs, backend, attrs: resultReshapeAttrs});\n\n  backend.disposeIntermediateTensorInfo(paddedX);\n  backend.disposeIntermediateTensorInfo(paddedXReshaped);\n  backend.disposeIntermediateTensorInfo(paddedXT);\n\n  return result;\n}\n\nexport const spaceToBatchNDConfig: KernelConfig = {\n  kernelName: SpaceToBatchND,\n  backendName: 'cpu',\n  kernelFunc: spaceToBatchND as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, SparseFillEmptyRows, SparseFillEmptyRowsInputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {sparseFillEmptyRowsImpl} from './SparseFillEmptyRows_impl';\n\nexport function sparseFillEmptyRows(args: {\n  inputs: SparseFillEmptyRowsInputs,\n  backend: MathBackendCPU\n}): [TensorInfo, TensorInfo, TensorInfo, TensorInfo] {\n  const {inputs, backend} = args;\n  const {indices, values, denseShape, defaultValue} = inputs;\n  if (denseShape.shape.length !== 1) {\n    throw new Error(`Dense shape must be a vector, saw:\n        ${denseShape.shape}`);\n  }\n  if (indices.shape.length !== 2) {\n    throw new Error(`Indices must be a matrix, saw:\n        ${indices.shape}`);\n  }\n  if (values.shape.length !== 1) {\n    throw new Error(`Values must be a vector, saw:\n        ${values.shape}`);\n  }\n  if (defaultValue.shape.length !== 0) {\n    throw new Error(`Default value must be a scalar, saw:\n        ${defaultValue.shape}`);\n  }\n\n  const $indices = backend.data.get(indices.dataId).values as TypedArray;\n  const $values = backend.data.get(values.dataId).values as TypedArray;\n  const $denseShape = backend.data.get(denseShape.dataId).values as TypedArray;\n  const $defaultValue =\n      backend.data.get(defaultValue.dataId).values[0] as number;\n\n  const [outputIndices, outputIndicesShape, outputValues,\n         emptyRowIndicator, reverseIndexMap] =\n      sparseFillEmptyRowsImpl(\n          $indices, indices.shape, indices.dtype, $values, values.dtype,\n          $denseShape, $defaultValue);\n  return [\n    backend.makeTensorInfo(outputIndicesShape, indices.dtype, outputIndices),\n    backend.makeTensorInfo(\n        [outputIndicesShape[0]], values.dtype, outputValues),\n    backend.makeTensorInfo(\n        [emptyRowIndicator.length], 'bool',\n        new Uint8Array(\n            emptyRowIndicator.map((value: boolean) => Number(value)))),\n    backend.makeTensorInfo(\n        [reverseIndexMap.length], indices.dtype,\n        new Int32Array(reverseIndexMap)),\n  ];\n}\n\nexport const sparseFillEmptyRowsConfig: KernelConfig = {\n  kernelName: SparseFillEmptyRows,\n  backendName: 'cpu',\n  kernelFunc: sparseFillEmptyRows as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, SparseReshape, SparseReshapeInputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {sparseReshapeImpl} from './SparseReshape_impl';\n\nexport function sparseReshape(\n    args: {inputs: SparseReshapeInputs, backend: MathBackendCPU}):\n    [TensorInfo, TensorInfo] {\n  const {inputs, backend} = args;\n  const {inputIndices, inputShape, newShape} = inputs;\n  if (inputIndices.shape.length !== 2) {\n    throw new Error(`Input indices should be a matrix but received shape\n        ${inputIndices.shape}`);\n  }\n  if (inputShape.shape.length !== 1) {\n    throw new Error(`Input shape should be a vector but received shape\n        ${inputShape.shape}`);\n  }\n\n  if (newShape.shape.length !== 1) {\n    throw new Error(\n        `Target shape should be a vector but received shape ${newShape.shape}`);\n  }\n\n  const $inputShape =\n      Array.from(backend.data.get(inputShape.dataId).values as TypedArray);\n  const $inputIndices =\n      backend.data.get(inputIndices.dataId).values as TypedArray;\n  const targetShape =\n      Array.from(backend.data.get(newShape.dataId).values as TypedArray);\n\n  const [newIndices, indicesShape, outputShape] = sparseReshapeImpl(\n      $inputIndices, inputIndices.shape, inputIndices.dtype, $inputShape,\n      targetShape);\n  return [\n    backend.makeTensorInfo(indicesShape, inputIndices.dtype, newIndices),\n    backend.makeTensorInfo(\n        [outputShape.length], newShape.dtype, new Int32Array(outputShape)),\n  ];\n}\n\nexport const sparseReshapeConfig: KernelConfig = {\n  kernelName: SparseReshape,\n  backendName: 'cpu',\n  kernelFunc: sparseReshape,\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, SparseSegmentMean, SparseSegmentMeanInputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {sparseSegmentReductionImpl} from './SparseSegmentReduction_impl';\n\nexport function sparseSegmentMean(\n    args: {inputs: SparseSegmentMeanInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {data, indices, segmentIds} = inputs;\n  if (data.shape.length < 1) {\n    throw new Error(\n        `Data should be at least 1 dimensional but received scalar`);\n  }\n  if (indices.shape.length !== 1) {\n    throw new Error(`Indices should be a vector but received shape\n          ${indices.shape}`);\n  }\n  if (segmentIds.shape.length !== 1) {\n    throw new Error(`Segment ids should be a vector but received shape\n          ${segmentIds.shape}`);\n  }\n  if (indices.shape[0] !== segmentIds.shape[0]) {\n    throw new Error(`segmentIds and indices should have same size.`);\n  }\n\n  const $data = backend.data.get(data.dataId).values as TypedArray;\n  const $indices = backend.data.get(indices.dataId).values as TypedArray;\n  const $segmentIds = backend.data.get(segmentIds.dataId).values as TypedArray;\n\n  const [outputData, outputDataShape] = sparseSegmentReductionImpl(\n      $data, data.shape, data.dtype, $indices, $segmentIds, true);\n  return backend.makeTensorInfo(outputDataShape, data.dtype, outputData);\n}\n\nexport const sparseSegmentMeanConfig: KernelConfig = {\n  kernelName: SparseSegmentMean,\n  backendName: 'cpu',\n  kernelFunc: sparseSegmentMean,\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, SparseSegmentSum, SparseSegmentSumInputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {sparseSegmentReductionImpl} from './SparseSegmentReduction_impl';\n\nexport function sparseSegmentSum(\n    args: {inputs: SparseSegmentSumInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {data, indices, segmentIds} = inputs;\n  if (data.shape.length < 1) {\n    throw new Error(\n        `Data should be at least 1 dimensional but received scalar`);\n  }\n  if (indices.shape.length !== 1) {\n    throw new Error(`Indices should be a vector but received shape\n         ${indices.shape}`);\n  }\n  if (segmentIds.shape.length !== 1) {\n    throw new Error(`Segment ids should be a vector but received shape\n         ${segmentIds.shape}`);\n  }\n  if (indices.shape[0] !== segmentIds.shape[0]) {\n    throw new Error(`segmentIds and indices should have same size.`);\n  }\n\n  const $data = backend.data.get(data.dataId).values as TypedArray;\n  const $indices = backend.data.get(indices.dataId).values as TypedArray;\n  const $segmentIds = backend.data.get(segmentIds.dataId).values as TypedArray;\n\n  const [outputData, outputDataShape] = sparseSegmentReductionImpl(\n      $data, data.shape, data.dtype, $indices, $segmentIds);\n  return backend.makeTensorInfo(outputDataShape, data.dtype, outputData);\n}\n\nexport const sparseSegmentSumConfig: KernelConfig = {\n  kernelName: SparseSegmentSum,\n  backendName: 'cpu',\n  kernelFunc: sparseSegmentSum,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, Rank, SparseToDense, SparseToDenseAttrs, SparseToDenseInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {scatterImpl} from './Scatter_impl';\n\nexport function sparseToDense(args: {\n  inputs: SparseToDenseInputs,\n  backend: MathBackendCPU,\n  attrs: SparseToDenseAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {sparseIndices, sparseValues, defaultValue} = inputs;\n  const {outputShape} = attrs;\n\n  const {sliceRank, numUpdates, sliceSize, strides, outputSize} =\n      backend_util.calculateShapes(sparseValues, sparseIndices, outputShape);\n  const sumDupeIndices = false;\n\n  const indicesBuf = backend.bufferSync<Rank, 'int32'>(sparseIndices);\n\n  let outBuf;\n  switch (sparseValues.dtype) {\n    case 'bool': {\n      const updatesBuf = backend.bufferSync<Rank, 'bool'>(sparseValues);\n      const $defaultValue =\n          Boolean(backend.data.get(defaultValue.dataId).values[0]);\n      outBuf = scatterImpl(\n          indicesBuf, updatesBuf, outputShape, outputSize, sliceSize,\n          numUpdates, sliceRank, strides, $defaultValue, sumDupeIndices);\n      break;\n    }\n    case 'float32': {\n      const updatesBuf = backend.bufferSync<Rank, 'float32'>(sparseValues);\n      const $defaultValue =\n          backend.data.get(defaultValue.dataId).values[0] as number;\n      outBuf = scatterImpl(\n          indicesBuf, updatesBuf, outputShape, outputSize, sliceSize,\n          numUpdates, sliceRank, strides, $defaultValue, sumDupeIndices);\n      break;\n    }\n    case 'int32': {\n      const updatesBuf = backend.bufferSync<Rank, 'int32'>(sparseValues);\n      const $defaultValue =\n          backend.data.get(defaultValue.dataId).values[0] as number;\n      outBuf = scatterImpl(\n          indicesBuf, updatesBuf, outputShape, outputSize, sliceSize,\n          numUpdates, sliceRank, strides, $defaultValue, sumDupeIndices);\n      break;\n    }\n    case 'string': {\n      const updatesBuf = backend.bufferSync<Rank, 'string'>(sparseValues);\n      const $defaultValue = util.decodeString(\n          backend.data.get(defaultValue.dataId).values[0] as Uint8Array);\n      outBuf = scatterImpl(\n          indicesBuf, updatesBuf, outputShape, outputSize, sliceSize,\n          numUpdates, sliceRank, strides, $defaultValue, sumDupeIndices);\n      break;\n    }\n    default:\n      throw new Error(`Unsupported type ${sparseValues.dtype}`);\n  }\n  return backend.makeTensorInfo(outputShape, outBuf.dtype, outBuf.values);\n}\n\nexport const sparseToDenseConfig: KernelConfig = {\n  kernelName: SparseToDense,\n  backendName: 'cpu',\n  kernelFunc: sparseToDense as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, SplitVAttrs, SplitVInputs} from '@tensorflow/tfjs-core';\nimport {KernelConfig, KernelFunc, SplitV, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {slice} from './Slice';\n\nexport function splitV(\n    args: {inputs: SplitVInputs, backend: MathBackendCPU, attrs: SplitVAttrs}):\n    TensorInfo[] {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {numOrSizeSplits, axis} = attrs;\n\n  const $axis = util.parseAxisParam(axis, x.shape)[0];\n  const splitSizes = backend_util.prepareSplitSize(x, numOrSizeSplits, $axis);\n\n  const begin = new Array(x.shape.length).fill(0);\n  const size = x.shape.slice();\n  return splitSizes.map(s => {\n    const sliceSize = [...size];\n    sliceSize[$axis] = s;\n    const sliceT =\n        slice({inputs: {x}, backend, attrs: {begin, size: sliceSize}});\n    begin[$axis] += s;\n    return sliceT;\n  });\n}\n\nexport const splitVConfig: KernelConfig = {\n  kernelName: SplitV,\n  backendName: 'cpu',\n  kernelFunc: splitV as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Square, SquareInputs} from '@tensorflow/tfjs-core';\nimport {KernelConfig} from '@tensorflow/tfjs-core';\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport const squareConfig: KernelConfig = {\n  kernelName: Square,\n  backendName: 'cpu',\n  kernelFunc: ({inputs, backend}) => {\n    const {x} = inputs as SquareInputs;\n    const cpuBackend = backend as MathBackendCPU;\n    assertNotComplex(x, 'square');\n\n    const values = cpuBackend.data.get(x.dataId).values as Float32Array;\n    const newValues = new Float32Array(values.length);\n    for (let i = 0; i < values.length; ++i) {\n      const value = values[i];\n      newValues[i] = value * value;\n    }\n    const dataId = cpuBackend.write(newValues, x.shape, x.dtype);\n    return {dataId, shape: x.shape, dtype: x.dtype};\n  }\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Step, StepAttrs} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const step = unaryKernelFunc(Step, (xi, attrs) => {\n  const stepAttrs = attrs as {} as StepAttrs;\n  if (isNaN(xi)) {\n    return NaN;\n  } else {\n    return xi > 0 ? 1 : stepAttrs.alpha;\n  }\n});\n\nexport const stepConfig: KernelConfig = {\n  kernelName: Step,\n  backendName: 'cpu',\n  kernelFunc: step,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Rank, slice_util, StridedSlice, StridedSliceAttrs, StridedSliceInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {reshape} from './Reshape';\nimport {slice} from './Slice';\nimport {stridedSliceImpl} from './StridedSlice_impl';\n\nexport function stridedSlice(args: {\n  inputs: StridedSliceInputs,\n  backend: MathBackendCPU,\n  attrs: StridedSliceAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {\n    begin,\n    end,\n    strides,\n    beginMask,\n    endMask,\n    ellipsisMask,\n    newAxisMask,\n    shrinkAxisMask\n  } = attrs;\n\n  assertNotComplex(x, 'stridedSlice');\n\n  const {\n    finalShapeSparse,\n    finalShape,\n    isIdentity,\n    sliceDim0,\n    isSimpleSlice,\n    begin: $begin,\n    end: $end,\n    strides: $strides\n  } =\n      slice_util.sliceInfo(\n          x.shape, begin, end, strides, beginMask, endMask, ellipsisMask,\n          newAxisMask, shrinkAxisMask);\n\n  let result;\n\n  // ref:\n  // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/strided_slice_op.cc\n  if (isIdentity) {\n    // Optimization #1, slice is a no-op plus reshape\n    result = reshape({inputs: {x}, backend, attrs: {shape: finalShape}});\n  } else if (sliceDim0 || isSimpleSlice) {\n    // Optimization #2, slice is memory contiguous (only occurs in dim 0)\n    util.assert(\n        x.shape.length >= 1,\n        () => `Input must have rank at least 1, got: ${x.shape.length}`);\n\n    const size = slice_util.computeOutShape($begin, $end, $strides);\n    // To tolerate begin[0] > end[0] (a 0-output slice), we min(begin, end).\n    const sliced = slice({inputs: {x}, backend, attrs: {begin: $begin, size}});\n    result =\n        reshape({inputs: {x: sliced}, backend, attrs: {shape: finalShape}});\n    backend.disposeIntermediateTensorInfo(sliced);\n  } else {\n    const xBuf = backend.bufferSync<Rank, 'float32'>(x);\n    const outBuf = stridedSliceImpl(finalShapeSparse, xBuf, $strides, $begin);\n\n    result = backend.makeTensorInfo(finalShape, outBuf.dtype, outBuf.values);\n  }\n\n  return result;\n}\n\nexport const stridedSliceConfig: KernelConfig = {\n  kernelName: StridedSlice,\n  backendName: 'cpu',\n  kernelFunc: stridedSlice as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, StringNGrams, StringNGramsAttrs, StringNGramsInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {stringNGramsImpl} from './StringNGrams_impl';\n\nexport function stringNGrams(args: {\n  inputs: StringNGramsInputs,\n  backend: MathBackendCPU,\n  attrs: StringNGramsAttrs\n}): [TensorInfo, TensorInfo] {\n  const {inputs, backend, attrs} = args;\n  const {\n    separator,\n    nGramWidths,\n    leftPad,\n    rightPad,\n    padWidth,\n    preserveShortSequences\n  } = attrs;\n  const {data, dataSplits} = inputs;\n  const $data = backend.data.get(data.dataId).values as Uint8Array[];\n  const $dataSplits = backend.data.get(dataSplits.dataId).values as Int32Array;\n\n  const [nGrams, nGramsSplits] = stringNGramsImpl(\n      $data, $dataSplits, separator, nGramWidths, leftPad, rightPad, padWidth,\n      preserveShortSequences);\n  return [\n    backend.makeTensorInfo([nGrams.length], 'string', nGrams),\n    backend.makeTensorInfo(dataSplits.shape, 'int32', nGramsSplits),\n  ];\n}\n\nexport const stringNGramsConfig: KernelConfig = {\n  kernelName: StringNGrams,\n  backendName: 'cpu',\n  kernelFunc: stringNGrams as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, StringSplit, StringSplitAttrs, StringSplitInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {stringSplitImpl} from './StringSplit_impl';\n\nexport function stringSplit(args: {\n  inputs: StringSplitInputs,\n  backend: MathBackendCPU,\n  attrs: StringSplitAttrs\n}): [TensorInfo, TensorInfo, TensorInfo] {\n  const {inputs, backend, attrs} = args;\n  const {skipEmpty} = attrs;\n  const {input, delimiter} = inputs;\n\n  if (input.dtype !== 'string') {\n    throw new Error('Input must be of datatype string');\n  }\n  if (input.shape.length !== 1) {\n    throw new Error(`Input must be a vector, got shape: ${input.shape}`);\n  }\n  if (delimiter.shape.length !== 0) {\n    throw new Error(\n        `Delimiter must be a scalar, got shape: ${delimiter.shape}`);\n  }\n\n  const $input = backend.data.get(input.dataId).values as Uint8Array[];\n  const $delimiter = backend.data.get(delimiter.dataId).values[0] as Uint8Array;\n\n  const [indices, values, shape] =\n      stringSplitImpl($input, $delimiter, skipEmpty);\n  const outputSize = values.length;\n  return [\n    backend.makeTensorInfo([outputSize, 2], 'int32', indices),\n    backend.makeTensorInfo([outputSize], 'string', values),\n    backend.makeTensorInfo([2], 'int32', new Int32Array(shape))\n  ];\n}\n\nexport const stringSplitConfig: KernelConfig = {\n  kernelName: StringSplit,\n  backendName: 'cpu',\n  kernelFunc: stringSplit as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, StringToHashBucketFast, StringToHashBucketFastAttrs, StringToHashBucketFastInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {stringToHashBucketFastImpl} from './StringToHashBucketFast_impl';\n\nexport function stringToHashBucketFast(args: {\n  inputs: StringToHashBucketFastInputs,\n  backend: MathBackendCPU,\n  attrs: StringToHashBucketFastAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {numBuckets} = attrs;\n  const {input} = inputs;\n\n  if (input.dtype !== 'string') {\n    throw new Error('Input must be of datatype string');\n  }\n  if (numBuckets <= 0) {\n    throw new Error(`Number of buckets must be at least 1`);\n  }\n\n  const $input = backend.data.get(input.dataId).values as Uint8Array[];\n\n  const output = stringToHashBucketFastImpl($input, numBuckets);\n  return backend.makeTensorInfo(input.shape, 'int32', output);\n}\n\nexport const stringToHashBucketFastConfig: KernelConfig = {\n  kernelName: StringToHashBucketFast,\n  backendName: 'cpu',\n  kernelFunc: stringToHashBucketFast as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Tan} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const tan = unaryKernelFunc(Tan, (xi) => Math.tan(xi));\n\nexport const tanConfig: KernelConfig = {\n  kernelName: Tan,\n  backendName: 'cpu',\n  kernelFunc: tan,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Tanh} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../utils/unary_utils';\n\nexport const tanh = unaryKernelFunc(Tanh, (xi) => Math.tanh(xi));\n\nexport const tanhConfig: KernelConfig = {\n  kernelName: Tanh,\n  backendName: 'cpu',\n  kernelFunc: tanh,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, TensorInfo, Tile, TileAttrs, TileInputs} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {tileImpl} from './Tile_impl';\n\nexport function tile(\n    args: {inputs: TileInputs, backend: MathBackendCPU, attrs: TileAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {reps} = attrs;\n\n  assertNotComplex(x, 'tile');\n  const outBuf = tileImpl(backend.bufferSync(x), reps);\n\n  return backend.makeTensorInfo(outBuf.shape, outBuf.dtype, outBuf.values);\n}\n\nexport const tileConfig: KernelConfig = {\n  kernelName: Tile,\n  backendName: 'cpu',\n  kernelFunc: tile as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, NumericDataType, TensorInfo, TopK, TopKAttrs, TopKInputs, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {topKImpl} from './TopK_impl';\n\nexport function topK(\n    args: {inputs: TopKInputs, backend: MathBackendCPU, attrs: TopKAttrs}):\n    [TensorInfo, TensorInfo] {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {k, sorted} = attrs;\n\n  assertNotComplex(x, 'topk');\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const [allTopKVals, allTopKIndices] =\n      topKImpl(xVals, x.shape, x.dtype as NumericDataType, k, sorted);\n\n  return [\n    backend.makeTensorInfo(\n        allTopKVals.shape, allTopKVals.dtype, allTopKVals.values),\n    backend.makeTensorInfo(\n        allTopKIndices.shape, allTopKIndices.dtype, allTopKIndices.values)\n  ];\n}\n\nexport const topKConfig: KernelConfig = {\n  kernelName: TopK,\n  backendName: 'cpu',\n  kernelFunc: topK as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, NumericDataType, TensorInfo, Transform, TransformAttrs, TransformInputs, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport function transform(args: {\n  inputs: TransformInputs,\n  attrs: TransformAttrs,\n  backend: MathBackendCPU\n}): TensorInfo {\n  const {inputs, attrs, backend} = args;\n  const {image, transforms} = inputs;\n  const {interpolation, fillMode, fillValue, outputShape} = attrs;\n\n  const [batch, imageHeight, imageWidth, numChannels] = image.shape;\n  const [outHeight, outWidth] =\n      outputShape != null ? outputShape : [imageHeight, imageWidth];\n  const outShape = [batch, outHeight, outWidth, numChannels];\n\n  const inStrides = util.computeStrides(image.shape);\n  const batchInStride = inStrides[0];\n  const rowInStride = inStrides[1];\n  const colInStride = inStrides[2];\n\n  const outStrides = util.computeStrides(outShape);\n  const batchOutStride = outStrides[0];\n  const rowOutStride = outStrides[1];\n  const colOutStride = outStrides[2];\n\n  const outVals = util.getTypedArrayFromDType(\n      image.dtype as NumericDataType, util.sizeFromShape(outShape));\n\n  outVals.fill(fillValue);\n\n  const imageVals = backend.data.get(image.dataId).values as TypedArray;\n  const transformVals =\n      backend.data.get(transforms.dataId).values as TypedArray;\n\n  // Ref TF implementation:\n  // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/image_ops.h\n  for (let b = 0; b < batch; ++b) {\n    const transform = transforms.shape[0] === 1 ?\n        transformVals :\n        transformVals.subarray(b * 8, b * 8 + 8);\n\n    for (let outY = 0; outY < outHeight; ++outY) {\n      for (let outX = 0; outX < outWidth; ++outX) {\n        for (let channel = 0; channel < numChannels; ++channel) {\n          let val;\n\n          const projection = transform[6] * outX + transform[7] * outY + 1;\n\n          if (projection === 0) {\n            // Return the fill value for infinite coordinates,\n            // which are outside the input image\n            continue;\n          }\n\n          const inX =\n              (transform[0] * outX + transform[1] * outY + transform[2]) /\n              projection;\n          const inY =\n              (transform[3] * outX + transform[4] * outY + transform[5]) /\n              projection;\n\n          const x = mapCoord(inX, imageWidth, fillMode);\n          const y = mapCoord(inY, imageHeight, fillMode);\n\n          switch (interpolation) {\n            case 'nearest':\n              val = nearestInterpolation(\n                  imageVals, imageHeight, imageWidth, batchInStride,\n                  rowInStride, colInStride, b, y, x, channel, fillValue);\n              break;\n            case 'bilinear':\n              val = bilinearInterpolation(\n                  imageVals, imageHeight, imageWidth, batchInStride,\n                  rowInStride, colInStride, b, y, x, channel, fillValue);\n              break;\n            default:\n              throw new Error(\n                  `Error in Transform: Expect 'nearest' or ` +\n                  `'bilinear', but got ${interpolation}`);\n          }\n\n          const ind =\n              b * batchOutStride + outY * rowOutStride +\n              outX * colOutStride + channel;\n\n          outVals[ind] = val;\n        }\n      }\n    }\n\n    return backend.makeTensorInfo(outShape, image.dtype, outVals);\n  }\n\n  const dataId = backend.write(outVals, outShape, image.dtype);\n  return {dataId, shape: image.shape, dtype: image.dtype};\n}\n\nexport const transformConfig: KernelConfig = {\n  kernelName: Transform,\n  backendName: 'cpu',\n  kernelFunc: transform as {} as KernelFunc\n};\n\nfunction mapCoord(\n    outCoord: number, len: number,\n    mode: 'constant'|'reflect'|'wrap'|'nearest') {\n  switch (mode) {\n    case 'reflect':\n      return mapCoordReflect(outCoord, len);\n    case 'wrap':\n      return mapCoordWrap(outCoord, len);\n    case 'nearest':\n      return mapCoordNearest(outCoord, len);\n    case 'constant':\n    default:\n      return mapCoordConstant(outCoord, len);\n  }\n}\n\nfunction mapCoordReflect(outCoord: number, len: number): number {\n  // Reflect [abcd] to [dcba|abcd|dcba].\n  let inCoord = outCoord;\n  if (inCoord < 0) {\n    if (len <= 1) {\n      inCoord = 0;\n    } else {\n      const sz2 = 2 * len;\n      if (inCoord < sz2) {\n        inCoord = sz2 * Math.trunc(-inCoord / sz2) + inCoord;\n      }\n      inCoord = inCoord < -len ? inCoord + sz2 : -inCoord - 1;\n    }\n  } else if (inCoord > len - 1) {\n    if (len <= 1) {\n      inCoord = 0;\n    } else {\n      const sz2 = 2 * len;\n      inCoord -= sz2 * Math.trunc(inCoord / sz2);\n      if (inCoord >= len) {\n        inCoord = sz2 - inCoord - 1;\n      }\n    }\n  }\n  // clamp is necessary because when outCoord = 3.5 and len = 4,\n  // inCoord = 3.5 and will be rounded to 4 in nearest interpolation.\n  return util.clamp(0, inCoord, len - 1);\n}\n\nfunction mapCoordWrap(outCoord: number, len: number): number {\n  // Wrap [abcd] to [abcd|abcd|abcd].\n  let inCoord = outCoord;\n  if (inCoord < 0) {\n    if (len <= 1) {\n      inCoord = 0;\n    } else {\n      const sz = len - 1;\n      inCoord += len * (Math.trunc(-inCoord / sz) + 1);\n    }\n  } else if (inCoord > len - 1) {\n    if (len <= 1) {\n      inCoord = 0;\n    } else {\n      const sz = len - 1;\n      inCoord -= len * Math.trunc(inCoord / sz);\n    }\n  }\n  // clamp is necessary because when outCoord = -0.5 and len = 4,\n  // inCoord = 3.5 and will be rounded to 4 in nearest interpolation.\n  return util.clamp(0, inCoord, len - 1);\n}\n\nfunction mapCoordConstant(outCoord: number, len: number): number {\n  return outCoord;\n}\n\nfunction mapCoordNearest(outCoord: number, len: number): number {\n  return util.clamp(0, outCoord, len - 1);\n}\n\nfunction readWithFillValue(\n    imageVals: TypedArray, imageHeight: number, imageWidth: number,\n    batchStride: number, rowStride: number, colStride: number, batch: number,\n    y: number, x: number, channel: number, fillValue: number): number {\n  const ind = batch * batchStride + y * rowStride + x * colStride + channel;\n  if (0 <= y && y < imageHeight && 0 <= x && x < imageWidth) {\n    return imageVals[ind];\n  } else {\n    return fillValue;\n  }\n}\n\nfunction nearestInterpolation(\n    imageVals: TypedArray, imageHeight: number, imageWidth: number,\n    batchStride: number, rowStride: number, colStride: number, batch: number,\n    y: number, x: number, channel: number, fillValue: number): number {\n  const $y = Math.round(y);\n  const $x = Math.round(x);\n\n  return readWithFillValue(\n      imageVals, imageHeight, imageWidth, batchStride, rowStride, colStride,\n      batch, $y, $x, channel, fillValue);\n}\n\nfunction bilinearInterpolation(\n    imageVals: TypedArray, imageHeight: number, imageWidth: number,\n    batchStride: number, rowStride: number, colStride: number, batch: number,\n    y: number, x: number, channel: number, fillValue: number) {\n  const yFloor = Math.floor(y);\n  const xFloor = Math.floor(x);\n  const yCeil = yFloor + 1;\n  const xCeil = xFloor + 1;\n  // f(x, yFloor) = (xCeil - x) / (xCeil - xFloor) * f(xFloor, yFloor)\n  //               + (x - xFloor) / (xCeil - xFloor) * f(xCeil, yFloor)\n  const valueYFloor =\n      (xCeil - x) *\n          readWithFillValue(\n              imageVals, imageHeight, imageWidth, batchStride, rowStride,\n              colStride, batch, yFloor, xFloor, channel, fillValue) +\n      (x - xFloor) *\n          readWithFillValue(\n              imageVals, imageHeight, imageWidth, batchStride, rowStride,\n              colStride, batch, yFloor, xCeil, channel, fillValue);\n  // f(x, yCeil) = (xCeil - x) / (xCeil - xFloor) * f(xFloor, yCeil)\n  //             + (x - xFloor) / (xCeil - xFloor) * f(xCeil, yCeil)\n  const valueYCeil =\n      (xCeil - x) *\n          readWithFillValue(\n              imageVals, imageHeight, imageWidth, batchStride, rowStride,\n              colStride, batch, yCeil, xFloor, channel, fillValue) +\n      (x - xFloor) *\n          readWithFillValue(\n              imageVals, imageHeight, imageWidth, batchStride, rowStride,\n              colStride, batch, yCeil, xCeil, channel, fillValue);\n  // f(x, y) = (yCeil - y) / (yCeil - yFloor) * f(x, yFloor)\n  //         + (y - yFloor) / (yCeil - yFloor) * f(x, yCeil)\n  return (yCeil - y) * valueYFloor + (y - yFloor) * valueYCeil;\n}\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, TensorInfo, Unique, UniqueAttrs, UniqueInputs} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nimport {uniqueImpl} from './Unique_impl';\n\nexport function unique(\n    args: {inputs: UniqueInputs, attrs: UniqueAttrs, backend: MathBackendCPU}):\n    TensorInfo[] {\n  const {inputs, attrs, backend} = args;\n  const {axis} = attrs;\n  const {x} = inputs;\n  assertNotComplex(x, 'unique');\n\n  const values = backend.data.get(x.dataId).values;\n  const {outputValues, outputShape, indices} =\n      uniqueImpl(values, axis, x.shape, x.dtype);\n  return [\n    backend.makeTensorInfo(outputShape, x.dtype, outputValues),\n    backend.makeTensorInfo([indices.length], 'int32', indices),\n  ];\n}\n\nexport const uniqueConfig: KernelConfig = {\n  kernelName: Unique,\n  backendName: 'cpu',\n  kernelFunc: unique as {} as KernelFunc,\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, TensorInfo, Unpack, UnpackAttrs, UnpackInputs} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {reshape} from './Reshape';\nimport {slice} from './Slice';\n\nexport function unpack(\n    args: {inputs: UnpackInputs, backend: MathBackendCPU, attrs: UnpackAttrs}):\n    TensorInfo[] {\n  const {inputs, backend, attrs} = args;\n  const {value} = inputs;\n  let {axis} = attrs;\n\n  if (axis < 0) {\n    axis += value.shape.length;\n  }\n\n  const valueRank = value.shape.length;\n\n  const num = value.shape[axis];\n  const outShape: number[] = new Array(valueRank - 1);\n  let outIndex = 0;\n  for (let i = 0; i < valueRank; i++) {\n    if (i !== axis) {\n      outShape[outIndex++] = value.shape[i];\n    }\n  }\n\n  const begin = new Array(valueRank).fill(0);\n  const size = value.shape.slice();\n  size[axis] = 1;\n  const res = new Array(num);\n  for (let i = 0; i < res.length; i++) {\n    begin[axis] = i;\n    const tempRes = slice({inputs: {x: value}, backend, attrs: {begin, size}});\n    res[i] = reshape({inputs: {x: tempRes}, backend, attrs: {shape: outShape}});\n    backend.disposeIntermediateTensorInfo(tempRes);\n  }\n\n  return res;\n}\n\nexport const unpackConfig: KernelConfig = {\n  kernelName: Unpack,\n  backendName: 'cpu',\n  kernelFunc: unpack as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, TensorInfo, UnsortedSegmentSum, UnsortedSegmentSumAttrs, UnsortedSegmentSumInputs, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {cast} from './Cast';\nimport {equal} from './Equal';\nimport {expandDims} from './ExpandDims';\nimport {multiply} from './Multiply';\nimport {pack} from './Pack';\nimport {sum} from './Sum';\n\nexport function unsortedSegmentSum(args: {\n  inputs: UnsortedSegmentSumInputs,\n  backend: MathBackendCPU,\n  attrs: UnsortedSegmentSumAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, segmentIds} = inputs;\n  const {numSegments} = attrs;\n\n  assertNotComplex(x, 'unsortedSegmentSum');\n\n  const xRank = x.shape.length;\n  const segmentIdsRank = segmentIds.shape.length;\n  const res = [];\n  const intermediates: TensorInfo[] = [];\n\n  // Reshape the segment id's so that they can be broadcast with\n  // x. The new shape should be [segmentIds.shape, 1, ..., 1]\n  const numIters = xRank - segmentIdsRank;\n  let $segmentIds = segmentIds;\n\n  for (let i = 0; i < numIters; ++i) {\n    const expanded = expandDims(\n        {inputs: {input: $segmentIds}, backend, attrs: {dim: i + 1}});\n    $segmentIds = expanded;\n    intermediates.push(expanded);\n  }\n\n  for (let i = 0; i < numSegments; ++i) {\n    const scalarValue = util.createScalarValue(i as {} as 'int32', 'int32');\n    const segmentId = backend.makeTensorInfo([], 'int32', scalarValue);\n    const mask =\n        equal({inputs: {a: segmentId, b: $segmentIds}, backend}) as TensorInfo;\n    const maskCasted =\n        cast({inputs: {x: mask}, backend, attrs: {dtype: 'float32'}});\n    const mul =\n        multiply({inputs: {a: maskCasted, b: x}, backend}) as TensorInfo;\n    const sumTensorInfo =\n        sum({inputs: {x: mul}, backend, attrs: {axis: 0, keepDims: false}});\n    res.push(sumTensorInfo);\n    intermediates.push(segmentId);\n    intermediates.push(mask);\n    intermediates.push(maskCasted);\n    intermediates.push(mul);\n    intermediates.push(sumTensorInfo);\n  }\n\n  const result = pack({inputs: res, backend, attrs: {axis: 0}});\n\n  intermediates.forEach(t => backend.disposeIntermediateTensorInfo(t));\n\n  return result;\n}\n\nexport const unsortedSegmentSumConfig: KernelConfig = {\n  kernelName: UnsortedSegmentSum,\n  backendName: 'cpu',\n  kernelFunc: unsortedSegmentSum as {} as KernelFunc\n};\n", "/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// We explicitly import the modular kernels so they get registered in the\n// global registry when we compile the library. A modular build would replace\n// the contents of this file and import only the kernels that are needed.\nimport {KernelConfig, registerKernel} from '@tensorflow/tfjs-core';\n\nimport {_fusedMatMulConfig} from './kernels/_FusedMatMul';\nimport {absConfig} from './kernels/Abs';\nimport {acosConfig} from './kernels/Acos';\nimport {acoshConfig} from './kernels/Acosh';\nimport {addConfig} from './kernels/Add';\nimport {addNConfig} from './kernels/AddN';\nimport {allConfig} from './kernels/All';\nimport {anyConfig} from './kernels/Any';\nimport {argMaxConfig} from './kernels/ArgMax';\nimport {argMinConfig} from './kernels/ArgMin';\nimport {asinConfig} from './kernels/Asin';\nimport {asinhConfig} from './kernels/Asinh';\nimport {atanConfig} from './kernels/Atan';\nimport {atan2Config} from './kernels/Atan2';\nimport {atanhConfig} from './kernels/Atanh';\nimport {avgPoolConfig} from './kernels/AvgPool';\nimport {avgPool3DConfig} from './kernels/AvgPool3D';\nimport {avgPool3DGradConfig} from './kernels/AvgPool3DGrad';\nimport {avgPoolGradConfig} from './kernels/AvgPoolGrad';\nimport {batchMatMulConfig} from './kernels/BatchMatMul';\nimport {batchNormConfig} from './kernels/BatchNorm';\nimport {batchToSpaceNDConfig} from './kernels/BatchToSpaceND';\nimport {bincountConfig} from './kernels/Bincount';\nimport {broadcastArgsConfig} from './kernels/BroadcastArgs';\nimport {castConfig} from './kernels/Cast';\nimport {ceilConfig} from './kernels/Ceil';\nimport {clipByValueConfig} from './kernels/ClipByValue';\nimport {complexConfig} from './kernels/Complex';\nimport {complexAbsConfig} from './kernels/ComplexAbs';\nimport {concatConfig} from './kernels/Concat';\nimport {conv2DConfig} from './kernels/Conv2D';\nimport {conv2DBackpropFilterConfig} from './kernels/Conv2DBackpropFilter';\nimport {conv2DBackpropInputConfig} from './kernels/Conv2DBackpropInput';\nimport {conv3DConfig} from './kernels/Conv3D';\nimport {conv3DBackpropFilterV2Config} from './kernels/Conv3DBackpropFilterV2';\nimport {conv3DBackpropInputV2Config} from './kernels/Conv3DBackpropInputV2';\nimport {cosConfig} from './kernels/Cos';\nimport {coshConfig} from './kernels/Cosh';\nimport {cropAndResizeConfig} from './kernels/CropAndResize';\nimport {cumprodConfig} from './kernels/Cumprod';\nimport {cumsumConfig} from './kernels/Cumsum';\nimport {denseBincountConfig} from './kernels/DenseBincount';\nimport {depthToSpaceConfig} from './kernels/DepthToSpace';\nimport {depthwiseConv2dNativeConfig} from './kernels/DepthwiseConv2dNative';\nimport {depthwiseConv2dNativeBackpropFilterConfig} from './kernels/DepthwiseConv2dNativeBackpropFilter';\nimport {depthwiseConv2dNativeBackpropInputConfig} from './kernels/DepthwiseConv2dNativeBackpropInput';\nimport {diagConfig} from './kernels/Diag';\nimport {dilation2DConfig} from './kernels/Dilation2D';\nimport {dilation2DBackpropFilterConfig} from './kernels/Dilation2DBackpropFilter';\nimport {dilation2DBackpropInputConfig} from './kernels/Dilation2DBackpropInput';\nimport {einsumConfig} from './kernels/Einsum';\nimport {eluConfig} from './kernels/Elu';\nimport {eluGradConfig} from './kernels/EluGrad';\nimport {equalConfig} from './kernels/Equal';\nimport {erfConfig} from './kernels/Erf';\nimport {expConfig} from './kernels/Exp';\nimport {expandDimsConfig} from './kernels/ExpandDims';\nimport {expm1Config} from './kernels/Expm1';\nimport {fftConfig} from './kernels/FFT';\nimport {fillConfig} from './kernels/Fill';\nimport {flipLeftRightConfig} from './kernels/FlipLeftRight';\nimport {floorConfig} from './kernels/Floor';\nimport {floorDivConfig} from './kernels/FloorDiv';\nimport {fusedConv2DConfig} from './kernels/FusedConv2D';\nimport {fusedDepthwiseConv2DConfig} from './kernels/FusedDepthwiseConv2D';\nimport {gatherNdConfig} from './kernels/GatherNd';\nimport {gatherV2Config} from './kernels/GatherV2';\nimport {greaterConfig} from './kernels/Greater';\nimport {greaterEqualConfig} from './kernels/GreaterEqual';\nimport {identityConfig} from './kernels/Identity';\nimport {ifftConfig} from './kernels/IFFT';\nimport {imagConfig} from './kernels/Imag';\nimport {isFiniteConfig} from './kernels/IsFinite';\nimport {isInfConfig} from './kernels/IsInf';\nimport {isNaNConfig} from './kernels/IsNaN';\nimport {leakyReluConfig} from './kernels/LeakyRelu';\nimport {lessConfig} from './kernels/Less';\nimport {lessEqualConfig} from './kernels/LessEqual';\nimport {linSpaceConfig} from './kernels/LinSpace';\nimport {logConfig} from './kernels/Log';\nimport {log1pConfig} from './kernels/Log1p';\nimport {logicalAndConfig} from './kernels/LogicalAnd';\nimport {logicalNotConfig} from './kernels/LogicalNot';\nimport {logicalOrConfig} from './kernels/LogicalOr';\nimport {LRNConfig} from './kernels/LRN';\nimport {LRNGradConfig} from './kernels/LRNGrad';\nimport {maxConfig} from './kernels/Max';\nimport {maximumConfig} from './kernels/Maximum';\nimport {maxPoolConfig} from './kernels/MaxPool';\nimport {maxPool3DConfig} from './kernels/MaxPool3D';\nimport {maxPool3DGradConfig} from './kernels/MaxPool3DGrad';\nimport {maxPoolGradConfig} from './kernels/MaxPoolGrad';\nimport {maxPoolWithArgmaxConfig} from './kernels/MaxPoolWithArgmax';\nimport {meanConfig} from './kernels/Mean';\nimport {minConfig} from './kernels/Min';\nimport {minimumConfig} from './kernels/Minimum';\nimport {mirrorPadConfig} from './kernels/MirrorPad';\nimport {modConfig} from './kernels/Mod';\nimport {multinomialConfig} from './kernels/Multinomial';\nimport {multiplyConfig} from './kernels/Multiply';\nimport {negConfig} from './kernels/Neg';\nimport {nonMaxSuppressionV3Config} from './kernels/NonMaxSuppressionV3';\nimport {nonMaxSuppressionV4Config} from './kernels/NonMaxSuppressionV4';\nimport {nonMaxSuppressionV5Config} from './kernels/NonMaxSuppressionV5';\nimport {notEqualConfig} from './kernels/NotEqual';\nimport {oneHotConfig} from './kernels/OneHot';\nimport {onesLikeConfig} from './kernels/OnesLike';\nimport {packConfig} from './kernels/Pack';\nimport {padV2Config} from './kernels/PadV2';\nimport {powConfig} from './kernels/Pow';\nimport {preluConfig} from './kernels/Prelu';\nimport {prodConfig} from './kernels/Prod';\nimport {raggedGatherConfig} from './kernels/RaggedGather';\nimport {raggedTensorToTensorConfig} from './kernels/RaggedTensorToTensor';\nimport {rangeConfig} from './kernels/Range';\nimport {realConfig} from './kernels/Real';\nimport {realDivConfig} from './kernels/RealDiv';\nimport {reciprocalConfig} from './kernels/Reciprocal';\nimport {reluConfig} from './kernels/Relu';\nimport {relu6Config} from './kernels/Relu6';\nimport {reshapeConfig} from './kernels/Reshape';\nimport {resizeBilinearConfig} from './kernels/ResizeBilinear';\nimport {resizeBilinearGradConfig} from './kernels/ResizeBilinearGrad';\nimport {resizeNearestNeighborConfig} from './kernels/ResizeNearestNeighbor';\nimport {resizeNearestNeighborGradConfig} from './kernels/ResizeNearestNeighborGrad';\nimport {reverseConfig} from './kernels/Reverse';\nimport {rotateWithOffsetConfig} from './kernels/RotateWithOffset';\nimport {roundConfig} from './kernels/Round';\nimport {rsqrtConfig} from './kernels/Rsqrt';\nimport {scatterNdConfig} from './kernels/ScatterNd';\nimport {searchSortedConfig} from './kernels/SearchSorted';\nimport {selectConfig} from './kernels/Select';\nimport {seluConfig} from './kernels/Selu';\nimport {sigmoidConfig} from './kernels/Sigmoid';\nimport {signConfig} from './kernels/Sign';\nimport {sinConfig} from './kernels/Sin';\nimport {sinhConfig} from './kernels/Sinh';\nimport {sliceConfig} from './kernels/Slice';\nimport {softmaxConfig} from './kernels/Softmax';\nimport {softplusConfig} from './kernels/Softplus';\nimport {spaceToBatchNDConfig} from './kernels/SpaceToBatchND';\nimport {sparseFillEmptyRowsConfig} from './kernels/SparseFillEmptyRows';\nimport {sparseReshapeConfig} from './kernels/SparseReshape';\nimport {sparseSegmentMeanConfig} from './kernels/SparseSegmentMean';\nimport {sparseSegmentSumConfig} from './kernels/SparseSegmentSum';\nimport {sparseToDenseConfig} from './kernels/SparseToDense';\nimport {splitVConfig} from './kernels/SplitV';\nimport {sqrtConfig} from './kernels/Sqrt';\nimport {squareConfig} from './kernels/Square';\nimport {squaredDifferenceConfig} from './kernels/SquaredDifference';\nimport {stepConfig} from './kernels/Step';\nimport {stridedSliceConfig} from './kernels/StridedSlice';\nimport {stringNGramsConfig} from './kernels/StringNGrams';\nimport {stringSplitConfig} from './kernels/StringSplit';\nimport {stringToHashBucketFastConfig} from './kernels/StringToHashBucketFast';\nimport {subConfig} from './kernels/Sub';\nimport {sumConfig} from './kernels/Sum';\nimport {tanConfig} from './kernels/Tan';\nimport {tanhConfig} from './kernels/Tanh';\nimport {tileConfig} from './kernels/Tile';\nimport {topKConfig} from './kernels/TopK';\nimport {transformConfig} from './kernels/Transform';\nimport {transposeConfig} from './kernels/Transpose';\nimport {uniqueConfig} from './kernels/Unique';\nimport {unpackConfig} from './kernels/Unpack';\nimport {unsortedSegmentSumConfig} from './kernels/UnsortedSegmentSum';\nimport {zerosLikeConfig} from './kernels/ZerosLike';\n\n// List all kernel configs here\nconst kernelConfigs: KernelConfig[] = [\n  _fusedMatMulConfig,\n  absConfig,\n  acosConfig,\n  acoshConfig,\n  addConfig,\n  addNConfig,\n  allConfig,\n  anyConfig,\n  argMaxConfig,\n  argMinConfig,\n  asinConfig,\n  asinhConfig,\n  atanConfig,\n  atan2Config,\n  atanhConfig,\n  avgPoolConfig,\n  avgPool3DConfig,\n  avgPool3DGradConfig,\n  avgPoolGradConfig,\n  batchMatMulConfig,\n  batchNormConfig,\n  batchToSpaceNDConfig,\n  bincountConfig,\n  broadcastArgsConfig,\n  castConfig,\n  ceilConfig,\n  clipByValueConfig,\n  complexConfig,\n  complexAbsConfig,\n  concatConfig,\n  conv2DConfig,\n  conv2DBackpropFilterConfig,\n  conv2DBackpropInputConfig,\n  conv3DConfig,\n  conv3DBackpropFilterV2Config,\n  conv3DBackpropInputV2Config,\n  cosConfig,\n  coshConfig,\n  cropAndResizeConfig,\n  cumprodConfig,\n  cumsumConfig,\n  denseBincountConfig,\n  depthToSpaceConfig,\n  depthwiseConv2dNativeConfig,\n  depthwiseConv2dNativeBackpropFilterConfig,\n  depthwiseConv2dNativeBackpropInputConfig,\n  diagConfig,\n  dilation2DConfig,\n  dilation2DBackpropFilterConfig,\n  dilation2DBackpropInputConfig,\n  einsumConfig,\n  eluConfig,\n  eluGradConfig,\n  equalConfig,\n  erfConfig,\n  expConfig,\n  expandDimsConfig,\n  expm1Config,\n  fftConfig,\n  fillConfig,\n  flipLeftRightConfig,\n  floorConfig,\n  floorDivConfig,\n  fusedConv2DConfig,\n  fusedDepthwiseConv2DConfig,\n  gatherNdConfig,\n  gatherV2Config,\n  greaterConfig,\n  greaterEqualConfig,\n  identityConfig,\n  ifftConfig,\n  imagConfig,\n  isFiniteConfig,\n  isInfConfig,\n  isNaNConfig,\n  leakyReluConfig,\n  lessConfig,\n  lessEqualConfig,\n  linSpaceConfig,\n  logConfig,\n  log1pConfig,\n  logicalAndConfig,\n  logicalNotConfig,\n  logicalOrConfig,\n  LRNConfig,\n  LRNGradConfig,\n  maxConfig,\n  maximumConfig,\n  maxPoolConfig,\n  maxPool3DConfig,\n  maxPool3DGradConfig,\n  maxPoolGradConfig,\n  maxPoolWithArgmaxConfig,\n  meanConfig,\n  minConfig,\n  minimumConfig,\n  mirrorPadConfig,\n  modConfig,\n  multinomialConfig,\n  multiplyConfig,\n  negConfig,\n  nonMaxSuppressionV3Config,\n  nonMaxSuppressionV4Config,\n  nonMaxSuppressionV5Config,\n  notEqualConfig,\n  oneHotConfig,\n  onesLikeConfig,\n  packConfig,\n  padV2Config,\n  powConfig,\n  preluConfig,\n  prodConfig,\n  raggedGatherConfig,\n  raggedTensorToTensorConfig,\n  rangeConfig,\n  realConfig,\n  realDivConfig,\n  reciprocalConfig,\n  reluConfig,\n  relu6Config,\n  reshapeConfig,\n  resizeBilinearConfig,\n  resizeBilinearGradConfig,\n  resizeNearestNeighborConfig,\n  resizeNearestNeighborGradConfig,\n  reverseConfig,\n  rotateWithOffsetConfig,\n  roundConfig,\n  rsqrtConfig,\n  scatterNdConfig,\n  searchSortedConfig,\n  selectConfig,\n  seluConfig,\n  sigmoidConfig,\n  signConfig,\n  sinConfig,\n  sinhConfig,\n  sliceConfig,\n  softmaxConfig,\n  softplusConfig,\n  spaceToBatchNDConfig,\n  sparseFillEmptyRowsConfig,\n  sparseReshapeConfig,\n  sparseSegmentMeanConfig,\n  sparseSegmentSumConfig,\n  sparseToDenseConfig,\n  splitVConfig,\n  sqrtConfig,\n  squareConfig,\n  squaredDifferenceConfig,\n  stepConfig,\n  stridedSliceConfig,\n  stringNGramsConfig,\n  stringSplitConfig,\n  stringToHashBucketFastConfig,\n  subConfig,\n  sumConfig,\n  tanConfig,\n  tanhConfig,\n  tileConfig,\n  topKConfig,\n  transformConfig,\n  transposeConfig,\n  uniqueConfig,\n  unpackConfig,\n  unsortedSegmentSumConfig,\n  zerosLikeConfig\n];\n\nfor (const kernelConfig of kernelConfigs) {\n  registerKernel(kernelConfig);\n}\n", "/** @license See the LICENSE file. */\n\n// This code is auto-generated, do not modify this file!\nconst version = '3.21.0';\nexport {version};\n", "/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport '@tensorflow/tfjs-core';\n// tslint:disable-next-line: no-imports-from-dist\nimport '@tensorflow/tfjs-core/dist/register_all_gradients';\n// tslint:disable-next-line: no-imports-from-dist\nimport '@tensorflow/tfjs-core/dist/public/chained_ops/register_all_chained_ops';\n\nexport * from '@tensorflow/tfjs-core';\nexport * from '@tensorflow/tfjs-layers';\nexport * from '@tensorflow/tfjs-converter';\n\n// Export data api as tf.data\nimport * as data from '@tensorflow/tfjs-data';\nexport {data};\n\n// Import and register backends.\nimport '@tensorflow/tfjs-backend-cpu';\nimport '@tensorflow/tfjs-backend-webgl';\n\n// Import versions of all sub-packages.\nimport {version_core} from '@tensorflow/tfjs-core';\nimport {version_cpu} from '@tensorflow/tfjs-backend-cpu';\nimport {version_webgl} from '@tensorflow/tfjs-backend-webgl';\nimport {version_data} from '@tensorflow/tfjs-data';\nimport {version_layers} from '@tensorflow/tfjs-layers';\nimport {version_converter} from '@tensorflow/tfjs-converter';\nimport {version as version_union} from './version';\n\nexport const version = {\n  'tfjs-core': version_core,\n  'tfjs-backend-cpu': version_cpu,\n  'tfjs-backend-webgl': version_webgl,\n  'tfjs-data': version_data,\n  'tfjs-layers': version_layers,\n  'tfjs-converter': version_converter,\n  'tfjs': version_union\n};\n"],
  "mappings": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA4BA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AACrC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACHA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACHA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AACvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,SACnC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,CAAC;AACpB;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,SAC1B,MAAwB,UAAkB;AACrD,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,MAAM,QAAQ;AACjC;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,SAC1B,MAAwB,UAAkB;AACrD,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,MAAM,QAAQ;AACjC;;;ACLA,qBAAoB,EAAG,UAAU,SAAS,SACtC,MAAa;AACf,OAAK,gBAAe;AACpB,SAAO,OAAO,MAAM,IAAI;AAC1B;;;ACJA,qBAAoB,EAAG,UAAU,SAAS,SACtC,MAAY;AACd,OAAK,gBAAe;AACpB,SAAO,OAAO,MAAM,IAAI;AAC1B;;;ACEA,qBAAoB,EAAG,UAAU,WAAW,WAAA;AAE1C,OAAK,gBAAe;AACpB,SAAO,KAAK,SAAS,GAAG,MAAM,qCAAqC;AACnE,SAAO,QAAQ,MAAM,CAAA,CAAE;AACzB;;;ACFA,qBAAoB,EAAG,UAAU,SAAS,SAC7B,OAAe;AAC1B,OAAK,gBAAe;AACpB,SAAO,KAAQ,MAAM,KAAK;AAC5B;;;ACRA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,CAAC,KAAK,IAAI,CAAC;AAClC;;;ACAA,qBAAoB,EAAG,UAAU,OAAO,SACpC,MAAc,SAAe;AAC/B,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,CAAC,MAAM,OAAO,CAAC;AACtC;;;ACFA,qBAAoB,EAAG,UAAU,OAAO,SACpC,MAAc,SAAiB,OAAa;AAC9C,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,CAAC,MAAM,SAAS,KAAK,CAAC;AAC7C;;;ACHA,qBAAoB,EAAG,UAAU,OAAO,SACpC,MAAc,SAAiB,OAAe,QAAc;AAC9D,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,CAAC,MAAM,SAAS,OAAO,MAAM,CAAC;AACrD;;;ACDA,qBAAoB,EAAG,UAAU,OAAO,SACpC,MAAc,SAAiB,OAAe,QAC9C,QAAc;AAChB,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,CAAC,MAAM,SAAS,OAAO,QAAQ,MAAM,CAAC;AAC7D;;;ACjBA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACHA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACJA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACLA,qBAAoB,EAAG,UAAU,QAAQ,SACrC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,MAAM,MAAM,CAAC;AACtB;;;ACFA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACFA,qBAAoB,EAAG,UAAU,UAC7B,SACa,YACT,SACAA,MACA,iBAAwC;AAC9C,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,YAAY,SAASA,MAAK,eAAe;AAChE;;;ACXA,qBAAoB,EAAG,UAAU,iBAAiB,SAC9C,YAAsB,OAAiB;AACzC,OAAK,gBAAe;AACpB,SAAO,eAAe,MAAM,YAAY,KAAK;AAC/C;;;ACAA,qBAAoB,EAAG,UAAU,YAAY,SACzCC,OACA,UACA,QACAC,QACA,iBAAwB;AAC1B,OAAK,gBAAe;AACpB,SAAO,UAAU,MAAMD,OAAM,UAAU,QAAQC,QAAO,eAAe;AACvE;;;ACZA,qBAAoB,EAAG,UAAU,cAAc,SAC3C,OAAkB;AACpB,OAAK,gBAAe;AACpB,SAAO,YAAY,MAAM,KAAK;AAChC;;;ACHA,qBAAoB,EAAG,UAAU,OAAO,SACpC,OAAe;AACjB,OAAK,gBAAe;AACpB,SAAO,KAAK,MAAM,KAAK;AACzB;;;ACJA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACHA,qBAAoB,EAAG,UAAU,cAAc,SAC3CC,MAAaC,MAAW;AAC1B,OAAK,gBAAe;AACpB,SAAO,YAAY,MAAMD,MAAKC,IAAG;AACnC;;;ACNA,qBAAoB,EAAG,UAAU,SAAS,SACtC,GAA0B,MAAa;AACzC,OAAK,gBAAe;AACpB,MAAI,aAAa,QAAQ;AACvB,QAAI,CAAC,CAAC;;AAER,SAAO,OAAO,CAAC,MAAM,GAAG,CAAC,GAAG,IAAI;AAClC;;;ACHA,qBAAoB,EAAG,UAAU,SAAS,SACtC,QAA+B,QAC/BC,MAA4C,YAC5C,UAAmB,iBAAwC;AAC7D,OAAK,gBAAe;AACpB,SAAO,OACI,MAAM,QAAQ,QAAQA,MAAK,YAAY,UACvC,eAAe;AAC5B;;;ACRA,qBAAoB,EAAG,UAAU,kBAC7B,SACI,QACA,aACA,SAAkCC,MAClC,iBAAwC;AAC9C,OAAK,gBAAe;AACpB,SAAO,gBACI,MAAM,QAAQ,aAAa,SAASA,MAAK,eAAe;AACrE;;;ACTA,qBAAoB,EAAG,UAAU,SAAS,SACtC,QAA+B,SAC/BC,MAA4B,YAC5B,WACA,iBAAwC;AAC1C,OAAK,gBAAe;AACpB,SAAO,OACI,MAAM,QAAQ,SAASA,MAAK,YAAY,WACxC,eAAe;AAC5B;;;ACXA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AACrC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACHA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACDA,qBAAoB,EAAG,UAAU,UAAU,SACzC,MACA,WACAC,UAAiB;AAEjB,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,MAAM,WAAWA,QAAO;AAC/C;;;ACVA,qBAAoB,EAAG,UAAU,SAAS,SACtC,MAAe,WAAqBC,UAAiB;AACvD,OAAK,gBAAe;AACpB,SAAO,OAAO,MAAM,MAAM,WAAWA,QAAO;AAC9C;;;ACJA,qBAAoB,EAAG,UAAU,eAAe,SAC5C,WAAmB,YAAyB;AAC9C,OAAK,gBAAe;AACpB,SAAO,aAAa,MAAM,WAAW,UAAU;AACjD;;;ACDA,qBAAoB,EAAG,UAAU,kBAC7B,SACI,QAA+B,SAC/BC,MAA4B,YAC5B,WACA,iBAAwC;AAC9C,OAAK,gBAAe;AACpB,SAAO,gBACI,MAAM,QAAQ,SAASA,MAAK,YAAY,WACxC,eAAe;AAC5B;;;ACXA,qBAAoB,EAAG,UAAU,aAC7B,SACI,QAA+B,SAC/BC,MAAqB,WACrB,YAAmB;AACzB,OAAK,gBAAe;AACpB,SAAO,WAAW,MAAM,QAAQ,SAASA,MAAK,WAAW,UAAU;AACrE;;;ACTA,qBAAoB,EAAG,UAAU,WAAW,SACxC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,SAAS,MAAM,CAAC;AACzB;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,SACnC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,CAAC;AACpB;;;ACLA,qBAAoB,EAAG,UAAU,MAAM,SACnC,GAAe;AACjB,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,CAAC;AACpB;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AACrC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACHA,qBAAoB,EAAG,UAAU,QAAQ,SACrC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,MAAM,MAAM,CAAC;AACtB;;;ACFA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AACrC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACHA,qBAAoB,EAAG,UAAU,gBAAgB,SACpC,MAAwB,UAAkB;AACrD,OAAK,gBAAe;AACpB,SAAO,cAAc,MAAM,MAAM,QAAQ;AAC3C;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AACrC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACLA,qBAAoB,EAAG,UAAU,aAAa,SAC1C,MAAa;AACf,OAAK,gBAAe;AACpB,SAAO,WAAW,MAAM,IAAI;AAC9B;;;ACFA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AAErC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACDA,qBAAoB,EAAG,UAAU,UAAU,WAAA;AACzC,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,CAAC,KAAK,IAAI,CAAC;AAClC;;;ACNA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACNA,qBAAoB,EAAG,UAAU,WAAW,SACxC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,SAAS,MAAM,CAAC;AACzB;;;ACFA,qBAAoB,EAAG,UAAU,SAAS,SAC7B,SAA4B,MAAa;AACpD,OAAK,gBAAe;AACpB,SAAO,OAAO,MAAM,SAAS,IAAI;AACnC;;;ACNA,qBAAoB,EAAG,UAAU,eAAe,SAC5C,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,aAAa,MAAM,CAAC;AAC7B;;;ACJA,qBAAoB,EAAG,UAAU,UAAU,SACvC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,CAAC;AACxB;;;ACFA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AAEtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACJA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACJA,qBAAoB,EAAG,UAAU,WAAW,WAAA;AAE1C,OAAK,gBAAe;AACpB,SAAO,SAAS,IAAI;AACtB;;;ACJA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACJA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAOC,OAAM,IAAI;AACnB;;;ACNA,qBAAoB,EAAG,UAAU,YAAY,SAChC,OAAa;AACxB,OAAK,gBAAe;AACpB,SAAO,UAAU,MAAM,KAAK;AAC9B;;;ACJA,qBAAoB,EAAG,UAAU,YAAY,SACzC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,UAAU,MAAM,CAAC;AAC1B;;;ACJA,qBAAoB,EAAG,UAAU,OAAO,SACpC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,KAAK,MAAM,CAAC;AACrB;;;ACHA,qBAAoB,EAAG,UAAU,6BAC7B,SACI,aAAsB,MAAe,OAAgB,MAAa;AACxE,OAAK,gBAAe;AACpB,SAAO,2BAA2B,MAAM,aAAa,MAAM,OAAO,IAAI;AACxE;;;ACJA,qBAAoB,EAAG,UAAU,aAAa,WAAA;AAE5C,OAAK,gBAAe;AACpB,SAAO,WAAW,IAAI;AACxB;;;ACJA,qBAAoB,EAAG,UAAU,aAAa,SACjC,MAAa;AACxB,OAAK,gBAAe;AACpB,SAAO,WAAW,MAAM,IAAI;AAC9B;;;ACLA,qBAAoB,EAAG,UAAU,YAAY,SAChC,MAAwB,UAAkB;AACrD,OAAK,gBAAe;AACpB,SAAO,UAAU,MAAM,MAAM,QAAQ;AACvC;;;ACHA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AACrC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACHA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACNA,qBAAoB,EAAG,UAAU,aAAa,SAC1C,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,WAAW,MAAM,CAAC;AAC3B;;;ACJA,qBAAoB,EAAG,UAAU,aAAa,WAAA;AAC5C,OAAK,gBAAe;AACpB,SAAO,WAAW,IAAI;AACxB;;;ACHA,qBAAoB,EAAG,UAAU,YAAY,SACzC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,UAAU,MAAM,CAAC;AAC1B;;;ACJA,qBAAoB,EAAG,UAAU,aAAa,SAC1C,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,WAAW,MAAM,CAAC;AAC3B;;;ACFA,qBAAoB,EAAG,UAAU,SAAS,SAC7B,GAAsB,YAC/B,YAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,OAAO,MAAM,GAAG,YAAY,UAAU;AAC/C;;;ACHA,qBAAoB,EAAG,UAAU,UAC7B,SACa,YACT,SACAC,MACA,iBAAwC;AAC9C,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,YAAY,SAASA,MAAK,eAAe;AAChE;;;ACXA,qBAAoB,EAAG,UAAU,MAAM,SACnC,MAAwB,UAAkB;AAC5C,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,MAAM,QAAQ;AACjC;;;ACLA,qBAAoB,EAAG,UAAU,UAAU,SACvC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,CAAC;AACxB;;;ACJA,qBAAoB,EAAG,UAAU,OAAO,SACpC,MAAwB,UAAkB;AAC5C,OAAK,gBAAe;AACpB,SAAO,KAAK,MAAM,MAAM,QAAQ;AAClC;;;ACHA,qBAAoB,EAAG,UAAU,MAAM,SACnC,MAAwB,UAAkB;AAC5C,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,MAAM,QAAQ;AACjC;;;ACLA,qBAAoB,EAAG,UAAU,UAAU,SACvC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,CAAC;AACxB;;;ACFA,qBAAoB,EAAG,UAAU,YAAY,SAChC,UACT,MAA2B;AAC7B,OAAK,gBAAe;AACpB,SAAO,UAAU,MAAM,UAAU,IAAI;AACvC;;;ACPA,qBAAoB,EAAG,UAAU,MAAM,SACnC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,CAAC;AACpB;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,SACnC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,CAAC;AACpB;;;ACFA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AACrC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACDA,qBAAoB,EAAG,UAAU,OAAO,SACpC,KAAgC,MAChC,UAAkB;AACpB,OAAK,gBAAe;AACpB,SAAO,KAAK,MAAM,KAAK,MAAM,QAAQ;AACvC;;;ACTA,qBAAoB,EAAG,UAAU,WAAW,SACxC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,SAAS,MAAM,CAAC;AACzB;;;ACHA,qBAAoB,EAAG,UAAU,SAAS,SACtC,OAAe,UAAU,GAAG,WAAW,GAAC;AAC1C,OAAK,gBAAe;AACpB,SAAO,OAAO,MAAM,OAAO,SAAS,QAAQ;AAC9C;;;ACHA,qBAAoB,EAAG,UAAU,WAAW,WAAA;AAE1C,OAAK,gBAAe;AACpB,SAAO,SAAS,IAAI;AACtB;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,SAC1B,UAAmC,eAAqB;AACnE,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,UAAU,aAAa;AAC1C;;;ACAA,qBAAoB,EAAG,UAAU,OAAO,SAC3B,aAAsC,aAC/C,SACA,cACA,SACA,iBAAwC;AAC1C,OAAK,gBAAe;AACpB,SAAO,KAAK,MAAM,aAAa,aAAa,SAAS,cAAc,SACvD,eAAe;AAC7B;;;ACfA,qBAAoB,EAAG,UAAU,MAAM,SACnCC,MAAsB;AACxB,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAMA,IAAG;AACtB;;;ACJA,qBAAoB,EAAG,UAAU,QAAQ,SAC5B,OAAmB;AAC9B,OAAK,gBAAe;AACpB,SAAO,MAAM,MAAM,KAAK;AAC1B;;;ACHA,qBAAoB,EAAG,UAAU,OAAO,SAC3B,MAAwB,UAAkB;AACrD,OAAK,gBAAe;AACpB,SAAO,KAAK,MAAM,MAAM,QAAQ;AAClC;;;ACHA,qBAAoB,EAAG,UAAU,aAAa,WAAA;AAE5C,OAAK,gBAAe;AACpB,SAAO,WAAW,IAAI;AACxB;;;ACNA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACHA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACIA,qBAAoB,EAAG,UAAU,YAAY,SAA2B,GAAI;AAE1E,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,EAAE,KAAK;AAC9B;;;ACZA,qBAAoB,EAAG,UAAU,UAAU,SACvC,OAAe;AACjB,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,KAAK;AAC5B;;;ACFA,qBAAoB,EAAG,UAAU,iBAC7B,SACa,YAA8B,cACvC,kBAA0B;AAChC,OAAK,gBAAe;AACpB,SAAO,eAAe,MAAM,YAAY,cAAc,gBAAgB;AACxE;;;ACNA,qBAAoB,EAAG,UAAU,wBAC7B,SACa,YAA8B,cACvC,kBAA0B;AAChC,OAAK,gBAAe;AACpB,SAAO,sBACH,MAAM,YAAY,cAAc,gBAAgB;AACtD;;;ACTA,qBAAoB,EAAG,UAAU,UAAU,SAC9B,MAAsB;AACjC,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,IAAI;AAC3B;;;ACFA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AAEtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACJA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACJA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,MAAM,IAAI;AACnB;;;ACNA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACCA,qBAAoB,EAAG,UAAU,kBAC7B,SACI,iBACA,iBAAsC,SACtCC,MAAqB,UACrB,YAA0B;AAChC,OAAK,gBAAe;AACpB,SAAO,gBACI,MAAM,iBAAiB,iBAAiB,SAASA,MAAK,UACtD,UAAU;AACvB;;;ACZA,qBAAoB,EAAG,UAAU,UAAU,WAAA;AAEzC,OAAK,gBAAe;AACpB,SAAO,QAAQ,IAAI;AACrB;;;ACJA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACHA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AACrC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACHA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACFA,qBAAoB,EAAG,UAAU,QAAQ,SAC5B,OAAwB,MAAsB;AACzD,OAAK,gBAAe;AACpB,SAAO,MAAM,MAAM,OAAO,IAAI;AAChC;;;ACLA,qBAAoB,EAAG,UAAU,UAAU,SAC9B,KAAW;AACtB,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,GAAG;AAC1B;;;ACJA,qBAAoB,EAAG,UAAU,WAAW,WAAA;AAE1C,OAAK,gBAAe;AACpB,SAAO,SAAS,IAAI;AACtB;;;ACLA,qBAAoB,EAAG,UAAU,iBAAiB,SAC9C,YAAsB,UAAoB;AAC5C,OAAK,gBAAe;AACpB,SAAO,eAAe,MAAM,YAAY,QAAQ;AAClD;;;ACJA,qBAAoB,EAAG,UAAU,QAAQ,SACrC,iBAAkC,MAAa;AACjD,OAAK,gBAAe;AACpB,SAAO,MAAM,MAAM,iBAAiB,IAAI;AAC1C;;;ACHA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACHA,qBAAoB,EAAG,UAAU,SAAS,WAAA;AAExC,OAAK,gBAAe;AACpB,SAAO,OAAO,IAAI;AACpB;;;ACLA,qBAAoB,EAAG,UAAU,oBAAoB,SACjD,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,kBAAkB,MAAM,CAAC;AAClC;;;ACLA,qBAAoB,EAAG,UAAU,UAAU,SACvC,MAAe;AACjB,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,IAAI;AAC3B;;;ACJA,qBAAoB,EAAG,UAAU,QAAQ,SACrC,GAAoB,MAAa;AACnC,OAAK,gBAAe;AACpB,QAAM,qBAAqB,aAAa,SAAS,CAAC,MAAM,CAAC,IAAI,CAAC,MAAM,GAAG,CAAC;AACxE,SAAO,MAAM,oBAAoB,IAAI;AACvC;;;ACHA,qBAAoB,EAAG,UAAU,OAAO,SAC3B,OAAc;AACzB,OAAK,gBAAe;AACpB,SAAO,KAAK,MAAM,KAAK;AACzB;;;ACDA,qBAAoB,EAAG,UAAU,eAAe,SAC9B,OAAiB,KAAe,SAC9C,WAAoB,SAAkB,cACtC,aAAsB,gBAAuB;AAC/C,OAAK,gBAAe;AACpB,SAAO,aACI,MAAM,OAAO,KAAK,SAAS,WAAW,SAAS,cAC/C,aAAa,cAAc;AACxC;;;ACbA,qBAAoB,EAAG,UAAU,MAAM,SACnC,GAAoB;AACtB,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,CAAC;AACpB;;;ACJA,qBAAoB,EAAG,UAAU,MAAM,SACnC,MAAwB,UAAkB;AAC5C,OAAK,gBAAe;AACpB,SAAO,IAAI,MAAM,MAAM,QAAQ;AACjC;;;ACFA,qBAAoB,EAAG,UAAU,MAAM,WAAA;AACrC,OAAK,gBAAe;AACpB,SAAO,IAAI,IAAI;AACjB;;;ACHA,qBAAoB,EAAG,UAAU,OAAO,WAAA;AACtC,OAAK,gBAAe;AACpB,SAAO,KAAK,IAAI;AAClB;;;ACJA,qBAAoB,EAAG,UAAU,OAAO,SACpC,MAAc;AAChB,OAAK,gBAAe;AACpB,SAAO,KAAK,MAAM,IAAI;AACxB;;;ACEA,qBAAoB,EAAG,UAAU,SAAS,WAAA;AAExC,OAAK,gBAAe;AACpB,SAAO,KAAQ,MAAM,MAAM;AAC7B;;;ACJA,qBAAoB,EAAG,UAAU,UAAU,WAAA;AAEzC,OAAK,gBAAe;AACpB,SAAO,KAAQ,MAAM,SAAS;AAChC;;;ACJA,qBAAoB,EAAG,UAAU,QAAQ,WAAA;AAEvC,OAAK,gBAAe;AACpB,SAAO,KAAQ,MAAM,OAAO;AAC9B;;;ACTA,qBAAoB,EAAG,UAAU,OAAO,SAC3B,GAAY,QAAgB;AACvC,OAAK,gBAAe;AACpB,SAAO,KAAK,MAAM,GAAG,MAAM;AAC7B;;;ACLA,qBAAoB,EAAG,UAAU,YAAY,SAChC,MAAe;AAC1B,OAAK,gBAAe;AACpB,SAAO,UAAU,MAAM,IAAI;AAC7B;;;ACJA,qBAAoB,EAAG,UAAU,SAAS,SAC7B,MAAa;AACxB,OAAK,gBAAe;AACpB,SAAO,OAAO,MAAM,IAAI;AAC1B;;;ACHA,qBAAoB,EAAG,UAAU,qBAC7B,SACa,YAAmC,aAAmB;AACrE,OAAK,gBAAe;AACpB,SAAO,mBAAmB,MAAM,YAAY,WAAW;AACzD;;;ACPA,qBAAoB,EAAG,UAAU,UAAU,SACvC,MAAa;AACf,OAAK,gBAAe;AACpB,SAAO,QAAQ,MAAM,IAAI;AAC3B;;;ACFA,qBAAoB,EAAG,UAAU,QAAQ,SACrC,WAA8B,GAAoB;AACpD,OAAK,gBAAe;AACpB,SAAO,MAAM,WAAW,MAAM,CAAC;AACjC;;;ACJA,qBAAoB,EAAG,UAAU,YAAY,WAAA;AAE3C,OAAK,gBAAe;AACpB,SAAO,UAAU,IAAI;AACvB;;;ACbA,IAAM,YAAY,qBAAa;AAiBzB,IAAO,iBAAP,MAAO,wBAAuB,cAAa;EAU/C,cAAA;AACE,UAAK;AAVA,SAAA,YAAY;AAGX,SAAA,WAAW;AAQjB,SAAK,OAAO,IAAI,YAAY,MAAM,OAAM,CAAE;EAC5C;EAPQ,aAAU;AAChB,WAAO,gBAAe;EACxB;EAOA,MAAM,QAAoC,OAAiB,OAAe;AAExE,QAAI,KAAK,UAAU;AACjB,WAAK,WAAW;AAChB,UAAI,IAAG,EAAG,IAAI,SAAS,GAAG;AACxB,6BAAa,KACT,oPAIgC;;;AAGxC,UAAM,SAAS,EAAC,IAAI,KAAK,WAAU,EAAE;AAErC,SAAK,KAAK,IAAI,QAAQ,EAAC,QAAQ,OAAO,UAAU,EAAC,CAAC;AAElD,WAAO;EACT;;;;;;;EAQA,eACI,OAAiB,OACjB,QAA4C;AAC9C,QAAI;AACJ,QAAI,UAAU,YAAY,UAAU,QAAQ,OAAO,SAAS,KACxD,aAAK,SAAS,OAAO,CAAC,CAAC,GAAG;AAC5B,YAAM,gBACD,OAA0B,IAAI,OAAK,aAAK,aAAa,CAAC,CAAC;AAE5D,cAAQ,KAAK,MAAM,eAAe,OAAO,KAAK;WACzC;AACL,cAAQ,KAAK,MAAM,QAAsB,OAAO,KAAK;;AAGvD,WAAO,EAAC,QAAQ,OAAO,OAAO,MAAK;EACrC;;EAGA,SAAS,QAAc;AACrB,QAAI,KAAK,KAAK,IAAI,MAAM,GAAG;AACzB,YAAM,aAAa,KAAK,KAAK,IAAI,MAAM;AACvC,aAAO,WAAW;;AAEpB,WAAO;EACT;;EAGA,OAAO,QAAc;AACnB,UAAM,aAAa,KAAK,KAAK,IAAI,MAAM;AACvC,eAAW;EACb;;EAGA,OAAO,QAAc;AACnB,QAAI,KAAK,KAAK,IAAI,MAAM,GAAG;AACzB,YAAM,aAAa,KAAK,KAAK,IAAI,MAAM;AACvC,iBAAW;;EAEf;EAEA,KACI,QAAgB,QAAoC,OACpD,OAAiB,UAAgB;AACnC,SAAK,KAAK,IAAI,QAAQ,EAAC,QAAQ,OAAO,SAAQ,CAAC;EACjD;EAEA,aAAU;AACR,WAAO,KAAK,KAAK,WAAU;EAC7B;EAEA,MAAM,KAAK,QAAc;AACvB,WAAO,KAAK,SAAS,MAAM;EAC7B;EACA,SAAS,QAAc;AACrB,UAAM,EAAC,OAAO,mBAAkB,IAAI,KAAK,KAAK,IAAI,MAAM;AAExD,QAAI,UAAU,aAAa;AACzB,YAAM,aACF,KAAK,SAAS,mBAAmB,KAAK,MAAM;AAChD,YAAM,aACF,KAAK,SAAS,mBAAmB,KAAK,MAAM;AAChD,aAAO,qBAAa,uBAAuB,YAAY,UAAU;;AAGnE,WAAO,KAAK,KAAK,IAAI,MAAM,EAAE;EAC/B;EAEA,WAA+C,GAAa;AAE1D,UAAM,OAAO,KAAK,SAAS,EAAE,MAAM;AACnC,QAAI,EAAE,UAAU,UAAU;AACxB,UAAI;AAEF,cAAM,UAAW,KAAsB,IAAI,OAAK,aAAK,aAAa,CAAC,CAAC;AACpE,eAAO,OAAO,EAAE,OAAsB,EAAE,OAAO,OAAO;eAEtD,IAAM;AACN,cAAM,IAAI,MAAM,kDAAkD;;;AAGtE,WAAO,OAAO,EAAE,OAAsB,EAAE,OAAO,IAAkB;EAEnE;EAEA,WACI,QAAoC,OAAiB,OAAe;AACtE,WAAO,OAAM,EAAG,yBACL,KAAK,eAAe,OAAO,OAAO,MAAM,GAAG,IAAI;EAC5D;;;;;;;;EASA,YAAY,QAAgB,QAAQ,OAAK;AACvC,QAAI,KAAK,KAAK,IAAI,MAAM,GAAG;AACzB,WAAK,KAAK,IAAI,MAAM,EAAE;AACtB,UAAI,CAAC,SAAS,KAAK,KAAK,IAAI,MAAM,EAAE,WAAW,GAAG;AAChD,eAAO;;AAGT,YAAM,EAAC,mBAAkB,IAAI,KAAK,KAAK,IAAI,MAAM;AAEjD,UAAI,sBAAsB,MAAM;AAC9B,aAAK,YAAY,mBAAmB,KAAK,QAAQ,IAAI;AACrD,aAAK,YAAY,mBAAmB,KAAK,QAAQ,IAAI;;AAGvD,WAAK,KAAK,OAAO,MAAM;;AAEzB,WAAO;EACT;EAEA,8BAA8B,YAAsB;AAClD,SAAK,YAAY,WAAW,MAAM;EACpC;EAEA,MAAM,KAAK,GAAa;AACtB,UAAM,QAAQ,aAAK,IAAG;AACtB,MAAC;AACD,UAAM,WAAW,aAAK,IAAG,IAAK;AAC9B,WAAO,EAAC,SAAQ;EAClB;EAEA,SAAM;AACJ,WAAO;;MAEL,YAAY;MACZ,SACI,CAAC,oHACoD;;EAE7D;EAEA,MAAM,WAAiB;AACrB,qBAAiB,CAAC,SAAS,GAAG,OAAO;AAErC,UAAM,WAAW,KAAK,SAAS,UAAU,MAAM;AAC/C,WAAO,UAAU,UAAU,OAAO,QAAQ;EAC5C;EAEA,UAAO;EAAI;EAEX,iBAAc;AACZ,WAAO;EACT;;EAGA,UAAO;AACL,WAAO,MAAM,QAAO;EACtB;;AA7Le,eAAA,aAAa;;;ACtC9B,IAAMC,WAAU;;;AC2BhB;EAAgB;EAAO,MAAM,IAAI,eAAc;EAAI;;AAAgB;;;ACT5D,IAAMC,OACT,gBAAgB,KAAK,CAAC,OAAO,MAAM,IAAI,KAAM,KAAK,IAAI,EAAE,IAAI,CAAE;AAE3D,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLR,SAAUC,WAAU,MAIzB;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,MAAK,IAAI;AAEhB,mBAAiB,CAAC,CAAC,GAAG,WAAW;AAEjC,QAAM,QAAQ,aAAK,cAAc,EAAE,KAAK;AACxC,QAAM,QAAQA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,UAAU,aAAK,uBAAuB,WAAW,KAAK;AAE5D,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,YAAQ,CAAC,IAAI,MAAM,CAAC,IAAI,IAAI,QAAQ,MAAM,CAAC,IAAI,MAAM,CAAC;;AAGxD,SAAOA,SAAQ,eAAe,EAAE,OAAO,WAAW,OAAO;AAC3D;AAEO,IAAM,kBAAgC;EAC3C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACxBd,IAAM,YAAY,6BACd,CAAC,QAAgB,WAAmB,SAAS,IAAI,SAAS,SAAS,MAAM;AAEvE,SAAUE,OAAM,MAAoD;AAExE,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,GAAG,MAAK,IAAI;AAEnB,mBAAiB,CAAC,GAAG,KAAK,GAAG,OAAO;AAEpC,QAAM,QAAQA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,QAAQA,SAAQ,KAAK,IAAI,MAAM,MAAM,EAAE;AAE7C,QAAM,CAAC,YAAY,WAAW,IAC1B,UAAU,EAAE,OAAO,MAAM,OAAO,OAAO,OAAO,SAAS;AAE3D,SAAOA,SAAQ,eAAe,aAAa,WAAW,UAAU;AAClE;AAEO,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACxBP,IAAME,QAAO,gBAAgB,MAAM,CAAC,OAAO,KAAK,IAAI,GAAG,EAAE,CAAC;AAE1D,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLP,IAAMC,SACT,gBAAgB,OAAO,CAAC,OAAO,KAAK,IAAI,KAAK,IAAI,GAAG,EAAE,GAAG,CAAC,CAAC;AAExD,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACCR,SAAU,gBACZC,UAAyB,GAAe,YACxC,wBAAqC,gBAAuB;AAC9D,MAAI,eAAe,UAAU;AAC3B,WAAO,SAAS,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAA,SAAO,CAAC;aAC7B,eAAe,QAAQ;AAChC,WAAOC,MAAK,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,SAAO,CAAC;aACzB,eAAe,OAAO;AAC/B,WAAOE,KAAI,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAF,SAAO,CAAC;aACxB,eAAe,SAAS;AACjC,WAAOG,OAAM,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAH,SAAO,CAAC;aAC1B,eAAe,SAAS;AACjC,WAAOI,OAAM,EAAC,QAAQ,EAAC,GAAG,OAAO,uBAAsB,GAAG,SAAAJ,SAAO,CAAC;aACzD,eAAe,aAAa;AACrC,WAAOK,WAAU,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAL,UAAS,OAAO,EAAC,OAAO,eAAc,EAAC,CAAC;aAC9D,eAAe,WAAW;AACnC,WAAOM,SAAQ,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAN,SAAO,CAAC;;AAEvC,QAAM,IAAI,MACN,cAAc,UAAU,gDAAgD;AAC9E;;;AC3BM,SAAUO,SACZ,MACyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,MAAK,IAAI;AAEhB,QAAM,QAAQ,aAAK,cAAc,EAAE,KAAK;AACxC,QAAM,SAAS,aAAK,uBAAuB,OAAO,KAAK;AACvD,QAAM,SAAS,aAAK,cAAc,MAAM;AAExC,eAAK,OACD,UAAU,QACV,MAAM,kBAAkB,MAAM,SAAS,MAAM,gCAC/B,EAAE,KAAK,SAAS,KAAK,+EACe;AAEtD,EAAAA,SAAQ,OAAO,EAAE,MAAM;AAEvB,QAAM,QAAQA,SAAQ,KAAK,IAAI,EAAE,MAAM;AAEvC,MAAI,MAAM,sBAAsB,MAAM;AACpC,UAAMC,QAAO,MAAM,mBAAmB;AACtC,UAAMC,QAAO,MAAM,mBAAmB;AAEtC,IAAAD,MAAK,QAAQ;AACb,IAAAC,MAAK,QAAQ;;AAGf,SAAO,EAAC,QAAQ,EAAE,QAAQ,OAAO,QAAQ,OAAO,EAAE,MAAK;AACzD;AAEO,IAAM,gBAA8B;EACzC,YAAY;EACZ,aAAa;EACb,YAAYH;;;;ACjCR,SAAU,YAAY,MAI3B;AACC,QAAM,EAAC,QAAQ,SAAAI,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,EAAC,IAAI;AACf,QAAM,EAAC,YAAY,WAAU,IAAI;AAEjC,mBAAiB,CAAC,GAAG,CAAC,GAAG,QAAQ;AAEjC,QAAM,QAAQ,EAAE,MAAM;AACtB,QAAM,QAAQ,EAAE,MAAM;AAEtB,QAAM,cAAc,aAAa,EAAE,MAAM,QAAQ,CAAC,IAAI,EAAE,MAAM,QAAQ,CAAC;AACvE,QAAM,cAAc,aAAa,EAAE,MAAM,QAAQ,CAAC,IAAI,EAAE,MAAM,QAAQ,CAAC;AAEvE,QAAM,cAAc,aAAa,EAAE,MAAM,QAAQ,CAAC,IAAI,EAAE,MAAM,QAAQ,CAAC;AACvE,QAAM,cAAc,aAAa,EAAE,MAAM,QAAQ,CAAC,IAAI,EAAE,MAAM,QAAQ,CAAC;AAEvE,QAAM,aAAa,EAAE,MAAM,MAAM,GAAG,EAAE;AACtC,QAAM,aAAa,EAAE,MAAM,MAAM,GAAG,EAAE;AAEtC,QAAM,YAAY,aAAK,cAAc,UAAU;AAC/C,QAAM,YAAY,aAAK,cAAc,UAAU;AAE/C,QAAM,oBAAoB,uBAAe,2BACrC,EAAE,MAAM,MAAM,GAAG,EAAE,GAAG,EAAE,MAAM,MAAM,GAAG,EAAE,CAAC;AAC9C,QAAM,WAAW,kBAAkB,OAAO,CAAC,aAAa,WAAW,CAAC;AAEpE,eAAK,OACD,gBAAgB,aAChB,MAAM,kCAAkC,WAAW,UAC5C,WAAW,4BAA4B,EAAE,KAAK,QAC9C,EAAE,KAAK,mBAAmB,UAAU,mBACpB,UAAU,cAAc;AAEnD,QAAM,WAAW,aAAa,CAAC,WAAW,aAAa,WAAW,IACpC,CAAC,WAAW,aAAa,WAAW;AAClE,QAAM,WAAW,aAAa,CAAC,WAAW,aAAa,WAAW,IACpC,CAAC,WAAW,aAAa,WAAW;AAGlE,QAAM,MAAMC,SAAQ,EAAC,QAAQ,EAAC,GAAG,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,OAAO,SAAQ,EAAC,CAAC;AACvE,QAAM,MAAMC,SAAQ,EAAC,QAAQ,EAAC,GAAG,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,OAAO,SAAQ,EAAC,CAAC;AAEvE,QAAM,YAAY,aAAa,IAAI,MAAM,CAAC,IAAI,IAAI,MAAM,CAAC;AACzD,QAAM,UAAU,aAAa,IAAI,MAAM,CAAC,IAAI,IAAI,MAAM,CAAC;AACvD,QAAM,WAAW,aAAa,IAAI,MAAM,CAAC,IAAI,IAAI,MAAM,CAAC;AACxD,QAAM,WAAW,KAAK,IAAI,WAAW,SAAS;AAE9C,QAAM,YAAYA,SAAQ,KAAK,IAAI,IAAI,MAAM,EAAE;AAC/C,QAAM,YAAYA,SAAQ,KAAK,IAAI,IAAI,MAAM,EAAE;AAE/C,QAAM,aAAa,aAAK,eAAe,IAAI,KAAK;AAChD,QAAM,aAAa,aAAK,eAAe,IAAI,KAAK;AAEhD,QAAM,CAAC,QAAQ,YAAY,UAAU,IAAI,aACrC,CAAC,WAAW,CAAC,GAAG,GAAG,WAAW,CAAC,CAAC,IAChC,CAAC,WAAW,CAAC,GAAG,WAAW,CAAC,GAAG,CAAC;AACpC,QAAM,CAAC,YAAY,YAAY,MAAM,IAAI,aACrC,CAAC,GAAG,WAAW,CAAC,GAAG,WAAW,CAAC,CAAC,IAChC,CAAC,WAAW,CAAC,GAAG,GAAG,WAAW,CAAC,CAAC;AAEpC,QAAM,OAAO,UAAU;AACvB,QAAM,SAAS,OAAO,CAAC,UAAU,SAAS,QAAQ,GAAG,IAAI,KAAK;AAE9D,QAAM,UAAU,OAAO;AACvB,QAAM,YAAYA,SAAQ;AAE1B,WAAS,KAAK,GAAG,KAAK,UAAU,MAAM;AACpC,aAAS,KAAK,GAAG,KAAK,SAAS,MAAM,WAAW;AAC9C,eAAS,KAAK,GAAG,KAAK,UAAU,MAAM,WAAW;AAC/C,iBAAS,KAAK,GAAG,KAAK,WAAW,MAAM,WAAW;AAEhD,gBAAM,SAAS,KAAK,IAAI,KAAK,WAAW,OAAO;AAC/C,gBAAM,SAAS,KAAK,IAAI,KAAK,WAAW,QAAQ;AAChD,gBAAM,SAAS,KAAK,IAAI,KAAK,WAAW,SAAS;AAEjD,mBAAS,IAAI,IAAI,IAAI,QAAQ,KAAK;AAChC,qBAAS,IAAI,IAAI,IAAI,QAAQ,KAAK;AAChC,kBAAIE,OAAM;AAEV,uBAAS,IAAI,IAAI,IAAI,QAAQ,KAAK;AAChC,sBAAM,eAAe,KAAK,IAAI,IAAI,YAAY,CAAC,IAAI;AACnD,sBAAM,eAAe,KAAK,IAAI,IAAI,YAAY,CAAC,IAAI;AACnD,sBAAM,OACF,UAAU,eAAe,IAAI,aAAa,IAAI,UAAU;AAC5D,sBAAM,OACF,UAAU,IAAI,aAAa,IAAI,aAAa,YAAY;AAC5D,gBAAAA,QAAO,OAAO;;AAEhB,sBAAQ,KAAK,QAAQ,IAAI,WAAW,EAAE,KAAKA;;;;;;;AAQvD,EAAAF,SAAQ,8BAA8B,GAAG;AACzC,EAAAA,SAAQ,8BAA8B,GAAG;AAGzC,SAAOA,SAAQ,eACX,UAAU,OAAO,OAAO,OAAO,MAAoB;AACzD;AAEO,IAAM,oBAAkC;EAC7C,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC9GR,SAAU,aAAa,MAI5B;AACC,QAAM,EAAC,QAAQ,SAAAG,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,GAAG,MAAM,uBAAsB,IAAI;AAC7C,QAAM,EAAC,YAAY,YAAY,YAAY,eAAc,IAAI;AAE7D,MAAI;AACJ,MAAI;AACJ,MAAI;AAEJ,QAAM,gBAA8B,CAAA;AAEpC,QAAM,YACF,YAAY,EAAC,QAAQ,EAAC,GAAG,EAAC,GAAG,OAAO,EAAC,YAAY,WAAU,GAAG,SAAAA,SAAO,CAAC;AAC1E,YAAU;AAEV,MAAI,MAAM;AACR,aAASC,KAAI,EAAC,QAAQ,EAAC,GAAG,SAAS,GAAG,KAAI,GAAG,SAAAD,SAAO,CAAC;AACrD,kBAAc,KAAK,OAAO;AAC1B,cAAU;;AAEZ,MAAI,YAAY;AACd,oBAAgB,gBACZA,UAAS,SAAS,YAAY,wBAAwB,cAAc;AACxE,kBAAc,KAAK,OAAO;AAC1B,cAAU;;AAGZ,aAAW,KAAK,eAAe;AAC7B,IAAAA,SAAQ,8BAA8B,CAAC;;AAGzC,SAAO;AACT;AAEO,IAAM,qBAAmC;EAC9C,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC7CP,IAAME,QAAO,gBAAgB,MAAM,CAAC,OAAO,KAAK,KAAK,EAAE,CAAC;AAExD,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLP,IAAMC,SAAQ,gBAAgB,OAAO,CAAC,OAAO,KAAK,MAAM,EAAE,CAAC;AAE3D,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACJR,SAAUC,MAAK,MAAmD;AAEtE,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,UAAU;AAEhB,mBAAiB,QAAQ,MAAM;AAE/B,QAAM,OACF,QAAQ,IAAI,OAAKA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE,MAAoB;AACpE,QAAM,SAAS,OAAO,QAAQ,CAAC,EAAE,OAAO,QAAQ,CAAC,EAAE,KAAkB;AACrE,QAAM,UAAU,OAAO;AACvB,WAAS,IAAI,GAAG,IAAI,QAAQ,QAAQ,KAAK;AACvC,UAAM,WAAW,KAAK,CAAC;AACvB,aAAS,IAAI,GAAG,IAAI,QAAQ,QAAQ,KAAK;AACvC,cAAQ,CAAC,KAAK,SAAS,CAAC;;;AAI5B,SAAOA,SAAQ,eAAe,OAAO,OAAO,OAAO,OAAO,OAAO,MAAM;AACzE;AAEO,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACtBR,SAAUE,KACZ,MAAmE;AAErE,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,MAAM,SAAQ,IAAI;AAEzB,mBAAiB,GAAG,KAAK;AAEzB,QAAM,WAAW,aAAK,eAAe,MAAM,EAAE,KAAK;AAClD,MAAI,OAAO;AACX,QAAM,eAAe,qBAAa,mBAAmB,MAAM,EAAE,MAAM,MAAM;AACzE,MAAI,KAAK;AACT,MAAI,gBAAgB,MAAM;AACxB,SAAKC,WAAU,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,MAAM,aAAY,EAAC,CAAC;AAClE,WAAO,qBAAa,iBAAiB,KAAK,QAAQ,EAAE,MAAM,MAAM;;AAGlE,uBAAa,2BAA2B,OAAO,MAAM,GAAG,MAAM,MAAM;AACpE,QAAM,CAAC,UAAU,WAAW,IACxB,qBAAa,0BAA0B,GAAG,OAAO,IAAI;AACzD,QAAM,aAAa,aAAK,cAAc,WAAW;AACjD,QAAM,OAAO,aAAK,oBAAoB,aAAK,cAAc,QAAQ,GAAG,GAAG,KAAK;AAE5E,QAAM,QAAQA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC1C,WAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,EAAE,GAAG;AACpC,UAAM,SAAS,IAAI;AACnB,QAAID,OAAM,MAAM,MAAM;AACtB,aAAS,IAAI,GAAG,IAAI,YAAY,EAAE,GAAG;AACnC,YAAM,QAAQ,MAAM,SAAS,CAAC;AAC9B,MAAAA,OAAMA,QAAO;;AAEf,SAAK,CAAC,IAAIA;;AAGZ,MAAI,gBAAgB,MAAM;AACxB,IAAAC,SAAQ,8BAA8B,EAAE;;AAG1C,QAAM,SAASA,SAAQ,eAAe,UAAU,GAAG,OAAO,IAAI;AAE9D,MAAI,UAAU;AACZ,UAAM,gBAAgB,qBAAa,qBAAqB,UAAU,QAAQ;AAC1E,UAAM,iBACFE,SAAQ,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,cAAa,EAAC,CAAC;AAEzE,IAAAA,SAAQ,8BAA8B,MAAM;AAE5C,WAAO;;AAGT,SAAO;AACT;AAEO,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACzDR,SAAUI,KACZ,MAAmE;AAErE,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,MAAM,SAAQ,IAAI;AAEzB,mBAAiB,GAAG,KAAK;AAEzB,QAAM,WAAW,aAAK,eAAe,MAAM,EAAE,KAAK;AAClD,MAAI,OAAO;AACX,QAAM,eAAe,qBAAa,mBAAmB,MAAM,EAAE,MAAM,MAAM;AACzE,MAAI,KAAK;AACT,MAAI,gBAAgB,MAAM;AACxB,SAAKC,WAAU,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,MAAM,aAAY,EAAC,CAAC;AAClE,WAAO,qBAAa,iBAAiB,KAAK,QAAQ,EAAE,MAAM,MAAM;;AAGlE,uBAAa,2BAA2B,OAAO,MAAM,GAAG,MAAM,MAAM;AACpE,QAAM,CAAC,UAAU,WAAW,IACxB,qBAAa,0BAA0B,GAAG,OAAO,IAAI;AACzD,QAAM,aAAa,aAAK,cAAc,WAAW;AACjD,QAAM,OAAO,aAAK,oBAAoB,aAAK,cAAc,QAAQ,GAAG,GAAG,KAAK;AAE5E,QAAM,QAAQA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC1C,WAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,EAAE,GAAG;AACpC,UAAM,SAAS,IAAI;AACnB,QAAI,SAAS,MAAM,MAAM;AACzB,aAAS,IAAI,GAAG,IAAI,YAAY,EAAE,GAAG;AACnC,YAAM,QAAQ,MAAM,SAAS,CAAC;AAC9B,eAAS,UAAU;;AAErB,SAAK,CAAC,IAAI;;AAGZ,MAAI,gBAAgB,MAAM;AACxB,IAAAA,SAAQ,8BAA8B,EAAE;;AAG1C,QAAM,SAASA,SAAQ,eAAe,UAAU,GAAG,OAAO,IAAI;AAE9D,MAAI,UAAU;AACZ,UAAM,gBAAgB,qBAAa,qBAAqB,UAAU,QAAQ;AAC1E,UAAM,iBACFE,SAAQ,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,cAAa,EAAC,CAAC;AAEzE,IAAAA,SAAQ,8BAA8B,MAAM;AAE5C,WAAO;;AAGT,SAAO;AACT;AAEO,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC1DR,SAAUI,QACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,KAAI,IAAI;AAEf,mBAAiB,GAAG,QAAQ;AAE5B,MAAI,OAAO,aAAK,eAAe,MAAM,EAAE,KAAK;AAC5C,QAAM,eAAe,qBAAa,mBAAmB,MAAM,EAAE,MAAM,MAAM;AACzE,MAAI,KAAK;AACT,QAAM,0BAA0B,CAAA;AAChC,MAAI,gBAAgB,MAAM;AACxB,SAAKC,WAAU,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,MAAM,aAAY,EAAC,CAAC;AAClE,4BAAwB,KAAK,EAAE;AAC/B,WAAO,qBAAa,iBAAiB,KAAK,QAAQ,GAAG,MAAM,MAAM;;AAGnE,SAAO,CAAC,KAAK,CAAC,CAAC;AACf,uBAAa,2BAA2B,UAAU,MAAM,GAAG,MAAM,MAAM;AACvE,QAAM,CAAC,UAAU,WAAW,IACxB,qBAAa,0BAA0B,GAAG,OAAO,IAAI;AAEzD,QAAM,UAAU,aAAK,cAAc,QAAQ;AAC3C,QAAM,OAAO,aAAK,oBAAoB,SAAS,OAAO;AACtD,QAAM,aAAa,aAAK,cAAc,WAAW;AAEjD,QAAM,QAAQA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC1C,WAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,EAAE,GAAG;AACpC,UAAM,SAAS,IAAI;AACnB,QAAIE,OAAM,MAAM,MAAM;AACtB,QAAI,WAAW;AACf,aAAS,IAAI,GAAG,IAAI,YAAY,EAAE,GAAG;AACnC,YAAM,QAAQ,MAAM,SAAS,CAAC;AAC9B,UAAI,QAAQA,MAAK;AACf,QAAAA,OAAM;AACN,mBAAW;;;AAGf,SAAK,CAAC,IAAI;;AAGZ,0BAAwB,QACpB,OAAKF,SAAQ,8BAA8B,CAAC,CAAC;AAEjD,SAAOA,SAAQ,eAAe,UAAU,SAAS,IAAI;AACvD;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACpDR,SAAUI,QACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,KAAI,IAAI;AAEf,mBAAiB,GAAG,QAAQ;AAE5B,MAAI,OAAO,aAAK,eAAe,MAAM,EAAE,KAAK;AAC5C,QAAM,eAAe,qBAAa,mBAAmB,MAAM,EAAE,MAAM,MAAM;AACzE,MAAI,KAAK;AACT,QAAM,0BAA0B,CAAA;AAChC,MAAI,gBAAgB,MAAM;AACxB,SAAKC,WAAU,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,MAAM,aAAY,EAAC,CAAC;AAClE,4BAAwB,KAAK,EAAE;AAC/B,WAAO,qBAAa,iBAAiB,KAAK,QAAQ,GAAG,MAAM,MAAM;;AAGnE,SAAO,CAAC,KAAK,CAAC,CAAC;AACf,uBAAa,2BAA2B,UAAU,MAAM,GAAG,MAAM,MAAM;AACvE,QAAM,CAAC,UAAU,WAAW,IACxB,qBAAa,0BAA0B,GAAG,OAAO,IAAI;AAEzD,QAAM,UAAU,aAAK,cAAc,QAAQ;AAC3C,QAAM,OAAO,aAAK,oBAAoB,SAAS,OAAO;AACtD,QAAM,aAAa,aAAK,cAAc,WAAW;AAEjD,QAAM,QAAQA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC1C,WAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,EAAE,GAAG;AACpC,UAAM,SAAS,IAAI;AACnB,QAAIE,OAAM,MAAM,MAAM;AACtB,QAAI,WAAW;AACf,aAAS,IAAI,GAAG,IAAI,YAAY,EAAE,GAAG;AACnC,YAAM,QAAQ,MAAM,SAAS,CAAC;AAC9B,UAAI,QAAQA,MAAK;AACf,QAAAA,OAAM;AACN,mBAAW;;;AAGf,SAAK,CAAC,IAAI;;AAGZ,0BAAwB,QACpB,OAAKF,SAAQ,8BAA8B,CAAC,CAAC;AAEjD,SAAOA,SAAQ,eAAe,UAAU,SAAS,IAAI;AACvD;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACtDP,IAAMI,QAAO,gBAAgB,MAAM,CAAC,OAAO,KAAK,KAAK,EAAE,CAAC;AAExD,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLP,IAAMC,SAAQ,gBAAgB,OAAO,CAAC,OAAO,KAAK,MAAM,EAAE,CAAC;AAE3D,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLP,IAAMC,QAAO,gBAAgB,MAAM,CAAC,OAAO,KAAK,KAAK,EAAE,CAAC;AAExD,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLP,IAAM,YAAY,6BACrB,CAAC,QAAQ,WAAW,KAAK,MAAM,QAAkB,MAAgB,CAAC;AAE/D,IAAMC,SAAQ,iBAAiB,OAAO,SAAS;AAE/C,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACRP,IAAMC,SAAQ,gBAAgB,OAAO,CAAC,OAAO,KAAK,MAAM,EAAE,CAAC;AAE3D,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACPR,SAAUC,MACZ,SAAqB,QAAkB,OAAiB,SACxD,UACA,UAAqB;AACvB,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,iBAAiB,SAAS;AAChC,QAAM,gBAAgB,SAAS;AAC/B,QAAM,wBAAwB,SAAS;AACvC,QAAM,uBAAuB,SAAS;AACtC,QAAM,SAAS,SAAS,QAAQ;AAChC,QAAM,UAAU,SAAS,QAAQ;AAEjC,QAAM,eACD,aAAa,QAAQ,OAAO,oBACP,OAAO;AAEjC,QAAM,SAAS,OAAO,SAAS,UAAU,KAAK;AAC9C,QAAM,aAAa,OAAO;AAE1B,QAAM,qBACF,SAAS,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC;AACrE,QAAM,mBAAmB,SAAS,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC;AACnE,QAAM,mBAAmB,SAAS,SAAS,CAAC;AAE5C,WAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,UAAM,oBAAoB,IAAI;AAC9B,UAAM,mBAAmB,IAAI,QAAQ,CAAC;AACtC,aAAS,IAAI,GAAG,IAAI,SAAS,YAAY,EAAE,GAAG;AAC5C,eAAS,KAAK,GAAG,KAAK,SAAS,WAAW,EAAE,IAAI;AAC9C,cAAM,WAAW,KAAK,eAAe;AACrC,cAAM,QAAQ,KAAK,IAAI,GAAG,QAAQ;AAClC,cAAM,QACF,KAAK,IAAI,SAAS,UAAU,wBAAwB,QAAQ;AAChE,cAAM,kBAAkB,oBAAoB,KAAK;AACjD,iBAAS,KAAK,GAAG,KAAK,SAAS,UAAU,EAAE,IAAI;AAC7C,gBAAM,WAAW,KAAK,cAAc;AACpC,gBAAM,QAAQ,KAAK,IAAI,GAAG,QAAQ;AAClC,gBAAM,QACF,KAAK,IAAI,SAAS,SAAS,uBAAuB,QAAQ;AAC9D,cAAI,cAAc;AAClB,cAAI,WAAW;AACf,cAAI,QAAQ;AACZ,mBAAS,KAAK,OAAO,KAAK,OAAO,MAAM,gBAAgB;AACrD,kBAAM,WAAW,mBAAmB,KAAK,QAAQ,CAAC;AAClD,qBAAS,KAAK,OAAO,KAAK,OAAO,MAAM,eAAe;AACpD,oBAAM,WAAW,WAAW,KAAK,QAAQ,CAAC;AAC1C,oBAAM,QAAQ,QAAQ,WAAW,CAAC;AAClC,kBAAK,aAAa,SAAS,QAAQ,aAAc;AAC/C,8BAAc;yBACL,aAAa,OAAO;AAC7B,4BAAY;AACZ;;;AAGJ,gBAAI,MAAM,WAAW,GAAG;AACtB;;;AAGJ,gBAAM,eAAe,kBAAkB,KAAK,mBAAmB;AAC/D,qBAAW,YAAY,IACnB,aAAa,QAAQ,WAAW,QAAQ;;;;;AAKpD,SAAO;AACT;AAEM,SAAU,iBACZ,SAAqB,QAAkB,OACvC,UAAmC,mBAAmB,OACtD,sBAAsB,OAAK;AAC7B,QAAM,eAAe,OAAO,SAAS,UAAU,OAAO;AACtD,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,iBAAiB,SAAS;AAChC,QAAM,gBAAgB,SAAS;AAC/B,QAAM,wBAAwB,SAAS;AACvC,QAAM,uBAAuB,SAAS;AACtC,QAAM,SAAS,SAAS,QAAQ;AAChC,QAAM,UAAU,SAAS,QAAQ;AAEjC,QAAM,OAAO,OAAO,QAAQ,OAAO,OAAO;AAC1C,WAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,aAAS,IAAI,GAAG,IAAI,SAAS,YAAY,EAAE,GAAG;AAC5C,eAAS,KAAK,GAAG,KAAK,SAAS,WAAW,EAAE,IAAI;AAC9C,cAAM,WAAW,KAAK,eAAe;AACrC,YAAI,QAAQ;AACZ,eAAO,QAAQ,GAAG;AAChB,mBAAS;;AAGX,cAAM,QACF,KAAK,IAAI,SAAS,UAAU,wBAAwB,QAAQ;AAChE,iBAAS,KAAK,GAAG,KAAK,SAAS,UAAU,EAAE,IAAI;AAC7C,gBAAM,WAAW,KAAK,cAAc;AACpC,cAAI,QAAQ;AACZ,iBAAO,QAAQ,GAAG;AAChB,qBAAS;;AAEX,gBAAM,QACF,KAAK,IAAI,SAAS,SAAS,uBAAuB,QAAQ;AAC9D,cAAI,WAAW,OAAO;AACtB,cAAI,cAAc;AAElB,mBAAS,KAAK,OAAO,KAAK,OAAO,MAAM,gBAAgB;AACrD,kBAAM,KAAK,KAAK;AAChB,qBAAS,KAAK,OAAO,KAAK,OAAO,MAAM,eAAe;AACpD,oBAAM,KAAK,KAAK;AAChB,oBAAM,QAAQ,KAAK,IAAI,GAAG,IAAI,IAAI,CAAC;AACnC,kBAAI,QAAQ,UAAU;AACpB,2BAAW;AACX,oBAAI,kBAAkB;AACpB,gCAAc,wBACR,IAAI,SAAS,WAAW,MAAM,SAAS,UAAU,MAC3C,SAAS,aACb,KACH,KAAK,SAAS,UAAU,MAAM,SAAS,aAAa;uBACpD;AACL,gCAAc,KAAK,uBAAuB;;;;;AAKlD,uBAAa,IAAI,aAAa,GAAG,IAAI,IAAI,CAAC;;;;;AAKlD,SAAO;AACT;AAEM,SAAU,OACZ,SAAqB,QAAkB,OAAiB,SACxD,UACA,UAAqB;AACvB,QAAM,cAAc,SAAS;AAC7B,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,gBAAgB,SAAS;AAC/B,QAAM,iBAAiB,SAAS;AAChC,QAAM,gBAAgB,SAAS;AAC/B,QAAM,uBAAuB,SAAS;AACtC,QAAM,wBAAwB,SAAS;AACvC,QAAM,uBAAuB,SAAS;AACtC,QAAM,WAAW,SAAS,QAAQ;AAClC,QAAM,SAAS,SAAS,QAAQ;AAChC,QAAM,UAAU,SAAS,QAAQ;AAEjC,QAAM,eACD,aAAa,QAAQ,OAAO,oBACP,OAAO;AAEjC,QAAM,SAAS,OAAO,SAAS,UAAU,KAAK;AAC9C,QAAM,aAAa,OAAO;AAE1B,QAAM,qBAAqB,SAAS,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC,IACjE,SAAS,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC;AAC9C,QAAM,qBACF,SAAS,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC;AACrE,QAAM,mBAAmB,SAAS,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC;AACnE,QAAM,mBAAmB,SAAS,SAAS,CAAC;AAE5C,WAAS,QAAQ,GAAG,QAAQ,SAAS,WAAW,EAAE,OAAO;AACvD,UAAM,oBAAoB,QAAQ;AAClC,UAAM,mBAAmB,QAAQ,QAAQ,CAAC;AAC1C,aAAS,UAAU,GAAG,UAAU,SAAS,YAAY,EAAE,SAAS;AAC9D,eAAS,SAAS,GAAG,SAAS,SAAS,UAAU,EAAE,QAAQ;AACzD,cAAM,eAAe,SAAS,cAAc;AAC5C,YAAI,YAAY;AAChB,eAAO,YAAY,GAAG;AACpB,uBAAa;;AAEf,cAAM,YACF,KAAK,IAAI,SAAS,SAAS,uBAAuB,YAAY;AAClE,cAAM,oBACF,oBAAoB,SAAS;AACjC,iBAAS,OAAO,GAAG,OAAO,SAAS,WAAW,EAAE,MAAM;AACpD,gBAAM,aAAa,OAAO,eAAe;AACzC,cAAI,UAAU;AACd,iBAAO,UAAU,GAAG;AAClB,uBAAW;;AAEb,gBAAM,UACF,KAAK,IAAI,SAAS,UAAU,wBAAwB,UAAU;AAClE,gBAAM,kBAAkB,oBAAoB,OAAO;AACnD,mBAAS,OAAO,GAAG,OAAO,SAAS,UAAU,EAAE,MAAM;AACnD,kBAAM,aAAa,OAAO,cAAc;AACxC,gBAAI,UAAU;AACd,mBAAO,UAAU,GAAG;AAClB,yBAAW;;AAEb,kBAAM,UACF,KAAK,IAAI,SAAS,SAAS,uBAAuB,UAAU;AAEhE,kBAAM,kBAAkB,kBAAkB,OAAO;AACjD,gBAAI,cAAc;AAClB,gBAAI,WAAW;AACf,gBAAI,QAAQ;AACZ,qBAAS,SAAS,WAAW,SAAS,WACjC,UAAU,eAAe;AAC5B,oBAAM,eAAe,mBAAmB,SAAS,QAAQ,CAAC;AAC1D,uBAAS,OAAO,SAAS,OAAO,SAAS,QAAQ,gBAAgB;AAC/D,sBAAM,aAAa,eAAe,OAAO,QAAQ,CAAC;AAClD,yBAAS,OAAO,SAAS,OAAO,SAC3B,QAAQ,eAAe;AAC1B,wBAAM,aAAa,aAAa,OAAO,QAAQ,CAAC;AAChD,wBAAM,QAAQ,QAAQ,aAAa,OAAO;AAC1C,sBAAK,aAAa,SAAS,QAAQ,aAAc;AAC/C,kCAAc;6BACL,aAAa,OAAO;AAC7B,gCAAY;AACZ;;AAEF,sBAAI,MAAM,WAAW,GAAG;AACtB;;;AAGJ,oBAAI,MAAM,WAAW,GAAG;AACtB;;;AAGJ,kBAAI,MAAM,WAAW,GAAG;AACtB;;;AAGJ,kBAAM,eAAe,kBAAkB;AACvC,uBAAW,YAAY,IACnB,aAAa,QAAQ,WAAW,QAAQ;;;;;;AAOtD,SAAO;AACT;AAEM,SAAU,mBACZ,MACA,UAAiC;AACnC,QAAM,eAAe,OAAO,SAAS,UAAU,OAAO;AACtD,QAAM,cAAc,SAAS;AAC7B,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,gBAAgB,SAAS;AAC/B,QAAM,iBAAiB,SAAS;AAChC,QAAM,gBAAgB,SAAS;AAC/B,QAAM,uBAAuB,SAAS;AACtC,QAAM,wBAAwB,SAAS;AACvC,QAAM,uBAAuB,SAAS;AACtC,QAAM,WAAW,SAAS,QAAQ;AAClC,QAAM,SAAS,SAAS,QAAQ;AAChC,QAAM,UAAU,SAAS,QAAQ;AAEjC,WAAS,QAAQ,GAAG,QAAQ,SAAS,WAAW,EAAE,OAAO;AACvD,aAAS,UAAU,GAAG,UAAU,SAAS,YAAY,EAAE,SAAS;AAC9D,eAAS,SAAS,GAAG,SAAS,SAAS,UAAU,EAAE,QAAQ;AACzD,cAAM,eAAe,SAAS,cAAc;AAC5C,YAAI,YAAY;AAChB,eAAO,YAAY,GAAG;AACpB,uBAAa;;AAEf,cAAM,YACF,KAAK,IAAI,SAAS,SAAS,uBAAuB,YAAY;AAClE,iBAAS,OAAO,GAAG,OAAO,SAAS,WAAW,EAAE,MAAM;AACpD,gBAAM,aAAa,OAAO,eAAe;AACzC,cAAI,UAAU;AACd,iBAAO,UAAU,GAAG;AAClB,uBAAW;;AAEb,gBAAM,UACF,KAAK,IAAI,SAAS,UAAU,wBAAwB,UAAU;AAClE,mBAAS,OAAO,GAAG,OAAO,SAAS,UAAU,EAAE,MAAM;AACnD,kBAAM,aAAa,OAAO,cAAc;AACxC,gBAAI,UAAU;AACd,mBAAO,UAAU,GAAG;AAClB,yBAAW;;AAEb,kBAAM,UACF,KAAK,IAAI,SAAS,SAAS,uBAAuB,UAAU;AAGhE,gBAAI,WAAW,OAAO;AACtB,gBAAI,cAAc;AAElB,qBAAS,SAAS,WAAW,SAAS,WACjC,UAAU,eAAe;AAC5B,oBAAM,SAAS,SAAS;AACxB,uBAAS,OAAO,SAAS,OAAO,SAAS,QAAQ,gBAAgB;AAC/D,sBAAM,OAAO,OAAO;AACpB,yBAAS,OAAO,SAAS,OAAO,SAC3B,QAAQ,eAAe;AAC1B,wBAAM,OAAO,OAAO;AACpB,wBAAM,QAAQ,KAAK,IAAI,OAAO,QAAQ,MAAM,MAAM,OAAO;AACzD,sBAAI,SAAS,UAAU;AACrB,+BAAW;AACX,kCACI,SAAS,wBAAwB,uBACjC,OAAO,wBAAwB;;;;;AAM3C,yBAAa,IAAI,aAAa,OAAO,QAAQ,MAAM,MAAM,OAAO;;;;;;AAO1E,SAAO;AACT;;;ACtTM,SAAUC,SACZ,MACyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,mBAAiB,GAAG,SAAS;AAC7B,QAAM,EAAC,YAAY,SAAS,KAAAC,MAAK,gBAAe,IAAI;AACpD,QAAM,YAAY;AAElB,eAAK,OACD,qBAAa,+BAA+B,SAAS,SAAS,GAC9D,MAAM,wEACa,OAAO,mBAAmB,SAAS,GAAG;AAE7D,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OAA2C,YAAY,SACzD,WAAWA,MAAK,eAAe;AACnC,MAAI;AAEJ,MAAI,SAAS,gBAAgB,KAAK,SAAS,iBAAiB,KACxD,aAAK,YAAY,SAAS,SAAS,SAAS,QAAQ,GAAG;AACzD,UAAM,SAAS,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,SAAO,CAAC;SAChC;AACL,UAAM,UAAUA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,UAAME,WAAU,aAAK,eAAe,EAAE,KAAK;AAC3C,UAAMC,UAASC,MAAK,SAAS,EAAE,OAAO,EAAE,OAAOF,UAAS,UAAU,KAAK;AACvE,UAAMF,SAAQ,eACV,SAAS,UAAU,EAAE,OAAOG,QAAO,MAAoB;;AAE7D,SAAO;AACT;AAEO,IAAM,gBAA8B;EACzC,YAAY;EACZ,aAAa;EACb,YAAYJ;;;;ACpCR,SAAU,UAAU,MAIzB;AACC,QAAM,EAAC,QAAQ,SAAAM,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,YAAY,SAAS,KAAAC,MAAK,iBAAiB,WAAU,IAAI;AAEhE,mBAAiB,GAAG,WAAW;AAE/B,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OAAmD,YAAY,SACjE,GAAmBA,MAAK,iBAAiB,UAAU;AAEvD,QAAM,UAAUD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,SAAS,OACX,SAAS,EAAE,OAAO,EAAE,OAAO,aAAK,eAAe,EAAE,KAAK,GAAG,UAAU,KAAK;AAE5E,SAAOA,SAAQ,eAAe,OAAO,OAAO,WAAW,OAAO,MAAM;AACtE;AAEO,IAAM,kBAAgC;EAC3C,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC1BR,SAAU,cAAc,MAI7B;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,IAAI,OAAAC,OAAK,IAAI;AACpB,QAAM,EAAC,YAAY,SAAS,KAAAC,MAAK,gBAAe,IAAI;AAEpD,mBAAiB,CAAC,IAAID,MAAK,GAAG,eAAe;AAE7C,QAAM,WAAW,qBAAa,kBAC1BA,OAAM,OAAmD,YACzD,SAAS,GAAmBC,MAAK,eAAe;AAEpD,QAAM,cAAc,SAAS;AAC7B,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,cAAc,SAAS;AAC7B,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,gBAAgB,SAAS;AAC/B,QAAM,iBAAiB,SAAS;AAChC,QAAM,gBAAgB,SAAS;AAC/B,QAAM,uBAAuB,SAAS;AACtC,QAAM,wBAAwB,SAAS;AACvC,QAAM,uBAAuB,SAAS;AACtC,QAAM,WAAW,uBAAuB,IAAI,SAAS,QAAQ;AAC7D,QAAM,UAAU,uBAAuB,IAAI,SAAS,QAAQ;AAC5D,QAAM,SAAS,wBAAwB,IAAI,SAAS,QAAQ;AAC5D,QAAM,KAAK,OAAOD,OAAM,OAAO,SAAS;AAExC,QAAM,gBAAgB,KAAK,cAAc,eAAe;AAExD,QAAM,QAAQD,SAAQ,WAA4B,EAAE;AAEpD,WAAS,QAAQ,GAAG,QAAQ,SAAS,WAAW,EAAE,OAAO;AACvD,aAAS,UAAU,GAAG,UAAU,SAAS,YAAY,EAAE,SAAS;AAC9D,eAAS,UAAU,GAAG,UAAU,SAAS,SAAS,EAAE,SAAS;AAC3D,iBAAS,QAAQ,GAAG,QAAQ,SAAS,UAAU,EAAE,OAAO;AACtD,mBAAS,QAAQ,GAAG,QAAQ,SAAS,SAAS,EAAE,OAAO;AAErD,kBAAM,gBAAgB,UAAU;AAChC,kBAAM,cAAc,QAAQ;AAC5B,kBAAM,cAAc,QAAQ;AAC5B,gBAAI,UAAU;AACd,qBAAS,SAAS,GAAG,SAAS,sBACzB,UAAU,eAAe;AAC5B,oBAAM,WAAW,gBAAgB,UAAU;AAC3C,kBAAI,UAAU,KAAK,WAAW,SAAS,YACnC,KAAK,MAAM,OAAO,MAAM,SAAS;AACnC;;AAEF,uBAAS,OAAO,GAAG,OAAO,uBACrB,QAAQ,gBAAgB;AAC3B,sBAAM,SAAS,cAAc,QAAQ;AACrC,oBAAI,QAAQ,KAAK,SAAS,SAAS,aAC/B,KAAK,MAAM,KAAK,MAAM,OAAO;AAC/B;;AAEF,yBAAS,OAAO,GAAG,OAAO,sBACrB,QAAQ,eAAe;AAC1B,wBAAM,SAAS,cAAc,QAAQ;AACrC,sBAAI,QAAQ,KAAK,SAAS,SAAS,YAC/B,KAAK,MAAM,KAAK,MAAM,OAAO;AAC/B;;AAGF,wBAAM,QACF,MAAM,IAAI,OAAO,SAAS,OAAO,OAAO,OAAO;AACnD,6BAAW;;;;AAIjB,eAAG,IACC,UAAU,eAAe,OAAO,SAAS,OAAO,OAAO,OAAO;;;;;;AAO5E,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,sBAAoC;EAC/C,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACzFR,SAAU,YAAY,MAI3B;AACC,QAAM,EAAC,QAAQ,SAAAG,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,IAAI,OAAAC,OAAK,IAAI;AACpB,QAAM,IAAIA;AACV,mBAAiB,CAAC,IAAIA,MAAK,GAAG,aAAa;AAC3C,QAAM,EAAC,YAAY,SAAS,KAAAC,KAAG,IAAI;AAEnC,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OAA2C,YAAY,SACzD,GAAmBA,IAAG;AAC1B,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,iBAAiB,SAAS;AAChC,QAAM,gBAAgB,SAAS;AAC/B,QAAM,wBAAwB,SAAS;AACvC,QAAM,uBAAuB,SAAS;AACtC,QAAM,UAAU,uBAAuB,IAAI,SAAS,QAAQ;AAC5D,QAAM,SAAS,wBAAwB,IAAI,SAAS,QAAQ;AAC5D,QAAM,KACF,OAAgB,EAAE,OAA2C,SAAS;AAE1E,QAAM,gBAAgB,KAAK,eAAe;AAE1C,QAAM,SAASF,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC3C,QAAM,QAAQ,OACV,GAAG,OAA2C,WAAW,MAAM;AAEnE,WAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,aAAS,IAAI,GAAG,IAAI,SAAS,YAAY,EAAE,GAAG;AAC5C,eAAS,MAAM,GAAG,MAAM,SAAS,UAAU,EAAE,KAAK;AAChD,iBAAS,MAAM,GAAG,MAAM,SAAS,SAAS,EAAE,KAAK;AAE/C,gBAAM,YAAY,MAAM;AACxB,gBAAM,YAAY,MAAM;AACxB,cAAI,UAAU;AACd,mBAAS,KAAK,GAAG,KAAK,uBAAuB,MAAM,gBAAgB;AACjE,kBAAM,OAAO,YAAY,MAAM;AAC/B,gBAAI,MAAM,KAAK,OAAO,SAAS,aAC3B,KAAK,MAAM,GAAG,MAAM,KAAK;AAC3B;;AAEF,qBAAS,KAAK,GAAG,KAAK,sBAAsB,MAAM,eAAe;AAC/D,oBAAM,OAAO,YAAY,MAAM;AAC/B,kBAAI,MAAM,KAAK,OAAO,SAAS,YAC3B,KAAK,MAAM,GAAG,MAAM,KAAK;AAC3B;;AAGF,oBAAM,QAAQ,MAAM,IAAI,GAAG,KAAK,KAAK,CAAC;AACtC,yBAAW;;;AAGf,aAAG,IAAI,UAAU,eAAe,GAAG,KAAK,KAAK,CAAC;;;;;AAKtD,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,oBAAkC;EAC7C,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACpER,SAAUG,WAAU,MAIzB;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,OAAAC,QAAO,QAAQ,MAAAC,OAAM,SAAQ,IAAI;AAE3C,eAAK,OACDA,MAAK,MAAM,WAAW,SAAS,MAAM,QACrC,MAAM,8EACY;AACtB,eAAK,OACD,UAAU,QAAQA,MAAK,MAAM,WAAW,OAAO,MAAM,QACrD,MAAM,4EACY;AACtB,eAAK,OACDD,UAAS,QAAQC,MAAK,MAAM,WAAWD,OAAM,MAAM,QACnD,MAAM,2EACY;AAEtB,mBAAiB,CAAC,GAAGC,OAAM,UAAUD,QAAO,MAAM,GAAG,WAAW;AAEhE,MAAI,EAAC,gBAAe,IAAI;AACxB,MAAI,mBAAmB,MAAM;AAC3B,sBAAkB;;AAGpB,QAAM,QAAQD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,QAAQA,SAAQ,KAAK,IAAIE,MAAK,MAAM,EAAE;AAC5C,QAAM,UAAUF,SAAQ,KAAK,IAAI,SAAS,MAAM,EAAE;AAClD,QAAM,QAAQC,SAAQD,SAAQ,KAAK,IAAIC,OAAM,MAAM,EAAE,SAC/B,IAAI,aAAa,CAAC,CAAC,CAAC;AAC1C,QAAM,UAAU,SACZD,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE,SAChC,IAAI,aAAa,CAAC,CAAC,CAAC;AACxB,QAAM,UAAU,IAAI,aAAa,MAAM,MAAM;AAE7C,QAAM,gBAAgB,QAAQ;AAC9B,QAAM,cAAc,MAAM;AAC1B,QAAM,gBAAgB,QAAQ;AAC9B,QAAM,cAAc,MAAM;AAE1B,MAAI,OAAO;AACX,MAAI,KAAK;AACT,MAAI,KAAK;AACT,MAAI,KAAK;AACT,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,EAAE,GAAG;AACrC,YAAQ,CAAC,IAAI,QAAQ,MAAM,KACtB,MAAM,CAAC,IAAI,MAAM,IAAI,KAAK,MAAM,IAAI,IACjC,KAAK,KAAK,QAAQ,IAAI,IAAI,eAAe;AACjD,QAAI,QAAQ,eAAe;AACzB,aAAO;;AAET,QAAI,MAAM,aAAa;AACrB,WAAK;;AAEP,QAAI,MAAM,aAAa;AACrB,WAAK;;AAEP,QAAI,MAAM,eAAe;AACvB,WAAK;;;AAGT,SAAOA,SAAQ,eAAe,EAAE,OAAO,EAAE,OAAO,OAAO;AACzD;AAEO,IAAM,kBAAgC;EAC3C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACnER,SAAUI,gBAAe,MAI9B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,YAAY,MAAK,IAAI;AAE5B,mBAAiB,CAAC,CAAC,GAAG,gBAAgB;AAEtC,QAAMC,QAAO,WAAW,OAAO,CAAC,GAAG,MAAM,IAAI,CAAC;AAE9C,QAAM,WAAW,qBAAa,YAAY,EAAE,OAAO,YAAYA,KAAI;AACnE,QAAM,WAAW,qBAAa,YAAY,SAAS,QAAQ,WAAW,MAAM;AAC5E,QAAM,mBACF,qBAAa,oBAAoB,EAAE,OAAO,YAAYA,KAAI;AAC9D,QAAM,mBACF,qBAAa,oBAAoB,OAAO,WAAW,MAAM;AAC7D,QAAM,YACF,qBAAa,aAAa,kBAAkB,OAAO,WAAW,MAAM;AAExE,QAAM,YAAYC,SAAQ,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,SAAQ,EAAC,CAAC;AAC1E,QAAM,cACFG,WAAU,EAAC,QAAQ,EAAC,GAAG,UAAS,GAAG,SAAAH,UAAS,OAAO,EAAC,MAAM,SAAQ,EAAC,CAAC;AACxE,QAAM,sBAAsBE,SACxB,EAAC,QAAQ,EAAC,GAAG,YAAW,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,iBAAgB,EAAC,CAAC;AACzE,QAAM,SAASI,OAAM;IACnB,QAAQ,EAAC,GAAG,oBAAmB;IAC/B,SAAAJ;IACA,OAAO,EAAC,OAAO,kBAAkB,MAAM,UAAS;GACjD;AAED,EAAAA,SAAQ,8BAA8B,SAAS;AAC/C,EAAAA,SAAQ,8BAA8B,WAAW;AACjD,EAAAA,SAAQ,8BAA8B,mBAAmB;AAEzD,SAAO;AACT;AAEO,IAAM,uBAAqC;EAChD,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC9CR,SAAUM,UAAS,MAIxB;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,QAAO,IAAI;AACrB,QAAM,EAAC,KAAI,IAAI;AAEf,QAAM,QAAQA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,cAAcA,SAAQ,KAAK,IAAI,QAAQ,MAAM,EAAE;AAErD,QAAM,UACF,aAAa,OAAO,aAAa,QAAQ,OAAO,QAAQ,OAAO,IAAI;AAEvE,SAAOA,SAAQ,eAAe,CAAC,IAAI,GAAG,QAAQ,OAAO,OAAO;AAC9D;AAEO,IAAM,iBAA+B;EAC1C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACtBR,SAAUE,eAAc,MAG7B;AACC,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,IAAI,GAAE,IAAI;AAEjB,QAAM,SAASA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC3C,QAAM,SAASA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAE3C,QAAM,iBAAiB,qBAAa,2BAChC,MAAM,KAAK,MAAM,GAAG,MAAM,KAAK,MAAM,CAAC;AAE1C,SAAOA,SAAQ,eACX,CAAC,eAAe,MAAM,GAAG,SAAS,WAAW,KAAK,cAAc,CAAC;AACvE;AAEO,IAAM,sBAAoC;EAC/C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACpBP,IAAME,eAAc,gBAAgB,aAAa,CAAC,IAAI,UAAS;AACpE,QAAM,YAAY;AAClB,MAAI,KAAK,UAAU,cAAc;AAC/B,WAAO,UAAU;;AAEnB,SAAO,KAAK,UAAU,eAAe,UAAU,eAAe;AAChE,CAAC;AAEM,IAAM,oBAAkC;EAC7C,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACXP,IAAM,aACT,CAAC,SAA6D;AAC5D,QAAM,EAAC,EAAC,IAAI,KAAK;AACjB,QAAM,aAAa,KAAK;AACxB,QAAM,eAAe,IAAI,aAAa,aAAK,cAAc,EAAE,KAAK,CAAC;AACjE,QAAM,cAAc,WAAW,KAAK,IAAI,EAAE,MAAM;AAChD,QAAMC,QAAO,YAAY,mBAAmB;AAC5C,QAAMC,QAAO,YAAY,mBAAmB;AAC5C,QAAM,WAAW,WAAW,KAAK,IAAID,MAAK,MAAM,EAAE;AAClD,QAAM,WAAW,WAAW,KAAK,IAAIC,MAAK,MAAM,EAAE;AAClD,WAAS,IAAI,GAAG,IAAI,SAAS,QAAQ,KAAK;AACxC,UAAMD,QAAO,SAAS,CAAC;AACvB,UAAMC,QAAO,SAAS,CAAC;AACvB,iBAAa,CAAC,IAAI,KAAK,MAAMD,OAAMC,KAAI;;AAGzC,SAAO,WAAW,WAAW,cAAc,EAAE,OAAO,SAAS;AAC/D;AAEG,IAAM,mBAAiC;EAC5C,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACtBR,SAAUC,MAAK,MAAmD;AAEtE,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,OAAAC,OAAK,IAAI;AAEhB,QAAMF,QAAOC,SAAQ,KAAK,IAAIC,OAAM,MAAM,EAAE,mBAAmB;AAC/D,QAAM,UAAUD,SAAQ,KAAK,IAAID,MAAK,MAAM,EAAE;AAK9C,SAAOC,SAAQ,eAAeD,MAAK,OAAOA,MAAK,OAAO,OAAO;AAC/D;AAEO,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACVR,SAAUG,QACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,KAAI,IAAI;AAEf,QAAM,QAAQ,aAAK,eAAe,MAAM,OAAO,CAAC,EAAE,KAAK,EAAE,CAAC;AAE1D,QAAM,SAAS,OAAO,IAAI,OAAK,EAAE,KAAK;AACtC,uBAAa,uBAAuB,QAAQ,KAAK;AAEjD,MAAI,WAAW,qBAAa,gBAAgB,OAAO,IAAI,OAAK,EAAE,KAAK,GAAG,KAAK;AAE3E,MAAI,aAAK,cAAc,QAAQ,MAAM,GAAG;AACtC,WAAOA,SAAQ,eAAe,UAAU,OAAO,CAAC,EAAE,OAAO,CAAA,CAAE;;AAI7D,QAAM,UAAU,OAAO,OAAO,OAAK,aAAK,cAAc,EAAE,KAAK,IAAI,CAAC;AAClE,MAAI,QAAQ,WAAW,GAAG;AACxB,WAAO,SAAS,EAAC,QAAQ,EAAC,GAAG,QAAQ,CAAC,EAAC,GAAG,SAAAA,SAAO,CAAC;;AAGpD,MAAI,QAAQ,CAAC,EAAE,UAAU,aAAa;AACpC,UAAM,QAAQ,QAAQ,IAAI,CAAC,MAAMC,MAAK,EAAC,QAAQ,EAAC,OAAO,EAAC,GAAG,SAAAD,SAAO,CAAC,CAAC;AACpE,UAAM,QAAQ,QAAQ,IAAI,CAAC,MAAME,MAAK,EAAC,QAAQ,EAAC,OAAO,EAAC,GAAG,SAAAF,SAAO,CAAC,CAAC;AAEpE,UAAM,eAAeD,QAAO,EAAC,QAAQ,OAAO,SAAAC,UAAS,OAAO,EAAC,MAAM,MAAK,EAAC,CAAC;AAC1E,UAAM,eAAeD,QAAO,EAAC,QAAQ,OAAO,SAAAC,UAAS,OAAO,EAAC,MAAM,MAAK,EAAC,CAAC;AAE1E,UAAM,SACFG,SAAQ,EAAC,QAAQ,EAAC,MAAM,cAAc,MAAM,aAAY,GAAG,SAAAH,SAAO,CAAC;AAEvE,UAAM,QAAQ,OAAKA,SAAQ,8BAA8B,CAAC,CAAC;AAC3D,UAAM,QAAQ,OAAKA,SAAQ,8BAA8B,CAAC,CAAC;AAC3D,IAAAA,SAAQ,8BAA8B,YAAY;AAClD,IAAAA,SAAQ,8BAA8B,YAAY;AAElD,WAAO;;AAUT,QAAM,WAAW,QAAQ,IAAI,OAAI;AAC/B,UAAM,YAAY,aAAK,cAAc,EAAE,MAAM,MAAM,KAAK,CAAC;AACzD,UAAM,QAAQ,CAAC,IAAI,SAAS;AAC5B,WAAOI,SAAQ,EAAC,QAAQ,EAAC,GAAG,EAAC,GAAG,SAAAJ,UAAS,OAAO,EAAC,MAAK,EAAC,CAAC;EAC1D,CAAC;AAED,QAAM,kBAAkB,SAAS,IAAI,OAAI;AACvC,WAAO,EAAC,MAAMA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE,QAAQ,OAAO,EAAE,MAAK;EACjE,CAAC;AAGD,aACI,qBAAa;IAAgB,SAAS,IAAI,OAAK,EAAE,KAAK;IAAG;;EAAY;AACzE,QAAM,eAAe,SAAS,CAAC,EAAE,MAAM,CAAC,MAAM;AAC9C,QAAM,UACF,WAAW,iBAAiB,UAAU,OAAO,CAAC,EAAE,OAAO,YAAY;AAEvE,QAAM,gBACF,qBAAa,gBAAgB,QAAQ,IAAI,OAAK,EAAE,KAAK,GAAG,KAAK;AAEjE,QAAM,UACFA,SAAQ,eAAe,eAAe,OAAO,CAAC,EAAE,OAAO,OAAO;AAElE,WAAS,QAAQ,OAAKA,SAAQ,8BAA8B,CAAC,CAAC;AAE9D,SAAO;AACT;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACrFR,SAAU,OACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAM,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,OAAM,IAAI;AACpB,QAAM,EAAC,SAAS,KAAAC,MAAK,YAAY,WAAW,gBAAe,IAAI;AAE/D,mBAAiB,CAAC,GAAG,MAAM,GAAG,QAAQ;AAEtC,QAAM,cAAc,qBAAa,wBAAwB,UAAU;AACnE,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OACF,OAAO,OAA2C,SAAS,WAAWA,MACtE,iBAAiB,OAAuB,WAAW;AAEvD,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,iBAAiB,SAAS;AAChC,QAAM,gBAAgB,SAAS;AAC/B,QAAM,UAAU,SAAS,QAAQ;AACjC,QAAM,SAAS,SAAS,QAAQ;AAChC,QAAM,iBAAiB,SAAS,eAAe;AAE/C,QAAM,IAAI,IAAI,aAAa,SAAS,UAAU,EAAE,KAAkB;AAElE,QAAM,WAAW,aAAK,eAAe,EAAE,KAAK;AAC5C,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AAEtD,QAAM,eAAe,SAAS,CAAC;AAC/B,QAAM,aAAa,iBAAiB,SAAS,CAAC,IAAI,SAAS,CAAC;AAC5D,QAAM,aAAa,iBAAiB,SAAS,CAAC,IAAI;AAClD,QAAM,iBAAiB,iBAAiB,IAAI,SAAS,CAAC;AACtD,QAAM,eAAe,EAAE,QAAQ,CAAC;AAChC,QAAM,aAAa,iBAAiB,EAAE,QAAQ,CAAC,IAAI,EAAE,QAAQ,CAAC;AAC9D,QAAM,aAAa,iBAAiB,EAAE,QAAQ,CAAC,IAAI;AACnD,QAAM,iBAAiB,iBAAiB,IAAI,EAAE,QAAQ,CAAC;AAEvD,QAAM,QAAQD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,QAAQA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAC9C,QAAM,QAAQ,EAAE;AAEhB,WAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,UAAM,WAAW,IAAI;AACrB,UAAM,WAAW,IAAI;AACrB,aAAS,KAAK,GAAG,KAAK,SAAS,WAAW,EAAE,IAAI;AAC9C,YAAM,WAAW,WAAW,KAAK;AACjC,YAAM,WAAW,KAAK,SAAS,eAAe;AAC9C,eAAS,KAAK,GAAG,KAAK,cAAc,EAAE,IAAI;AACxC,cAAM,KAAK,WAAW,KAAK;AAC3B,YAAI,KAAK,KAAK,MAAM,SAAS,UAAU;AACrC;;AAEF,cAAM,WAAW,KAAK,cAAc,CAAC;AACrC,cAAM,WAAW,WAAW,KAAK;AACjC,iBAAS,KAAK,GAAG,KAAK,SAAS,UAAU,EAAE,IAAI;AAC7C,gBAAM,WAAW,WAAW,KAAK;AACjC,gBAAM,WAAW,KAAK,SAAS,cAAc;AAC7C,mBAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,kBAAM,KAAK,WAAW,KAAK;AAC3B,gBAAI,KAAK,KAAK,MAAM,SAAS,SAAS;AACpC;;AAEF,kBAAM,WAAW,WAAW,KAAK,cAAc,CAAC;AAChD,kBAAM,WAAW,WAAW,KAAK;AACjC,gBAAI,WAAW;AACf,qBAAS,KAAK,GAAG,KAAK,SAAS,YAAY,EAAE,IAAI;AAC/C,oBAAM,OAAO,MAAM,WAAW,KAAK,cAAc;AACjD,uBAAS,KAAK,GAAG,KAAK,SAAS,aAAa,EAAE,IAAI;AAChD,sBAAM,WAAW,KAAK,cAAc,KAChC,OAAO,MAAM,WAAW,EAAE;;AAEhC,0BAAY,SAAS;;;;;;;AAQjC,SAAOA,SAAQ,eAAe,EAAE,OAAO,EAAE,OAAO,KAAK;AACvD;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACrFR,SAAU,qBAAqB,MAIpC;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,GAAE,IAAI;AAChB,QAAM,EAAC,SAAS,KAAAC,MAAK,YAAY,iBAAiB,YAAW,IAAI;AAEjE,mBAAiB,CAAC,GAAG,EAAE,GAAG,sBAAsB;AAEhD,QAAM,cAAc,qBAAa,wBAAwB,UAAU;AACnE,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OAA2C,aAAa,SAC1D,GAAmBA,MAAK,iBAAiB,OACzC,WAAW;AAEf,QAAM,EAAC,cAAc,aAAa,cAAc,YAAW,IAAI;AAC/D,QAAM,iBAAiB,SAAS,eAAe;AAC/C,QAAM,KAAK,IAAI,aAAa,SAAS,aAAa,SAAS;AAE3D,QAAM,UAAU,SAAS,QAAQ;AACjC,QAAM,SAAS,SAAS,QAAQ;AAChC,QAAM,QAAQD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,SAASA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAE3C,QAAM,OAAO,IAAI,aAAa,EAAE,OAAO,EAAE,OAAO,KAAK;AACrD,QAAM,QAAQ,IAAI,aAAa,GAAG,OAAO,GAAG,OAAO,MAAM;AAEzD,WAAS,KAAK,GAAG,KAAK,cAAc,EAAE,IAAI;AACxC,UAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,MAAM,SAAS,MAAM,YAAY,CAAC;AACjE,UAAM,QAAQ,KAAK,IACf,SAAS,YAAY,SAAS,WAAW,SAAS,MAAM,YAAY;AAExE,aAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,YAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,MAAM,UAAU,MAAM,WAAW,CAAC;AACjE,YAAM,QAAQ,KAAK,IACf,SAAS,WAAW,SAAS,UAAU,UAAU,MAAM,WAAW;AAEtE,eAAS,KAAK,GAAG,KAAK,SAAS,YAAY,EAAE,IAAI;AAC/C,iBAAS,KAAK,GAAG,KAAK,SAAS,aAAa,EAAE,IAAI;AAChD,cAAI,UAAU;AACd,mBAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,qBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,oBAAM,KAAK,KAAK,KAAK,eAAe;AACpC,uBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,sBAAM,KAAK,KAAK,KAAK,cAAc;AACnC,oBAAI,gBAAgB;AAClB,6BAAY,KAAK,IAAI,GAAG,IAAI,IAAI,EAAE,IAC7B,MAAM,IAAI,GAAG,IAAI,IAAI,EAAE;uBACvB;AACL,6BAAY,KAAK,IAAI,GAAG,IAAI,IAAI,EAAE,IAC7B,MAAM,IAAI,GAAG,IAAI,IAAI,EAAE;;;;;AAKpC,aAAG,IAAI,SAAS,IAAI,IAAI,IAAI,EAAE;;;;;AAMtC,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,6BAA2C;EACtD,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACrER,SAAU,oBAAoB,MAInC;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,IAAI,OAAM,IAAI;AACrB,QAAM,EAAC,YAAY,SAAS,KAAAC,MAAK,YAAY,gBAAe,IAAI;AAEhE,mBAAiB,CAAC,IAAI,MAAM,GAAG,qBAAqB;AAEpD,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AACtD,QAAM,YAAY,aAAK,eAAe,GAAG,KAAK;AAE9C,MAAI,cAAc,qBAAa,wBAAwB,UAAU;AACjE,QAAM,WAAW,qBAAa,kBAC1B,YAAY,OAAO,OAA2C,SAC9D,GAAmBA,MAAK,iBAAiB,OAAO,WAAW;AAE/D,QAAM,KAAK,IAAI,aAAa,SAAS,SAAS,SAAS;AACvD,QAAM,WAAW,GAAG;AACpB,QAAM,WAAWD,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC7C,QAAM,YAAYA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAClD,QAAM,CAAC,OAAO,OAAO,KAAK,IAAI;AAC9B,QAAM,EACJ,WACA,cACA,aACA,YACA,UACA,SACA,aACA,WACA,UACA,cACA,YAAW,IACT;AACJ,gBAAc,SAAS;AACvB,QAAM,SAAS,eAAe,IAAI,SAAS,QAAQ;AACnD,QAAM,UAAU,cAAc,IAAI,SAAS,QAAQ;AAEnD,QAAM,iBAAiB,gBAAgB;AACvC,QAAM,eAAe,GAAG,QAAQ,CAAC;AACjC,QAAM,aAAa,iBAAiB,GAAG,QAAQ,CAAC,IAAI,GAAG,QAAQ,CAAC;AAChE,QAAM,aAAa,iBAAiB,GAAG,QAAQ,CAAC,IAAI;AACpD,QAAM,iBAAiB,iBAAiB,IAAI,GAAG,QAAQ,CAAC;AACxD,QAAM,eAAe,UAAU,CAAC;AAChC,QAAM,aAAa,iBAAiB,UAAU,CAAC,IAAI,UAAU,CAAC;AAC9D,QAAM,aAAa,iBAAiB,UAAU,CAAC,IAAI;AACnD,QAAM,iBAAiB,iBAAiB,IAAI,UAAU,CAAC;AAEvD,WAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,aAAS,KAAK,GAAG,KAAK,YAAY,EAAE,IAAI;AACtC,eAAS,KAAK,GAAG,KAAK,UAAU,EAAE,IAAI;AACpC,cAAM,WAAW,KAAK;AACtB,cAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,KAAK,WAAW,YAAY,CAAC;AAC5D,cAAM,QACF,KAAK,IAAI,YAAY,eAAe,YAAY,YAAY;AAEhE,iBAAS,KAAK,GAAG,KAAK,SAAS,EAAE,IAAI;AACnC,gBAAM,WAAW,KAAK;AACtB,gBAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,KAAK,WAAW,WAAW,CAAC;AAC3D,gBAAM,QACF,KAAK,IAAI,WAAW,cAAc,YAAY,WAAW;AAE7D,cAAI,UAAU;AACd,mBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,kBAAM,KAAK,KAAK,eAAe;AAE/B,qBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,oBAAM,KAAK,KAAK,cAAc;AAC9B,oBAAM,WACF,eAAe,IAAI,aAAa,KAAK,aAAa;AACtD,oBAAM,YAAY,SAAS,eAAe,IAAI,MAC1C,SAAS,cAAc,IAAI,MAAM,QAAQ;AAE7C,uBAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,sBAAM,QAAQ,SAAS,WAAW,iBAAiB,EAAE;AACrD,sBAAM,SAAS,UAAU,YAAY,EAAE;AACvC,2BAAW,QAAQ;;;;AAIzB,gBAAM,WAAW,eAAe,IAAI,aAAa,KAC7C,aAAa,KAAK,iBAAiB;AACvC,mBAAS,QAAQ,IAAI;;;;;AAM7B,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,4BAA0C;EACrD,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACjGR,SAAU,OACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,OAAM,IAAI;AACpB,QAAM,EAAC,SAAS,KAAAC,MAAK,UAAS,IAAI;AAElC,mBAAiB,CAAC,GAAG,MAAM,GAAG,QAAQ;AAEtC,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OACF,OAAO,OAAmD,SAC1D,WAAWA,IAAG;AAElB,QAAM,EACJ,aACA,cACA,aACA,eACA,gBACA,eACA,QAAO,IACL;AACJ,QAAM,WAAW,QAAQ;AACzB,QAAM,UAAU,QAAQ;AACxB,QAAM,SAAS,QAAQ;AACvB,QAAM,IAAI,IAAI,aAAa,SAAS,UAAU,EAAE,KAAkB;AAElE,QAAM,QAAQD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,QAAQA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAC9C,QAAM,QAAQ,EAAE;AAEhB,QAAM,WAAW,aAAK,eAAe,EAAE,KAAK;AAC5C,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AAEtD,WAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,UAAM,WAAW,IAAI,SAAS,CAAC;AAC/B,UAAM,WAAW,IAAI,EAAE,QAAQ,CAAC;AAChC,aAAS,KAAK,GAAG,KAAK,SAAS,UAAU,EAAE,IAAI;AAC7C,YAAM,WAAW,WAAW,KAAK,EAAE,QAAQ,CAAC;AAC5C,YAAM,WAAW,KAAK,SAAS,cAAc;AAC7C,eAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,cAAM,KAAK,WAAW,KAAK;AAC3B,YAAI,KAAK,KAAK,MAAM,SAAS,SAAS;AACpC;;AAEF,cAAM,WAAW,KAAK,cAAc,CAAC;AACrC,cAAM,WAAW,WAAW,KAAK,SAAS,CAAC;AAE3C,iBAAS,KAAK,GAAG,KAAK,SAAS,WAAW,EAAE,IAAI;AAC9C,gBAAM,WAAW,WAAW,KAAK,EAAE,QAAQ,CAAC;AAC5C,gBAAM,WAAW,KAAK,SAAS,eAAe;AAC9C,mBAAS,KAAK,GAAG,KAAK,cAAc,EAAE,IAAI;AACxC,kBAAM,KAAK,WAAW,KAAK;AAC3B,gBAAI,KAAK,KAAK,MAAM,SAAS,UAAU;AACrC;;AAEF,kBAAM,WAAW,WAAW,KAAK,cAAc,CAAC;AAChD,kBAAM,WAAW,WAAW,KAAK,SAAS,CAAC;AAC3C,qBAAS,KAAK,GAAG,KAAK,SAAS,UAAU,EAAE,IAAI;AAC7C,oBAAM,WAAW,WAAW,KAAK,SAAS;AAC1C,oBAAM,WAAW,KAAK,SAAS,cAAc;AAC7C,uBAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,sBAAM,KAAK,WAAW,KAAK;AAC3B,oBAAI,KAAK,KAAK,MAAM,SAAS,SAAS;AACpC;;AAEF,sBAAM,WAAW,WAAW,KAAK,cAAc,CAAC;AAChD,sBAAM,WAAW,WAAW,KAAK,SAAS;AAC1C,oBAAI,WAAW;AACf,yBAAS,KAAK,GAAG,KAAK,SAAS,YAAY,EAAE,IAAI;AAC/C,wBAAM,OAAO,MAAM,WAAW,EAAE;AAChC,2BAAS,KAAK,GAAG,KAAK,SAAS,aAAa,EAAE,IAAI;AAChD,0BAAM,WAAW,EAAE,KAAK,OAAO,MAAM,WAAW,EAAE;;AAEpD,8BAAY,SAAS;;;;;;;;;AAUrC,SAAOA,SAAQ,eAAe,EAAE,OAAO,EAAE,OAAO,EAAE,MAAM;AAC1D;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC3FR,SAAU,uBAAuB,MAItC;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,GAAE,IAAI;AAChB,QAAM,EAAC,SAAS,KAAAC,MAAK,YAAW,IAAI;AAEpC,mBAAiB,CAAC,GAAG,EAAE,GAAG,wBAAwB;AAElD,QAAM,WAAW,aAAK,eAAe,EAAE,KAAK;AAC5C,QAAM,YAAY,aAAK,eAAe,GAAG,KAAK;AAE9C,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OAAmD,aAAa,SAClE,GAAmBA,IAAG;AAE1B,QAAM,cAAc,SAAS;AAC7B,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,cAAc,SAAS;AAC7B,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAE7B,QAAM,KAAK,IAAI,aAAa,SAAS,aAAa,SAAS;AAC3D,QAAM,WAAW,GAAG;AACpB,QAAM,CAAC,MAAM,MAAM,MAAM,IAAI,IAAI,GAAG;AACpC,QAAM,WAAWD,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC7C,QAAM,CAAC,MAAM,MAAM,MAAM,IAAI,IAAI;AACjC,QAAM,UAAUA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,CAAC,KAAK,KAAK,KAAK,GAAG,IAAI;AAE7B,QAAM,WAAW,SAAS,QAAQ;AAClC,QAAM,UAAU,SAAS,QAAQ;AACjC,QAAM,SAAS,SAAS,QAAQ;AAEhC,WAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,UAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,MAAM,WAAW,MAAM,WAAW,CAAC;AAClE,UAAM,QAAQ,KAAK,IACf,SAAS,WAAW,SAAS,UAAU,WAAW,MAAM,WAAW;AACvE,UAAM,WAAW,KAAK;AAEtB,aAAS,KAAK,GAAG,KAAK,cAAc,EAAE,IAAI;AACxC,YAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,MAAM,SAAS,MAAM,YAAY,CAAC;AACjE,YAAM,QAAQ,KAAK,IACf,SAAS,YAAY,SAAS,WAAW,SAAS,MAAM,YAAY;AACxE,YAAM,WAAW,KAAK,OAAO;AAE7B,eAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,cAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,MAAM,UAAU,MAAM,WAAW,CAAC;AACjE,cAAM,QAAQ,KAAK,IACf,SAAS,WAAW,SAAS,UAAU,UAAU,MAAM,WAAW;AACtE,cAAM,WAAW,KAAK,OAAO;AAE7B,iBAAS,KAAK,GAAG,KAAK,SAAS,YAAY,EAAE,IAAI;AAC/C,gBAAM,WAAW,KAAK,OAAO;AAE7B,mBAAS,KAAK,GAAG,KAAK,SAAS,aAAa,EAAE,IAAI;AAChD,gBAAI,UAAU;AACd,qBAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,oBAAM,WAAW,IAAI;AACrB,oBAAM,WAAW,IAAI;AAErB,uBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,sBAAM,KAAK,KAAK,KAAK,cAAc;AACnC,sBAAM,WAAW,KAAK,MAAM;AAC5B,sBAAM,WAAW,KAAK,OAAO;AAE7B,yBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,wBAAM,KAAK,KAAK,KAAK,eAAe;AACpC,wBAAM,WAAW,KAAK,MAAM;AAC5B,wBAAM,WAAW,KAAK,OAAO;AAE7B,2BAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,0BAAM,KAAK,KAAK,KAAK,cAAc;AACnC,0BAAM,WAAW,KAAK,MAAM;AAC5B,0BAAM,WAAW,KAAK,OAAO;AAE7B,+BAAW,QAAQ,WAAW,EAAE,IAAI,SAAS,WAAW,EAAE;;;;;AAKlE,qBAAS,WAAW,EAAE,IAAI;;;;;;AAOpC,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,+BAA6C;EACxD,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACjGR,SAAU,sBAAsB,MAIrC;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,IAAI,OAAM,IAAI;AACrB,QAAM,EAAC,KAAAC,MAAK,SAAS,WAAU,IAAI;AAEnC,mBAAiB,CAAC,EAAE,GAAG,uBAAuB;AAE9C,QAAM,YAAY,aAAK,eAAe,GAAG,KAAK;AAC9C,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AAEtD,QAAM,WAAW,qBAAa,kBAC1B,YAAY,OAAO,OACnB,SAAS,GAAmBA,IAAG;AAEnC,QAAM,KAAK,IAAI,aAAa,SAAS,SAAS,SAAS;AACvD,QAAM,WAAW,GAAG;AACpB,QAAM,CAAC,MAAM,MAAM,MAAM,IAAI,IAAI,GAAG;AACpC,QAAM,WAAWD,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC7C,QAAM,CAAC,MAAM,MAAM,MAAM,IAAI,IAAI;AACjC,QAAM,YAAYA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAClD,QAAM,CAAC,OAAO,OAAO,OAAO,KAAK,IAAI;AACrC,QAAM,EACJ,WACA,aACA,cACA,aACA,YACA,SACA,UACA,SACA,aACA,UACA,WACA,UACA,aACA,cACA,YAAW,IACT;AACJ,QAAM,WAAW,cAAc,IAAI,SAAS,QAAQ;AACpD,QAAM,SAAS,eAAe,IAAI,SAAS,QAAQ;AACnD,QAAM,UAAU,cAAc,IAAI,SAAS,QAAQ;AAEnD,WAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,aAAS,KAAK,GAAG,KAAK,YAAY,EAAE,IAAI;AAEtC,eAAS,KAAK,GAAG,KAAK,SAAS,EAAE,IAAI;AACnC,cAAM,WAAW,KAAK;AACtB,cAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,KAAK,WAAW,WAAW,CAAC;AAC3D,cAAM,QACF,KAAK,IAAI,WAAW,cAAc,YAAY,WAAW;AAG7D,iBAAS,KAAK,GAAG,KAAK,UAAU,EAAE,IAAI;AACpC,gBAAM,WAAW,KAAK;AACtB,gBAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,KAAK,WAAW,YAAY,CAAC;AAC5D,gBAAM,QACF,KAAK,IAAI,YAAY,eAAe,YAAY,YAAY;AAEhE,mBAAS,KAAK,GAAG,KAAK,SAAS,EAAE,IAAI;AACnC,kBAAM,WAAW,KAAK;AACtB,kBAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,KAAK,WAAW,WAAW,CAAC;AAC3D,kBAAM,QACF,KAAK,IAAI,WAAW,cAAc,YAAY,WAAW;AAE7D,gBAAI,UAAU;AACd,qBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,oBAAM,KAAK,KAAK,cAAc;AAE9B,uBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,sBAAM,KAAK,KAAK,eAAe;AAE/B,yBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,wBAAM,KAAK,KAAK,cAAc;AAC9B,wBAAM,WAAW,OAAO,IAAI,OAAO,KAAK,OAAO,KAAK,OAAO;AAC3D,wBAAM,YAAY,SAAS,cAAc,IAAI,MACzC,SAAS,eAAe,IAAI,MAC5B,SAAS,cAAc,IAAI,MAAM,QAAQ;AAE7C,2BAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,0BAAM,QAAQ,SAAS,WAAW,EAAE;AACpC,0BAAM,SAAS,UAAU,YAAY,EAAE;AACvC,+BAAW,QAAQ;;;;;AAK3B,qBAAS,OAAO,IAAI,OAAO,KAAK,OAAO,KAAK,OAAO,KAAK,EAAE,IACtD;;;;;;AAOd,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,8BAA4C;EACvD,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACzGP,IAAME,OAAM,gBAAgB,KAAK,CAAC,OAAO,KAAK,IAAI,EAAE,CAAC;AAErD,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLP,IAAMC,QAAO,gBAAgB,MAAM,CAAC,OAAO,KAAK,KAAK,EAAE,CAAC;AAExD,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLR,SAAU,cAAc,MAI7B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAAC,QAAO,OAAO,OAAM,IAAI;AAC/B,QAAM,EAAC,UAAU,QAAQ,mBAAkB,IAAI;AAE/C,QAAM,CAAC,OAAO,aAAa,YAAY,WAAW,IAAIA,OAAM;AAC5D,QAAM,WAAW,MAAM,MAAM,CAAC;AAE9B,QAAM,CAAC,YAAY,SAAS,IAAI;AAChC,QAAM,SACF,OAAO,CAAC,UAAU,YAAY,WAAW,WAAW,GAAG,SAAS;AAEpE,QAAM,UAAUD,SAAQ,KAAK,IAAI,MAAM,MAAM,EAAE;AAC/C,QAAM,aAAaA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AACnD,QAAM,YAAYA,SAAQ,KAAK,IAAIC,OAAM,MAAM,EAAE;AAEjD,QAAM,WACF,aAAK,eAAeA,OAAM,KAAK;AACnC,QAAM,YAAY,aAAK,eACnB,OAAO,KAAK;AAKhB,WAAS,IAAI,GAAG,IAAI,UAAU,KAAK;AACjC,UAAM,WAAW,IAAI;AACrB,UAAM,KAAK,QAAQ,QAAQ;AAC3B,UAAM,KAAK,QAAQ,WAAW,CAAC;AAC/B,UAAM,KAAK,QAAQ,WAAW,CAAC;AAC/B,UAAM,KAAK,QAAQ,WAAW,CAAC;AAE/B,UAAM,OAAe,WAAW,CAAC;AACjC,QAAI,QAAQ,OAAO;AACjB;;AAGF,UAAM,cACD,aAAa,KAAM,KAAK,OAAO,cAAc,MAAM,aAAa,KAAK;AAC1E,UAAM,aACD,YAAY,KAAM,KAAK,OAAO,aAAa,MAAM,YAAY,KAAK;AAEvE,aAAS,IAAI,GAAG,IAAI,YAAY,KAAK;AACnC,YAAM,OAAgB,aAAa,IAC/B,MAAM,cAAc,KAAK,IAAK,cAC9B,OAAO,KAAK,OAAO,cAAc;AAErC,UAAI,OAAO,KAAK,OAAO,cAAc,GAAG;AACtC,iBAAS,IAAI,GAAG,IAAI,WAAW,KAAK;AAClC,mBAAS,IAAI,GAAG,IAAI,aAAa,KAAK;AACpC,kBAAM,MACF,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC;AAC7D,mBAAO,OAAO,GAAG,IAAI;;;AAGzB;;AAGF,UAAI,WAAW,YAAY;AACzB,cAAM,SAAS,KAAK,MAAM,IAAI;AAC9B,cAAM,YAAY,KAAK,KAAK,IAAI;AAChC,cAAM,QAAQ,OAAO;AAErB,iBAAS,IAAI,GAAG,IAAI,WAAW,KAAK;AAClC,gBAAM,OAAQ,YAAY,IACtB,MAAM,aAAa,KAAK,IAAI,aAC5B,OAAO,KAAK,OAAO,aAAa;AAEpC,cAAI,OAAO,KAAK,OAAO,aAAa,GAAG;AACrC,qBAAS,IAAI,GAAG,IAAI,aAAa,KAAK;AACpC,oBAAM,MACF,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC;AAC7D,qBAAO,OAAO,GAAG,IAAI;;AAEvB;;AAGF,gBAAM,UAAU,KAAK,MAAM,IAAI;AAC/B,gBAAM,WAAW,KAAK,KAAK,IAAI;AAC/B,gBAAM,QAAQ,OAAO;AAErB,mBAAS,IAAI,GAAG,IAAI,aAAa,KAAK;AACpC,gBAAI,MAAM,IAAI,UAAU,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC,IACrD,OAAO,SAAS,CAAC;AACrB,kBAAM,UAAU,UAAU,GAAG;AAE7B,kBAAM,IAAI,WAAW,SAAS,CAAC,IAAI,SAAS,SAAS,CAAC,IAClD,OAAO,SAAS,CAAC;AACrB,kBAAM,WAAW,UAAU,GAAG;AAE9B,kBAAM,IAAI,UAAU,SAAS,CAAC,IAAI,YAAY,SAAS,CAAC,IACpD,OAAO,SAAS,CAAC;AACrB,kBAAM,aAAa,UAAU,GAAG;AAEhC,kBAAM,IAAI,WAAW,SAAS,CAAC,IAAI,YAAY,SAAS,CAAC,IACrD,OAAO,SAAS,CAAC;AACrB,kBAAM,cAAc,UAAU,GAAG;AAEjC,kBAAM,MAAM,WAAW,WAAW,WAAW;AAC7C,kBAAM,SAAS,cAAc,cAAc,cAAc;AAEzD,kBAAM,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC;AAC/D,mBAAO,OAAO,GAAG,IAAI,OAAQ,SAAS,OAAO;;;aAG5C;AACL,iBAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,gBAAM,OAAQ,YAAY,IACtB,MAAM,aAAa,KAAK,IAAI,aAC5B,OAAO,KAAK,OAAO,aAAa;AAEpC,cAAI,OAAO,KAAK,OAAO,aAAa,GAAG;AACrC,qBAAS,IAAI,GAAG,IAAI,aAAa,KAAK;AACpC,oBAAM,MACF,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC;AAC7D,qBAAO,OAAO,GAAG,IAAI;;AAEvB;;AAGF,gBAAM,WAAW,KAAK,MAAM,IAAI;AAChC,gBAAM,WAAW,KAAK,MAAM,IAAI;AAChC,mBAAS,IAAI,GAAG,IAAI,aAAa,KAAK;AACpC,kBAAM,QAAQ,IAAI,WAAW,SAAS,CAAC,IAAI,WAAW,SAAS,CAAC,IAC5D,OAAO,SAAS,CAAC;AACrB,kBAAM,SACF,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC,IAAI,IAAI,UAAU,CAAC;AAC7D,mBAAO,OAAO,MAAM,IAAI,UAAU,KAAK;;;;;;AAOjD,SAAOD,SAAQ,eAAe,OAAO,OAAO,OAAO,OAAO,OAAO,MAAM;AACzE;AAEO,IAAM,sBAAoC;EAC/C,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC7IR,SAAUE,SACZ,MAC2B;AAC7B,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,MAAM,WAAW,SAAAC,SAAO,IAAI;AAEnC,mBAAiB,GAAG,SAAS;AAE7B,QAAM,cAAc,qBAAa,mBAAmB,CAAC,IAAI,GAAG,EAAE,MAAM,MAAM;AAC1E,MAAI,KAAK;AACT,MAAI,eAAe,MAAM;AACvB,SAAKC,WAAU,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAF,UAAS,OAAO,EAAC,MAAM,YAAW,EAAC,CAAC;;AAEnE,QAAM,eAAe,qBAAa,iBAAiB,GAAG,EAAE,MAAM,MAAM,EAAE,CAAC;AAEvE,MAAI,iBAAiB,GAAG,MAAM,SAAS,GAAG;AACxC,UAAM,IAAI,MACN,qDACQ,GAAG,MAAM,SAAS,CAAC,iBAAiB,YAAY,EAAE;;AAGhE,QAAM,cAAc,WAAW,GAAG,OAAO,OAAO;AAChD,QAAM,OAAO,aAAK,mBACD,aAAK,cAAc,GAAG,KAAK,GAAG,WAAW;AAE1D,QAAM,QAAQA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC1C,QAAM,WAAW,GAAG,MAAM,GAAG,MAAM,SAAS,CAAC;AAC7C,QAAM,gBAAgBC,WAClB,CAAC,GAAW,MAAc,IAAI,WAAW,IAAI,IAC7C,CAAC,GAAW,MAAc,IAAI;AAClC,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK,UAAU;AAC/C,aAAS,IAAI,GAAG,IAAI,UAAU,KAAK;AACjC,YAAM,MAAM,cAAc,GAAG,CAAC;AAC9B,UAAI,MAAM,GAAG;AACX,aAAK,GAAG,IAAI,YAAY,IAAI,MAAM,GAAG;aAChC;AACL,cAAM,UAAU,cAAc,GAAG,IAAI,CAAC;AACtC,aAAK,GAAG,IAAI,YAAY,MAAM,OAAO,IAAI,KAAK,OAAO,IAC7B,MAAM,GAAG,IAAI,KAAK,OAAO;;;;AAKvD,QAAM,SAASD,SAAQ,eAAe,GAAG,OAAO,aAAa,IAAI;AAEjE,MAAI,eAAe,MAAM;AACvB,UAAM,qBAAqB,qBAAa,uBAAuB,WAAW;AAC1E,UAAM,0BAA0BE,WAC5B,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAF,UAAS,OAAO,EAAC,MAAM,mBAAkB,EAAC,CAAC;AAErE,IAAAA,SAAQ,8BAA8B,MAAM;AAC5C,IAAAA,SAAQ,8BAA8B,EAAE;AAExC,WAAO;;AAGT,SAAO;AACT;AAEO,IAAM,gBAA8B;EACzC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC/DR,SAAUI,QACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,MAAM,WAAW,SAAAC,SAAO,IAAI;AAEnC,mBAAiB,GAAG,QAAQ;AAE5B,QAAM,cAAc,qBAAa,mBAAmB,CAAC,IAAI,GAAG,EAAE,MAAM,MAAM;AAC1E,MAAI,KAAK;AACT,MAAI,eAAe,MAAM;AACvB,SAAKC,WAAU,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAF,UAAS,OAAO,EAAC,MAAM,YAAW,EAAC,CAAC;;AAEnE,QAAM,eAAe,qBAAa,iBAAiB,GAAG,EAAE,MAAM,MAAM,EAAE,CAAC;AAEvE,MAAI,iBAAiB,GAAG,MAAM,SAAS,GAAG;AACxC,UAAM,IAAI,MACN,oDACQ,GAAG,MAAM,SAAS,CAAC,iBAAiB,YAAY,EAAE;;AAGhE,QAAM,cAAc,WAAW,GAAG,OAAO,OAAO;AAChD,QAAM,OAAO,aAAK,oBACD,aAAK,cAAc,GAAG,KAAK,GAAG,WAAW;AAE1D,QAAM,QAAQA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC1C,QAAM,WAAW,GAAG,MAAM,GAAG,MAAM,SAAS,CAAC;AAC7C,QAAM,gBAAgBC,WAClB,CAAC,GAAW,MAAc,IAAI,WAAW,IAAI,IAC7C,CAAC,GAAW,MAAc,IAAI;AAClC,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK,UAAU;AAC/C,aAAS,IAAI,GAAG,IAAI,UAAU,KAAK;AACjC,YAAM,MAAM,cAAc,GAAG,CAAC;AAC9B,UAAI,MAAM,GAAG;AACX,aAAK,GAAG,IAAI,YAAY,IAAI,MAAM,GAAG;aAChC;AACL,cAAM,UAAU,cAAc,GAAG,IAAI,CAAC;AACtC,aAAK,GAAG,IAAI,YAAY,MAAM,OAAO,IAAI,KAAK,OAAO,IAC7B,MAAM,GAAG,IAAI,KAAK,OAAO;;;;AAKvD,QAAM,SAASD,SAAQ,eAAe,GAAG,OAAO,aAAa,IAAI;AAEjE,MAAI,eAAe,MAAM;AACvB,UAAM,qBAAqB,qBAAa,uBAAuB,WAAW;AAC1E,UAAM,0BAA0BE,WAC5B,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAF,UAAS,OAAO,EAAC,MAAM,mBAAkB,EAAC,CAAC;AAErE,IAAAA,SAAQ,8BAA8B,MAAM;AAC5C,IAAAA,SAAQ,8BAA8B,EAAE;AAExC,WAAO;;AAGT,SAAO;AACT;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AChER,SAAUI,eAAc,MAI7B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,QAAO,IAAI;AACrB,QAAM,EAAC,MAAM,aAAY,IAAI;AAE7B,MAAI,EAAE,MAAM,WAAW,GAAG;AACxB,UAAM,QAAQA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,UAAM,cAAcA,SAAQ,KAAK,IAAI,QAAQ,MAAM,EAAE;AAErD,UAAM,UACF,aAAa,OAAO,aAAa,QAAQ,OAAO,QAAQ,OAAO,IAAI;AAEvE,WAAOA,SAAQ,eAAe,CAAC,IAAI,GAAG,QAAQ,OAAO,OAAO;aACnD,EAAE,MAAM,WAAW,GAAG;AAC/B,UAAM,OAAOA,SAAQ,WAA4B,CAAC;AAClD,UAAM,aAAaA,SAAQ,WAA4B,OAAO;AAE9D,UAAM,SAAS,mBAAmB,MAAM,YAAY,MAAM,YAAY;AAEtE,WAAOA,SAAQ,eAAe,OAAO,OAAO,QAAQ,OAAO,OAAO,MAAM;;AAG1E,QAAM,IAAI,MACN,qEACG,EAAE,MAAM,MAAM,GAAG;AAC1B;AAEO,IAAM,sBAAoC;EAC/C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACnCR,SAAUE,cAAa,MAI5B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,WAAW,WAAU,IAAI;AAEhC,eAAK,OACD,eAAe,QACf,MAAM,+DACF,UAAU,EAAE;AAEpB,QAAM,YAAY,EAAE,MAAM,CAAC;AAC3B,QAAM,cAAc,EAAE,MAAM,CAAC;AAC7B,QAAM,aAAa,EAAE,MAAM,CAAC;AAC5B,QAAM,aAAa,EAAE,MAAM,CAAC;AAE5B,QAAM,eAAe,cAAc;AACnC,QAAM,cAAc,aAAa;AACjC,QAAM,cAAc,cAAc,YAAY;AAE9C,QAAM,UAAUA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,SACF,IAAI,aAAa,YAAY,eAAe,cAAc,WAAW;AAEzE,MAAI,YAAY;AAChB,WAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,aAAS,IAAI,GAAG,IAAI,cAAc,EAAE,GAAG;AACrC,YAAM,MAAM,KAAK,MAAM,IAAI,SAAS;AACpC,YAAM,UAAW,IAAI;AACrB,eAAS,IAAI,GAAG,IAAI,aAAa,EAAE,GAAG;AACpC,cAAM,MAAM,KAAK,MAAM,IAAI,SAAS;AACpC,cAAM,UAAW,IAAI;AACrB,cAAM,WAAW,UAAU,YAAY,WAAW;AAClD,iBAAS,IAAI,GAAG,IAAI,aAAa,EAAE,GAAG;AACpC,gBAAM,MAAM,IAAI;AAChB,gBAAM,WACF,MAAM,cAAc,MAAM,cAAc,MAAM,cAAc;AAChE,iBAAO,WAAW,IAAI,QAAQ,QAAQ;;;;;AAM9C,SAAOA,SAAQ,eACX,CAAC,WAAW,cAAc,aAAa,WAAW,GAAG,EAAE,OAAO,MAAM;AAC1E;AAEO,IAAM,qBAAmC;EAC9C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACpDR,SAAU,sBAAsB,MAIrC;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,OAAM,IAAI;AACpB,QAAM,EAAC,SAAS,KAAAC,MAAK,WAAW,gBAAe,IAAI;AAEnD,mBAAiB,CAAC,GAAG,MAAM,GAAG,uBAAuB;AAErD,QAAM,WAAW,aAAK,eAAe,EAAE,KAAK;AAC5C,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AAEtD,MAAI,aAAa;AACjB,MAAI,cAAc,MAAM;AACtB,iBAAa,CAAC,GAAG,CAAC;;AAGpB,eAAK,OACD,qBAAa,+BAA+B,SAAS,UAAU,GAC/D,MAAM,gFACgB,OAAO,mBAAmB,UAAU,GAAG;AAEjE,QAAM,WAAW,qBAAa;IAC1B,EAAE;IACF,OAAO;IAA2C;IAAS;IAC3DA;IAAK;IAAiB;;EAAoB;AAE9C,QAAM,EAAC,cAAc,aAAa,gBAAgB,eAAe,QAAO,IACpE;AACJ,QAAM,UAAU,QAAQ;AACxB,QAAM,SAAS,QAAQ;AACvB,QAAM,QAAQ,SAAS,cAAc,SAAS;AAC9C,QAAM,IAAI,IAAI,aAAa,SAAS,UAAU,EAAE,KAAkB;AAClE,QAAM,QAAQD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,QAAQA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAC9C,QAAM,QAAQ,EAAE;AAEhB,WAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,UAAM,WAAW,IAAI,SAAS,CAAC;AAC/B,UAAM,WAAW,IAAI,EAAE,QAAQ,CAAC;AAChC,aAAS,KAAK,GAAG,KAAK,SAAS,WAAW,EAAE,IAAI;AAC9C,YAAM,WAAW,WAAW,KAAK,EAAE,QAAQ,CAAC;AAC5C,YAAM,WAAW,KAAK,SAAS,eAAe;AAC9C,eAAS,KAAK,GAAG,KAAK,cAAc,EAAE,IAAI;AACxC,cAAM,KAAK,WAAW,KAAK;AAC3B,YAAI,KAAK,KAAK,MAAM,SAAS,UAAU;AACrC;;AAEF,cAAM,WAAW,KAAK,cAAc,CAAC;AACrC,cAAM,WAAW,WAAW,KAAK,SAAS,CAAC;AAC3C,iBAAS,KAAK,GAAG,KAAK,SAAS,UAAU,EAAE,IAAI;AAC7C,gBAAM,WAAW,WAAW,KAAK,EAAE,QAAQ,CAAC;AAC5C,gBAAM,WAAW,KAAK,SAAS,cAAc;AAC7C,mBAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,kBAAM,KAAK,WAAW,KAAK;AAC3B,gBAAI,KAAK,KAAK,MAAM,SAAS,SAAS;AACpC;;AAEF,kBAAM,WAAW,WAAW,KAAK,cAAc,CAAC;AAChD,kBAAM,WAAW,WAAW,KAAK,SAAS;AAC1C,gBAAI,WAAW;AACf,gBAAI,WAAW;AACf,qBAAS,KAAK,GAAG,KAAK,SAAS,YAAY,EAAE,IAAI;AAC/C,oBAAM,OAAO,MAAM,WAAW,EAAE;AAChC,uBAAS,IAAI,GAAG,IAAI,OAAO,EAAE,GAAG;AAC9B,sBAAM,WAAW,CAAC,KAAK,OAAO,MAAM,WAAW,CAAC;;AAElD,0BAAY;AACZ,0BAAY;;;;;;;AAQxB,SAAOA,SAAQ,eAAe,EAAE,OAAO,EAAE,OAAO,EAAE,MAAM;AAC1D;AAEO,IAAM,8BAA4C;EACvD,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACpFR,SAAU,oCAAoC,MAInD;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,GAAE,IAAI;AAChB,QAAM,EAAC,SAAS,WAAW,KAAAC,MAAK,iBAAiB,YAAW,IAAI;AAEhE,mBAAiB,CAAC,GAAG,EAAE,GAAG,qCAAqC;AAE/D,QAAM,WAAW,qBAAa;IAC1B,EAAE;IAA2C;IAAa;IAC1D;IAAWA;IAAK;IAAiB;;EAAoB;AAEzD,QAAM,EAAC,cAAc,aAAa,cAAc,YAAW,IAAI;AAE/D,QAAM,KAAK,IAAI,aAAa,SAAS,aAAa,SAAS;AAE3D,QAAM,UAAU,SAAS,QAAQ;AACjC,QAAM,SAAS,SAAS,QAAQ;AAChC,QAAM,QAAQ,SAAS,cAAc,SAAS;AAE9C,QAAM,QAAQD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,OAAO,IAAI,aAAa,EAAE,OAAO,EAAE,OAAO,KAAK;AACrD,QAAM,SAASA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC3C,QAAM,QAAQ,IAAI,aAAa,GAAG,OAAO,GAAG,OAAO,MAAM;AACzD,WAAS,KAAK,GAAG,KAAK,cAAc,EAAE,IAAI;AACxC,UAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,MAAM,SAAS,MAAM,YAAY,CAAC;AACjE,UAAM,QAAQ,KAAK,IACf,SAAS,YAAY,SAAS,WAAW,SAAS,MAAM,YAAY;AAExE,aAAS,KAAK,GAAG,KAAK,aAAa,EAAE,IAAI;AACvC,YAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,MAAM,UAAU,MAAM,WAAW,CAAC;AACjE,YAAM,QAAQ,KAAK,IACf,SAAS,WAAW,SAAS,UAAU,UAAU,MAAM,WAAW;AAEtE,eAAS,KAAK,GAAG,KAAK,SAAS,aAAa,EAAE,IAAI;AAChD,cAAM,KAAK,KAAK,MAAM,KAAK,KAAK;AAChC,cAAM,KAAK,KAAK;AAEhB,YAAI,UAAU;AACd,iBAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,mBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,kBAAM,KAAK,KAAK,KAAK,eAAe;AACpC,qBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,oBAAM,KAAK,KAAK,KAAK,cAAc;AACnC,yBAAY,KAAK,IAAI,GAAG,IAAI,IAAI,EAAE,IAC7B,MAAM,IAAI,GAAG,IAAI,IAAI,EAAE;;;;AAIlC,WAAG,IAAI,SAAS,IAAI,IAAI,IAAI,EAAE;;;;AAKpC,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,4CAA0D;EACrE,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC/DR,SAAU,mCAAmC,MAIlD;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,IAAI,OAAM,IAAI;AACrB,QAAM,EAAC,SAAS,WAAW,KAAAC,MAAK,iBAAiB,WAAU,IAAI;AAE/D,mBAAiB,CAAC,IAAI,MAAM,GAAG,oCAAoC;AAEnE,QAAM,YAAY,aAAK,eAAe,GAAG,KAAK;AAC9C,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AAEtD,QAAM,WAAW,qBAAa;IAC1B;IAAY,OAAO;IAA2C;IAC9D;IAAWA;IAAK;IAAiB;;EAAoB;AAEzD,QAAM,KAAK,IAAI,aAAa,SAAS,SAAS,SAAS;AACvD,QAAM,WAAW,GAAG;AACpB,QAAM,CAAC,MAAM,MAAM,IAAI,IAAI,GAAG;AAC9B,QAAM,WAAWD,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC7C,QAAM,CAAC,MAAM,MAAM,IAAI,IAAI;AAC3B,QAAM,YAAYA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAClD,QAAM,CAAC,OAAO,OAAO,KAAK,IAAI;AAC9B,QAAM,EACJ,WACA,cACA,aACA,YACA,UACA,SACA,aACA,WACA,UACA,cACA,YAAW,IACT;AACJ,QAAM,SAAS,eAAe,IAAI,SAAS,QAAQ;AACnD,QAAM,UAAU,cAAc,IAAI,SAAS,QAAQ;AACnD,QAAM,QAAQ,cAAc;AAE5B,WAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,aAAS,KAAK,GAAG,KAAK,YAAY,EAAE,IAAI;AACtC,eAAS,KAAK,GAAG,KAAK,UAAU,EAAE,IAAI;AACpC,cAAM,WAAW,KAAK;AACtB,cAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,KAAK,WAAW,YAAY,CAAC;AAC5D,cAAM,QACF,KAAK,IAAI,YAAY,eAAe,YAAY,YAAY;AAEhE,iBAAS,KAAK,GAAG,KAAK,SAAS,EAAE,IAAI;AACnC,gBAAM,WAAW,KAAK;AACtB,gBAAM,QAAQ,KAAK,IAAI,GAAG,KAAK,KAAK,WAAW,WAAW,CAAC;AAC3D,gBAAM,QACF,KAAK,IAAI,WAAW,cAAc,YAAY,WAAW;AAE7D,cAAI,UAAU;AACd,mBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,kBAAM,KAAK,KAAK,eAAe;AAE/B,qBAAS,KAAK,OAAO,KAAK,OAAO,EAAE,IAAI;AACrC,oBAAM,KAAK,KAAK,cAAc;AAC9B,oBAAM,WAAW,OAAO,IAAI,OAAO,KAAK,OAAO;AAC/C,oBAAM,YAAY,SAAS,eAAe,IAAI,MAC1C,SAAS,cAAc,IAAI,MAAM,QAAQ;AAE7C,uBAAS,KAAK,GAAG,KAAK,OAAO,EAAE,IAAI;AACjC,sBAAM,KAAK,KAAK,QAAQ;AACxB,sBAAM,QAAQ,SAAS,WAAW,EAAE;AACpC,sBAAM,SAAS,UAAU,YAAY,EAAE;AACvC,2BAAW,QAAQ;;;;AAIzB,mBAAS,OAAO,IAAI,OAAO,KAAK,OAAO,KAAK,EAAE,IAAI;;;;;AAM1D,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,2CAAyD;EACpE,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACvFR,SAAUE,MAAK,MAAmD;AAEtE,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,EAAC,IAAI;AAEZ,QAAM,QAAQ,aAAK,cAAc,EAAE,KAAK;AAExC,QAAM,QAAQA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,SAAS,OAAO,CAAC,OAAO,KAAK,GAAG,EAAE,KAAK;AAC7C,QAAM,OAAO,OAAO;AACpB,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,SAAK,IAAI,QAAQ,CAAC,IAAI,MAAM,CAAC;;AAG/B,QAAM,WAAW,CAAC,GAAG,EAAE,OAAO,GAAG,EAAE,KAAK;AAExC,SAAOA,SAAQ,eAAe,UAAU,OAAO,OAAO,OAAO,MAAM;AACrE;AAEO,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACtBP,IAAM,mBAAiC;EAC5C,YAAY;EACZ,aAAa;EACb,YAAY,CAAC,EAAC,QAAQ,SAAAE,UAAS,MAAK,MAAK;AACvC,UAAM,EAAC,GAAG,OAAM,IAAI;AACpB,UAAM,EAAC,SAAS,KAAAC,MAAK,UAAS,IAAI;AAClC,UAAM,aAAaD;AAEnB,UAAM,QAAQ,WAAW,KAAK,IAAI,EAAE,MAAM,EAAE;AAC5C,UAAM,QAAQ,EAAE,MAAM;AAEtB,UAAM,aAAa,WAAW,KAAK,IAAI,OAAO,MAAM,EAAE;AACtD,UAAM,aAAa,OAAO,MAAM;AAEhC,UAAM,EACJ,WACA,UACA,SACA,YACA,WACA,UACA,SACA,cACA,aACA,cACA,aACA,gBACA,eACA,SAAQ,IAEN,qBAAa,sBACT,EAAE,OACF,OAAO,OAAmC,SAASC,MACnD,QAAyB,SAAS;AAE1C,UAAM,UAAU,aAAK,cAAc,QAAQ;AAC3C,UAAM,UAAU,SAAS;AACzB,UAAM,aAAa,aAAK,kBAAkB,EAAE,OAAO,OAAO;AAM1D,aAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,eAAS,OAAO,GAAG,OAAO,WAAW,EAAE,MAAM;AAC3C,cAAM,OAAO,OAAO,eAAe,QAAQ;AAC3C,iBAAS,OAAO,GAAG,OAAO,UAAU,EAAE,MAAM;AAC1C,gBAAM,OAAO,OAAO,cAAc,QAAQ;AAC1C,mBAAS,IAAI,GAAG,IAAI,YAAY,EAAE,GAAG;AACnC,gBAAI,SAAS,OAAO;AACpB,qBAAS,IAAI,GAAG,IAAI,cAAc,EAAE,GAAG;AACrC,oBAAM,MAAM,OAAO,IAAI;AACvB,kBAAI,OAAO,KAAK,MAAM,UAAU;AAC9B,yBAAS,IAAI,GAAG,IAAI,aAAa,EAAE,GAAG;AACpC,wBAAM,MAAM,OAAO,IAAI;AACvB,sBAAI,OAAO,KAAK,MAAM,SAAS;AAC7B,0BAAM,SAAS,aAAK,WAChB,CAAC,GAAG,KAAK,KAAK,CAAC,GAAG,OAAO,aAAK,eAAe,EAAE,KAAK,CAAC;AACzD,0BAAM,cAAc,aAAK,WACrB,CAAC,GAAG,GAAG,CAAC,GAAG,YACX,aAAK,eAAe,OAAO,KAAK,CAAC;AACrC,0BAAM,MAAM,MAAM,MAAM,IAAI,WAAW,WAAW;AAClD,wBAAI,MAAM,QAAQ;AAChB,+BAAS;;;;;;AAMnB,kBAAM,cAAc,aAAK,WACrB,CAAC,GAAG,MAAM,MAAM,CAAC,GAAG,SAAS,aAAK,eAAe,QAAQ,CAAC;AAC9D,uBAAW,WAAW,IAAI;;;;;AAMlC,UAAM,SAAS,WAAW,MACtB,aAAK,aAAa,YAAY,EAAE,KAAK,GAAG,UAAU,EAAE,KAAK;AAE7D,WAAO,EAAC,QAAQ,OAAO,UAAU,OAAO,EAAE,MAAK;EACjD;;;;AChFK,IAAM,iCAA+C;EAC1D,YAAY;EACZ,aAAa;EACb,YAAY,CAAC,EAAC,QAAQ,SAAAC,UAAS,MAAK,MAAK;AACvC,UAAM,EAAC,GAAG,QAAQ,GAAE,IAChB;AACJ,UAAM,EAAC,SAAS,KAAAC,MAAK,UAAS,IAAI;AAClC,UAAM,aAAaD;AAEnB,UAAM,KACF,aAAK,cACD,EAAE,OAAO,WAAW,KAAK,IAAI,EAAE,MAAM,EAAE,MAAoB;AAGnE,UAAM,UAAU,aAAK,cACD,OAAO,OACP,WAAW,KAAK,IAAI,OAAO,MAAM,EAAE,MACrB;AAElC,UAAM,EACJ,WACA,UACA,SACA,YACA,WACA,UACA,SACA,cACA,aACA,cACA,aACA,gBACA,eACA,SAAQ,IAEN,qBAAa,sBACT,EAAE,OACF,OAAO,OAAmC,SAASC,MACnD,QAAyB,SAAS;AAE1C,iBAAK,OACD,GAAG,SAAS,SAAS,QACrB,MAAM,YAAY,wBAAwB,0CACD,SAAS,MAAM,aACjD,GAAG,IAAI,EAAE;AAEpB,UAAM,MACF,aAAK,cACD,UAAU,WAAW,KAAK,IAAI,GAAG,MAAM,EAAE,MAAoB;AAKrE,UAAM,YAAY,aAAK,0BACD,OAAO,OAAO,OAAO,KAAK;AAOhD,aAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,eAAS,OAAO,GAAG,OAAO,WAAW,EAAE,MAAM;AAC3C,cAAM,OAAO,OAAO,eAAe,QAAQ;AAC3C,iBAAS,OAAO,GAAG,OAAO,UAAU,EAAE,MAAM;AAC1C,gBAAM,OAAO,OAAO,cAAc,QAAQ;AAC1C,mBAAS,IAAI,GAAG,IAAI,YAAY,EAAE,GAAG;AACnC,gBAAI,SAAS,OAAO;AACpB,gBAAI,OAAO;AACX,gBAAI,OAAO;AACX,qBAAS,IAAI,GAAG,IAAI,cAAc,EAAE,GAAG;AACrC,oBAAM,MAAM,OAAO,IAAI;AACvB,kBAAI,OAAO,KAAK,MAAM,UAAU;AAC9B,yBAAS,IAAI,GAAG,IAAI,aAAa,EAAE,GAAG;AACpC,wBAAM,MAAM,OAAO,IAAI;AACvB,sBAAI,OAAO,KAAK,MAAM,SAAS;AAC7B,0BAAM,MAAM,GAAG,CAAC,EAAE,GAAG,EAAE,GAAG,EAAE,CAAC,IAAI,QAAQ,CAAC,EAAE,CAAC,EAAE,CAAC;AAChD,wBAAI,MAAM,QAAQ;AAChB,+BAAS;AACT,6BAAO;AACP,6BAAO;;;;;;AAMjB,sBAAU,IAAI,EAAE,IAAI,EAAE,CAAC,KAAK,IAAI,CAAC,EAAE,IAAI,EAAE,IAAI,EAAE,CAAC;;;;;AAMxD,UAAM,SAAS,WAAW,MACtB,aAAK,aAAa,WAAW,EAAE,KAAK,GAAG,OAAO,OAAO,OAAO,KAAK;AAErE,WAAO,EAAC,QAAQ,OAAO,OAAO,OAAO,OAAO,OAAO,MAAK;EAC1D;;;;AChGK,IAAM,gCAA8C;EACzD,YAAY;EACZ,aAAa;EACb,YAAY,CAAC,EAAC,QAAQ,SAAAC,UAAS,MAAK,MAAK;AACvC,UAAM,EAAC,GAAG,QAAQ,GAAE,IAChB;AACJ,UAAM,EAAC,SAAS,KAAAC,MAAK,UAAS,IAAI;AAClC,UAAM,aAAaD;AAEnB,UAAM,KACF,aAAK,cACD,EAAE,OAAO,WAAW,KAAK,IAAI,EAAE,MAAM,EAAE,MAAoB;AAGnE,UAAM,UAAU,aAAK,cACD,OAAO,OACP,WAAW,KAAK,IAAI,OAAO,MAAM,EAAE,MACrB;AAElC,UAAM,EACJ,WACA,UACA,SACA,YACA,WACA,UACA,SACA,cACA,aACA,cACA,aACA,gBACA,eACA,SAAQ,IAEN,qBAAa,sBACT,EAAE,OACF,OAAO,OAAmC,SAASC,MACnD,QAAyB,SAAS;AAE1C,iBAAK,OACD,GAAG,SAAS,SAAS,QACrB,MAAM,YAAY,uBAAuB,0CACA,SAAS,MAAM,aACjD,GAAG,IAAI,EAAE;AAEpB,UAAM,MACF,aAAK,cACD,UAAU,WAAW,KAAK,IAAI,GAAG,MAAM,EAAE,MAAoB;AAKrE,UAAM,YACF,aAAK,0BAA0B,EAAE,OAAO,EAAE,KAAK;AAOnD,aAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,eAAS,OAAO,GAAG,OAAO,WAAW,EAAE,MAAM;AAC3C,cAAM,OAAO,OAAO,eAAe,QAAQ;AAC3C,iBAAS,OAAO,GAAG,OAAO,UAAU,EAAE,MAAM;AAC1C,gBAAM,OAAO,OAAO,cAAc,QAAQ;AAC1C,mBAAS,IAAI,GAAG,IAAI,YAAY,EAAE,GAAG;AACnC,gBAAI,SAAS,OAAO;AACpB,gBAAI,SAAU,OAAO,IAAK,IAAI;AAC9B,gBAAI,SAAU,OAAO,IAAK,IAAI;AAC9B,qBAAS,IAAI,GAAG,IAAI,cAAc,EAAE,GAAG;AACrC,oBAAM,MAAM,OAAO,IAAI;AACvB,kBAAI,OAAO,KAAK,MAAM,UAAU;AAC9B,yBAAS,IAAI,GAAG,IAAI,aAAa,EAAE,GAAG;AACpC,wBAAM,MAAM,OAAO,IAAI;AACvB,sBAAI,OAAO,KAAK,MAAM,SAAS;AAC7B,0BAAM,MAAM,GAAG,CAAC,EAAE,GAAG,EAAE,GAAG,EAAE,CAAC,IAAI,QAAQ,CAAC,EAAE,CAAC,EAAE,CAAC;AAChD,wBAAI,MAAM,QAAQ;AAChB,+BAAS;AACT,+BAAS;AACT,+BAAS;;;;;;AAMnB,sBAAU,CAAC,EAAE,MAAM,EAAE,MAAM,EAAE,CAAC,KAAK,IAAI,CAAC,EAAE,IAAI,EAAE,IAAI,EAAE,CAAC;;;;;AAM/D,UAAM,SAAS,WAAW,MACtB,aAAK,aAAa,WAAW,EAAE,KAAK,GAAG,EAAE,OAAO,EAAE,KAAK;AAE3D,WAAO,EAAC,QAAQ,OAAO,EAAE,OAAO,OAAO,EAAE,MAAK;EAChD;;;;AC3FI,SAAUC,KACZ,MAAmE;AAErE,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,MAAM,SAAQ,IAAI;AAEzB,mBAAiB,GAAG,KAAK;AAEzB,MAAI;AACJ,MAAI,EAAE,UAAU,QAAQ;AACtB,SAAKC,MAAK,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,OAAO,QAAO,EAAC,CAAC;SACpD;AACL,SAAK,SAAS,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAA,SAAO,CAAC;;AAGtC,QAAM,QAAQ,GAAG,MAAM;AACvB,QAAM,OAAO,aAAK,eAAe,MAAM,GAAG,KAAK;AAC/C,QAAM,cAAc,qBAAa,mBAAmB,MAAM,KAAK;AAE/D,MAAI,gBAAgB;AACpB,MAAI,YAAY;AAChB,MAAI,eAAe,MAAM;AACvB,gBACIE,WAAU,EAAC,QAAQ,EAAC,GAAG,GAAE,GAAG,SAAAF,UAAS,OAAO,EAAC,MAAM,YAAW,EAAC,CAAC;AACpE,oBAAgB,qBAAa,iBAAiB,cAAc,QAAQ,KAAK;;AAG3E,uBAAa,2BACT,OAAO,eAAe,UAAU,MAAM,MAAM;AAEhD,QAAM,CAAC,UAAU,WAAW,IACxB,qBAAa,0BAA0B,UAAU,OAAO,aAAa;AACzE,QAAM,cAAc,qBAAa,WAAW,UAAU,OAAO,OAAO;AACpE,MAAI,SAASG,OAAMH,UAAS,UAAU,WAAW;AACjD,QAAM,aAAa,aAAK,cAAc,WAAW;AACjD,QAAM,OAAOA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAE7C,QAAM,QAAQA,SAAQ,KAAK,IAAI,UAAU,MAAM,EAAE;AACjD,WAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,EAAE,GAAG;AACpC,UAAM,SAAS,IAAI;AACnB,QAAID,OAAM;AACV,aAAS,IAAI,GAAG,IAAI,YAAY,EAAE,GAAG;AACnC,MAAAA,QAAO,MAAM,SAAS,CAAC;;AAEzB,SAAK,CAAC,IAAIA;;AAGZ,MAAI,UAAU;AACZ,UAAM,WAAW,qBAAa,qBAAqB,OAAO,OAAO,IAAI;AACrE,UAAM,YAAY;AAClB,aAASK,SAAQ,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAJ,UAAS,OAAO,EAAC,OAAO,SAAQ,EAAC,CAAC;AACzE,IAAAA,SAAQ,8BAA8B,SAAS;;AAGjD,EAAAA,SAAQ,8BAA8B,EAAE;AAExC,MAAI,eAAe,MAAM;AACvB,IAAAA,SAAQ,8BAA8B,SAAS;;AAGjD,SAAO;AACT;AAEO,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACpER,SAAUM,QACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,SAAQ,IAAI;AACnB,QAAM,UAAU;AAEhB,QAAM,EAAC,SAAS,YAAY,OAAM,IAC9B,qBAAa,qBAAqB,UAAU,QAAQ,MAAM;AAC9D,uBAAa,oBAAoB,QAAQ,QAAQ,QAAQ,OAAO;AAChE,QAAM,EAAC,MAAM,MAAK,IAAI,qBAAa,qBAAqB,YAAY,MAAM;AAE1E,QAAM,SAAS,MAAM;AACrB,MAAI,MAAuB;AAC3B,MAAI,mBAAmB,QAAQ;AAC/B,QAAM,mBAAiC,CAAA;AACvC,WAAS,IAAI,GAAG,IAAI,QAAQ,EAAE,GAAG;AAC/B,eAAW,UAAU,MAAM,CAAC,GAAG;AAC7B,YAAM,EAAC,oBAAoB,MAAM,YAAY,aAAY,IACrD,qBAAa,qBAAqB,kBAAkB,OAAO,MAAM,CAAC;AACtE,UAAI;AACJ,UAAI,qBAAa,sBAAsB,IAAI,GAAG;AAC5C,YAAI,QAAQ,MAAM;aACb;AACL,YAAIC,WAAU,EAAC,QAAQ,EAAC,GAAG,QAAQ,MAAM,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,KAAI,EAAC,CAAC;AACpE,yBAAiB,KAAK,CAAC;;AAEzB,YAAM,cAAwB,EAAE,MAAM,MAAK;AAC3C,eAAS,IAAI,GAAG,IAAI,aAAa,QAAQ,EAAE,GAAG;AAC5C,oBAAY,OAAO,aAAa,CAAC,GAAG,GAAG,CAAC;;AAG1C,UAAI,CAAC,aAAK,YAAY,EAAE,OAAO,WAAW,GAAG;AAC3C,YAAIE,SAAQ,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,YAAW,EAAC,CAAC;AAC/D,yBAAiB,KAAK,CAAC;;AAEzB,UAAI,QAAQ,MAAM;AAChB,cAAM;aACD;AAEL,cAAM,SAAS,EAAC,QAAQ,EAAC,GAAG,GAAG,GAAG,IAAG,GAAG,SAAAA,SAAO,CAAC;AAChD,yBAAiB,KAAK,GAAG;;;AAG7B,QAAI,IAAI,SAAS,GAAG;AAClB,UAAI,KAAK,CAAC,KAAK,GAAG;AAChB,cAAMG,KAAI;UACR,QAAQ,EAAC,GAAG,IAAG;UACf,SAAAH;UACA,OAAO;YACL,MAAM,KAAK,CAAC,KAAK,QAAQ,SAAS;YAClC,UAAU;;SAEb;AACD,yBAAiB,KAAK,GAAG;;AAE3B;;;AAKJ,aAAW,cAAc,kBAAkB;AACzC,QAAI,eAAe,KAAK;AACtB;;AAEF,IAAAA,SAAQ,8BAA8B,UAAU;;AAGlD,SAAO;AACT;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC9ER,SAAU,QAAQ,MAAsD;AAE5E,QAAM,EAAC,QAAQ,SAAAK,SAAO,IAAI;AAC1B,QAAM,EAAC,IAAI,EAAC,IAAI;AAEhB,mBAAiB,CAAC,IAAI,CAAC,GAAG,SAAS;AAEnC,QAAM,eAAe,IAAI,aAAa,aAAK,cAAc,EAAE,KAAK,CAAC;AACjE,QAAM,SAASA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC1C,QAAM,WAAWA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC7C,WAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,EAAE,GAAG;AACtC,UAAM,IAAI,OAAO,CAAC;AAClB,QAAI,KAAK,GAAG;AACV,mBAAa,CAAC,IAAI,SAAS,CAAC;WACvB;AACL,mBAAa,CAAC,IAAI,SAAS,CAAC,KAAK,IAAI;;;AAIzC,SAAOA,SAAQ,eAAe,EAAE,OAAO,WAAW,YAAY;AAChE;AAEO,IAAM,gBAA8B;EACzC,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC1Bd,IAAM,IAAI,qBAAa;AACvB,IAAM,KAAK,qBAAa;AACxB,IAAM,KAAK,qBAAa;AACxB,IAAM,KAAK,qBAAa;AACxB,IAAM,KAAK,qBAAa;AACxB,IAAM,KAAK,qBAAa;AAEjB,IAAMC,OAAM,gBACf,KACA,CAAC,OAAM;AACL,QAAMC,QAAO,KAAK,KAAK,EAAE;AACzB,QAAM,IAAI,KAAK,IAAI,EAAE;AACrB,QAAM,IAAI,KAAO,IAAM,IAAI;AAC3B,SAAOA,SACF,QACK,KAAK,IAAI,MAAM,IAAK,MAAM,IAAI,MAAM,IAAI,MAAM,IAC/C,KAAK,IAAI,CAAC,IAAI,CAAC;AAC1B,CAAC;AAGE,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACtBR,SAAUE,YAAW,MAI1B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAAC,OAAK,IAAI;AAChB,QAAM,EAAC,IAAG,IAAI;AAEd,QAAM,YAAYA,OAAM,MAAM;AAC9B,QAAM,WAAWA,OAAM,MAAM,MAAK;AAClC,MAAI,OAAO;AACX,MAAI,MAAM,GAAG;AAEX,iBAAK,OACD,EAAE,YAAY,MAAM,KACpB,MAAM,iCAAiC,EAAG,YAAY,EAAE,KACpD,SAAS,GAAG;AACpB,WAAO,YAAY,MAAM;;AAE3B,WAAS,OAAO,MAAM,GAAG,CAAC;AAE1B,SAAOC,SAAQ,EAAC,QAAQ,EAAC,GAAGD,OAAK,GAAG,SAAAD,UAAS,OAAO,EAAC,OAAO,SAAQ,EAAC,CAAC;AACxE;AAEO,IAAM,mBAAiC;EAC5C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC5BP,IAAM,cACT,6BAA6B,CAAC,GAAW,MAAc,IAAI,CAAC;AACzD,IAAMI,OAAM,iBAAiB,SAAS,WAAW;AAEjD,IAAM,gBAA8B;EACzC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACKR,SAAU,SACZC,QAAmB,SACnB,YAA0B;AAC5B,QAAM,aAAaA,OAAM;AACzB,QAAM,QAAQ,WAAW,CAAC;AAC1B,QAAM,WAAW,WAAW,CAAC;AAE7B,QAAM,YAAY,WAAW,KAAK,IAAIA,OAAM,MAAM;AAElD,QAAM,SAAS,UAAU,mBAAmB;AAC5C,QAAM,SAAS,UAAU,mBAAmB;AAG5C,QAAM,cAAc,CAAC,OAAO,QAAQ;AACpC,QAAM,aAAa,aAAK,cAAc,WAAW;AACjD,QAAM,aAAa,aAAK,uBAAuB,WAAW,UAAU;AACpE,QAAM,aAAa,aAAK,uBAAuB,WAAW,UAAU;AAEpE,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAE9B,UAAM,IAAIC,OAAM;MACd,QAAQ,EAAC,GAAG,OAAM;MAClB,SAAS;MACT,OAAO,EAAC,OAAO,CAAC,GAAG,CAAC,GAAG,MAAM,CAAC,GAAG,QAAQ,EAAC;KAC3C;AACD,UAAM,IAAIA,OAAM;MACd,QAAQ,EAAC,GAAG,OAAM;MAClB,SAAS;MACT,OAAO,EAAC,OAAO,CAAC,GAAG,CAAC,GAAG,MAAM,CAAC,GAAG,QAAQ,EAAC;KAC3C;AAED,UAAMD,SAAQE,SAAQ,EAAC,QAAQ,EAAC,MAAM,GAAG,MAAM,EAAC,GAAG,SAAS,WAAU,CAAC;AAGvE,UAAM,EAAC,MAAAC,OAAM,MAAAC,MAAI,IAAI,QAAQJ,QAAO,SAAS,UAAU;AACvD,UAAM,MAAM,qBAAa,uBAAuBG,OAAMC,KAAI;AAE1D,aAAS,IAAI,GAAG,IAAI,UAAU,KAAK;AACjC,YAAM,IAAI,qBAAa,oBAAoB,KAAK,CAAC;AACjD,iBAAW,IAAI,WAAW,CAAC,IAAI,EAAE;AACjC,iBAAW,IAAI,WAAW,CAAC,IAAI,EAAE;;AAGnC,eAAW,8BAA8B,CAAC;AAC1C,eAAW,8BAA8B,CAAC;AAC1C,eAAW,8BAA8BJ,MAAK;;AAGhD,QAAM,YACF,WAAW,eAAe,aAAa,WAAW,UAAU;AAChE,QAAM,YACF,WAAW,eAAe,aAAa,WAAW,UAAU;AAEhE,QAAM,SAASE,SACX,EAAC,QAAQ,EAAC,MAAM,WAAW,MAAM,UAAS,GAAG,SAAS,WAAU,CAAC;AAErE,aAAW,8BAA8B,SAAS;AAClD,aAAW,8BAA8B,SAAS;AAElD,SAAO;AACT;AAEM,SAAU,QACZF,QAAmB,SACnB,YAA0B;AAC5B,QAAM,YAAY,aAAK,cAAcA,OAAM,KAAK;AAEhD,QAAM,YAAY,WAAW,KAAK,IAAIA,OAAM,MAAM;AAElD,QAAM,WACF,WAAW,KAAK,IAAI,UAAU,mBAAmB,KAAK,MAAM,EAAE;AAGlE,QAAM,WACF,WAAW,KAAK,IAAI,UAAU,mBAAmB,KAAK,MAAM,EAAE;AAGlE,MAAI,cAAc,SAAS,GAAG;AAC5B,UAAM,SACF,UAAU,UAAU,UAAU,WAAW,SAAS,UAAU;AAEhE,UAAM,cAAc,CAACA,OAAM,MAAM,CAAC,GAAGA,OAAM,MAAM,CAAC,CAAC;AAEnD,QAAI,SAAS;AACX,YAAM,WACF,WAAW,eAAe,aAAa,WAAW,OAAO,IAAI;AACjE,YAAM,WACF,WAAW,eAAe,aAAa,WAAW,OAAO,IAAI;AAEjE,YAAM,WAAuB,WAAW,eACpC,CAAA,GAAI,WACJ,aAAK,kBAAkB,WAA8B,SAAS,CAAC;AACnE,YAAM,eACF,SAAS,EAAC,QAAQ,EAAC,GAAG,SAAQ,GAAG,SAAS,WAAU,CAAC;AAEzD,YAAM,cACF,cAAc,WACV,EAAC,QAAQ,EAAC,GAAG,UAAU,GAAG,SAAQ,GAAG,SAAS,WAAU,CAAC;AAEjE,YAAM,cACF,cAAc,WACV,EAAC,QAAQ,EAAC,GAAG,UAAU,GAAG,aAAY,GAAG,SAAS,WAAU,CAAC;AAGrE,YAAM,cACF,WAAW,KAAK,IAAI,YAAY,MAAM,EAAE;AAC5C,YAAM,cACF,WAAW,KAAK,IAAI,YAAY,MAAM,EAAE;AAE5C,iBAAW,8BAA8B,QAAQ;AACjD,iBAAW,8BAA8B,QAAQ;AACjD,iBAAW,8BAA8B,QAAQ;AACjD,iBAAW,8BAA8B,YAAY;AACrD,iBAAW,8BAA8B,WAAW;AACpD,iBAAW,8BAA8B,WAAW;AAEpD,aAAO,EAAC,MAAM,aAAa,MAAM,YAAW;;AAG9C,WAAO;SACF;AACL,UAAM,OAAO,qBAAa,uBAAuB,UAAU,QAAQ;AAEnE,UAAM,YACF,yBAAyB,MAAM,WAAW,OAAO;AAErD,WAAO,qBAAa,uBAAuB,SAAS;;AAExD;AAEA,SAAS,cAAc,MAAY;AACjC,UAAQ,OAAO,OAAO,OAAO;AAC/B;AAGA,SAAS,UACL,UAAwB,UAAwB,MAChD,SACA,YAA0B;AAC5B,MAAI,SAAS,GAAG;AACd,WAAO,EAAC,MAAM,UAAU,MAAM,SAAQ;;AAGxC,QAAM,OAAO,qBAAa,uBAAuB,UAAU,QAAQ;AAEnE,QAAM,OAAO,OAAO;AAEpB,QAAM,cAAc,qBAAa,qBAAqB,IAAI;AAE1D,QAAM,eAAe,YAAY;AACjC,QAAM,eAAe,YAAY;AAEjC,QAAM,YAAY,CAAC,aAAa,MAAM;AAEtC,QAAM,eACF,WAAW,eAAe,WAAW,WAAW,YAAY;AAChE,QAAM,eACF,WAAW,eAAe,WAAW,WAAW,YAAY;AAEhE,QAAM,iBAAiBE,SACnB,EAAC,QAAQ,EAAC,MAAM,cAAc,MAAM,aAAY,GAAG,SAAS,WAAU,CAAC;AAE3E,QAAM,aAAa,qBAAa,oBAAoB,IAAI;AAExD,QAAM,cAAc,WAAW;AAC/B,QAAM,cAAc,WAAW;AAE/B,QAAM,WAAW,CAAC,YAAY,MAAM;AAEpC,QAAM,cACF,WAAW,eAAe,UAAU,WAAW,WAAW;AAC9D,QAAM,cACF,WAAW,eAAe,UAAU,WAAW,WAAW;AAE9D,QAAM,gBAAgBA,SAClB,EAAC,QAAQ,EAAC,MAAM,aAAa,MAAM,YAAW,GAAG,SAAS,WAAU,CAAC;AAGzE,QAAM,eACF,UAAU,cAAc,cAAc,MAAM,SAAS,UAAU;AAEnE,QAAM,gBAAgB,aAAa;AACnC,QAAM,gBAAgB,aAAa;AAEnC,QAAM,aAAa,CAAC,cAAc,MAAM;AAExC,QAAM,gBACF,WAAW,eAAe,YAAY,WAAW,aAAa;AAClE,QAAM,gBACF,WAAW,eAAe,YAAY,WAAW,aAAa;AAElE,QAAM,kBAAkBA,SAAQ;IAC9B,QAAQ,EAAC,MAAM,eAAe,MAAM,cAAa;IACjD,SAAS;GACV;AAED,QAAM,cACF,UAAU,aAAa,aAAa,MAAM,SAAS,UAAU;AAEjE,QAAM,eAAe,YAAY;AACjC,QAAM,eAAe,YAAY;AAEjC,QAAM,YAAY,CAAC,aAAa,MAAM;AAEtC,QAAM,eACF,WAAW,eAAe,WAAW,WAAW,YAAY;AAChE,QAAM,eACF,WAAW,eAAe,WAAW,WAAW,YAAY;AAEhE,QAAM,iBAAiBA,SACnB,EAAC,QAAQ,EAAC,MAAM,cAAc,MAAM,aAAY,GAAG,SAAS,WAAU,CAAC;AAE3E,QAAM,IAAI,qBAAa,UAAU,MAAM,OAAO;AAC9C,QAAM,SAAS,CAAC,EAAE,KAAK,MAAM;AAE7B,QAAM,YAAY,WAAW,eAAe,QAAQ,WAAW,EAAE,IAAI;AACrE,QAAM,YAAY,WAAW,eAAe,QAAQ,WAAW,EAAE,IAAI;AAErE,QAAM,cAAcA,SAChB,EAAC,QAAQ,EAAC,MAAM,WAAW,MAAM,UAAS,GAAG,SAAS,WAAU,CAAC;AAErE,QAAM,eACF,SACI,EAAC,QAAQ,EAAC,GAAG,aAAa,GAAG,eAAc,GAAG,SAAS,WAAU,CAAC;AAG1E,QAAM,UAAUG,KAAI;IACF,QAAQ,EAAC,GAAG,iBAAiB,GAAG,aAAY;IAC5C,SAAS;GACV;AACjB,QAAM,UAAUC,KAAI;IACF,QAAQ,EAAC,GAAG,iBAAiB,GAAG,aAAY;IAC5C,SAAS;GACV;AAEjB,QAAM,cAAcH,MAAK,EAAC,QAAQ,EAAC,OAAO,QAAO,GAAG,SAAS,WAAU,CAAC;AACxE,QAAM,cAAcA,MAAK,EAAC,QAAQ,EAAC,OAAO,QAAO,GAAG,SAAS,WAAU,CAAC;AAExE,QAAM,cAAcC,MAAK,EAAC,QAAQ,EAAC,OAAO,QAAO,GAAG,SAAS,WAAU,CAAC;AACxE,QAAM,cAAcA,MAAK,EAAC,QAAQ,EAAC,OAAO,QAAO,GAAG,SAAS,WAAU,CAAC;AAExE,QAAM,QAAQG,QAAO;IACnB,QAAQ,CAAC,aAAuB,WAAqB;IACrD,SAAS;IACT,OAAO,EAAC,MAAM,EAAC;GAChB;AACD,QAAM,QAAQA,QAAO;IACnB,QAAQ,CAAC,aAAuB,WAAqB;IACrD,SAAS;IACT,OAAO,EAAC,MAAM,EAAC;GAChB;AAED,QAAM,YAAY,WAAW,KAAK,IAAI,MAAM,MAAM,EAAE;AACpD,QAAM,YAAY,WAAW,KAAK,IAAI,MAAM,MAAM,EAAE;AAEpD,aAAW,8BAA8B,YAAY;AACrD,aAAW,8BAA8B,YAAY;AACrD,aAAW,8BAA8B,cAAc;AACvD,aAAW,8BAA8B,WAAW;AACpD,aAAW,8BAA8B,WAAW;AACpD,aAAW,8BAA8B,aAAa;AACtD,aAAW,8BAA8B,aAAa;AACtD,aAAW,8BAA8B,aAAa;AACtD,aAAW,8BAA8B,eAAe;AACxD,aAAW,8BAA8B,YAAY;AACrD,aAAW,8BAA8B,YAAY;AACrD,aAAW,8BAA8B,cAAc;AACvD,aAAW,8BAA8B,SAAS;AAClD,aAAW,8BAA8B,SAAS;AAClD,aAAW,8BAA8B,WAAW;AACpD,aAAW,8BAA8B,YAAY;AACrD,aAAW,8BAA8B,OAAO;AAChD,aAAW,8BAA8B,OAAO;AAChD,aAAW,8BAA8B,WAAW;AACpD,aAAW,8BAA8B,WAAW;AACpD,aAAW,8BAA8B,WAAW;AACpD,aAAW,8BAA8B,WAAW;AACpD,aAAW,8BAA8B,KAAK;AAC9C,aAAW,8BAA8B,KAAK;AAE9C,SAAO,EAAC,MAAM,WAAW,MAAM,UAAS;AAC1C;AAGA,SAAS,yBACL,MAAkB,MAAc,SAAgB;AAClD,QAAM,MAAM,IAAI,aAAa,OAAO,CAAC;AAErC,WAAS,IAAI,GAAG,IAAI,MAAM,KAAK;AAC7B,QAAIJ,QAAO;AACX,QAAIC,QAAO;AACX,aAAS,IAAI,GAAG,IAAI,MAAM,KAAK;AAC7B,YAAM,IAAI,qBAAa,SAAS,IAAI,GAAG,MAAM,OAAO;AACpD,YAAM,OAAO,qBAAa,oBAAoB,MAAsB,CAAC;AACrE,MAAAD,SAAQ,KAAK,OAAO,EAAE,OAAO,KAAK,OAAO,EAAE;AAC3C,MAAAC,SAAQ,KAAK,OAAO,EAAE,OAAO,KAAK,OAAO,EAAE;;AAE7C,QAAI,SAAS;AACX,MAAAD,SAAQ;AACR,MAAAC,SAAQ;;AAEV,yBAAa,mBAAmB,KAAKD,OAAMC,OAAM,CAAC;;AAEpD,SAAO;AACT;;;AC3TM,SAAUI,KAAI,MAAkD;AAEpE,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,OAAAC,OAAK,IAAI;AAEhB,QAAM,YAAY,aAAK,cAAcA,OAAM,KAAK;AAGhD,QAAM,qBAAqBA,OAAM,MAAMA,OAAM,MAAM,SAAS,CAAC;AAC7D,QAAM,QAAQ,YAAY;AAE1B,QAAM,UAAUC,SAAQ;IACtB,QAAQ,EAAC,GAAGD,OAAK;IACjB,SAAAD;IACA,OAAO,EAAC,OAAO,CAAC,OAAO,kBAAkB,EAAC;GAC3C;AAED,QAAM,SAAS,SAAS,SAAS,OAAOA,QAAO;AAE/C,QAAM,iBACFE,SAAQ,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAOC,OAAM,MAAK,EAAC,CAAC;AAEvE,EAAAD,SAAQ,8BAA8B,OAAO;AAC7C,EAAAA,SAAQ,8BAA8B,MAAM;AAE5C,SAAO;AACT;AAEO,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACjCR,SAAUI,MAAK,MAAiD;AAEpE,QAAM,EAAC,SAAAC,UAAS,MAAK,IAAI;AACzB,QAAM,EAAC,OAAO,OAAO,MAAK,IAAI;AAE9B,QAAM,SAAS,SAAS,aAAK,WAAW,KAAK;AAC7C,QAAM,SAAS,aAAK,kBAAkB,QAAQ,aAAK,cAAc,KAAK,CAAC;AACvE,aAAW,QAAQ,OAAO,MAAM;AAEhC,SAAOA,SAAQ,eAAe,OAAO,QAAQ,MAAM;AACrD;AAEO,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYD;;AAGd,SAAS,WACL,QAAoB,OAAsB,OAAe;AAC3D,MAAI,UAAU,UAAU;AACrB,WAAoB,KAAK,KAAe;SACpC;AACJ,WAAsB,KAAK,KAAe;;AAE/C;;;ACxBO,IAAM,sBAAoC;EAC/C,YAAY;EACZ,aAAa;EACb,YAAY,CAAC,EAAC,QAAQ,OAAO,SAAAE,SAAO,MAAK;AACvC,UAAM,EAAC,OAAAC,OAAK,IAAI;AAChB,UAAM,aAAaD;AAEnB,UAAM,SAAS,aAAK,uBAChBC,OAAM,OAA0B,aAAK,cAAcA,OAAM,KAAK,CAAC;AACnE,UAAM,CAAC,OAAO,aAAa,YAAY,WAAW,IAAIA,OAAM;AAE5D,UAAM,YAAY,WAAW,KAAK,IAAIA,OAAM,MAAM,EAAE;AAEpD,aAAS,WAAW,GAAG,WAAW,OAAO,YAAY;AACnD,YAAM,cAAc,WAAW,aAAa,cAAc;AAE1D,eAAS,MAAM,GAAG,MAAM,aAAa,OAAO;AAC1C,cAAM,YAAY,OAAO,aAAa;AAEtC,iBAAS,MAAM,GAAG,MAAM,YAAY,OAAO;AACzC,gBAAM,YAAY,MAAM;AAExB,mBAAS,UAAU,GAAG,UAAU,aAAa,WAAW;AACtD,kBAAM,SAAS,KAAK,MAAM,aAAa,MAAM,CAAC;AAC9C,kBAAM,SAAS,cAAc,YAAY,YAAY;AAErD,gBAAI,cAAc,UAAU,MAAM;AAElC,gBAAI,UAAU,KAAK,SAAS,YAAY;AAEtC,oBAAM,mBAAmB,SAAS;AAClC,oBAAM,WACF,cAAc,YAAY,mBAAmB;AACjD,4BAAc,UAAU,QAAQ;;AAElC,mBAAO,MAAM,IAAI;;;;;AAMzB,UAAM,SAAS,WAAW,MAAM,QAAQA,OAAM,OAAOA,OAAM,KAAK;AAChE,WAAO,EAAC,QAAQ,OAAOA,OAAM,OAAO,OAAOA,OAAM,MAAK;EACxD;;;;AC3CK,IAAM,eACT,6BAA6B,CAAC,GAAW,MAAc,KAAK,MAAM,IAAI,CAAC,CAAC;AACrE,IAAMC,YACT,iBAAiB,UAAU,cAAc,MAAwB,OAAO;AAErE,IAAM,iBAA+B;EAC1C,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLR,SAAU,YAAY,MAI3B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,QAAQ,MAAM,uBAAsB,IAAI;AAClD,QAAM,EACJ,SACA,KAAAC,MACA,YACA,WACA,iBACA,YACA,eAAc,IACZ;AAEJ,MAAI,SAAS,OAAO;IAClB,QAAQ,EAAC,GAAG,OAAM;IAClB,SAAAD;IACA,OAAO,EAAC,SAAS,KAAAC,MAAK,YAAY,WAAW,gBAAe;GAC7D;AAED,MAAI,MAAM;AACR,UAAM,YAAY;AAKlB,QAAI,eAAe,UAAU,KAAK,MAAM,WAAW,KAC/C,KAAK,MAAM,CAAC,MAAM,GAAG;AACvB,YAAM,eAAeC,SACjB,EAAC,QAAQ,EAAC,GAAG,KAAI,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,CAAC,KAAK,MAAM,CAAC,GAAG,GAAG,CAAC,EAAC,EAAC,CAAC;AACvE,eACIG,KAAI,EAAC,QAAQ,EAAC,GAAG,QAAQ,GAAG,aAAY,GAAG,SAAAH,SAAO,CAAC;AACvD,MAAAA,SAAQ,8BAA8B,YAAY;WAC7C;AAGL,eAASG,KAAI,EAAC,QAAQ,EAAC,GAAG,QAAQ,GAAG,KAAI,GAAG,SAAAH,SAAO,CAAC;;AAEtD,IAAAA,SAAQ,8BAA8B,SAAS;;AAGjD,MAAI,YAAY;AACd,UAAM,YAAY;AAKlB,QAAI,eAAe,UAAU,eAAe,WACxC,uBAAuB,MAAM,WAAW,KACxC,uBAAuB,MAAM,CAAC,MAAM,GAAG;AACzC,YAAM,gBAAgBE,SAAQ;QAC5B,QAAQ,EAAC,GAAG,uBAAsB;QAClC,SAAAF;QACA,OAAO,EAAC,OAAO,CAAC,uBAAuB,MAAM,CAAC,GAAG,GAAG,CAAC,EAAC;OACvD;AACD,eAAS,gBACLA,UAAS,QAAQ,YAAY,eAAe,cAAc;AAC9D,MAAAA,SAAQ,8BAA8B,aAAa;WAC9C;AACL,eAAS,gBACLA,UAAS,QAAQ,YAAY,wBAAwB,cAAc;;AAEzE,IAAAA,SAAQ,8BAA8B,SAAS;;AAGjD,SAAO;AACT;AAEO,IAAM,oBAAkC;EAC7C,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC3ER,SAAU,qBAAqB,MAIpC;AACC,QAAM,EAAC,QAAQ,SAAAI,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,QAAQ,MAAM,uBAAsB,IAAI;AAClD,QAAM,EACJ,SACA,KAAAC,MACA,YACA,WACA,iBACA,YACA,eAAc,IACZ;AAEJ,MAAI,SAAS,sBAAsB;IACjC,QAAQ,EAAC,GAAG,OAAM;IAClB,SAAAD;IACA,OAAO,EAAC,SAAS,KAAAC,MAAK,YAAY,WAAW,gBAAe;GAC7D;AAED,MAAI,MAAM;AACR,UAAM,YAAY;AAClB,aAASC,KAAI,EAAC,QAAQ,EAAC,GAAG,QAAQ,GAAG,KAAI,GAAG,SAAAF,SAAO,CAAC;AACpD,IAAAA,SAAQ,8BAA8B,SAAS;;AAEjD,MAAI,YAAY;AACd,UAAM,YAAY;AAClB,aAAS,gBACLA,UAAS,QAAQ,YAAY,wBAAwB,cAAc;AACvE,IAAAA,SAAQ,8BAA8B,SAAS;;AAGjD,SAAO;AACT;AAEO,IAAM,6BAA2C;EACtD,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC1CR,SAAU,SACZ,MAAuD;AACzD,QAAM,EAAC,QAAQ,SAAAG,SAAO,IAAI;AAC1B,QAAM,EAAC,QAAQ,QAAO,IAAI;AAE1B,QAAM,aAAa,aAAK,cAAc,OAAO,KAAK;AAElD,QAAM,eAAe,QAAQ;AAC7B,QAAM,YAAY,aAAa,aAAa,SAAS,CAAC;AAEtD,QAAM,CAAC,aAAa,WAAW,WAAW,OAAO,IAC7C,qBAAa,mBAAmB,QAAQ,OAAO;AACnD,MAAI,cAAc,GAAG;AACnB,WAAOA,SAAQ,eAAe,aAAa,OAAO,OAAO,CAAA,CAAE;;AAG7D,QAAM,cAAcA,SAAQ,KAAK,IAAI,QAAQ,MAAM,EAAE;AACrD,QAAM,YAAYA,SAAQ,WAA4B,MAAM;AAC5D,QAAM,SAAS,aACX,aAAa,WAAW,OAAO,OAAO,WAAW,WAAW,WAC5D,SAAS,OAAO,OAAO,UAAU;AAErC,SAAOA,SAAQ,eAAe,aAAa,OAAO,OAAO,OAAO,MAAM;AACxE;AAEO,IAAM,iBAA+B;EAC1C,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC3BR,SAAU,SAAS,MAIxB;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,QAAO,IAAI;AACrB,QAAM,EAAC,MAAM,UAAS,IAAI;AAE1B,mBAAiB,CAAC,GAAG,OAAO,GAAG,UAAU;AAGzC,QAAM,aAAa,aAAK,eAAe,MAAM,EAAE,KAAK,EAAE,CAAC;AACvD,QAAM,cAAcA,SAAQ,KAAK,IAAI,QAAQ,MAAM,EAAE;AACrD,QAAM,UAAU,EAAE,MAAM,UAAU;AAClC,WAAS,IAAI,GAAG,IAAI,YAAY,QAAQ,EAAE,GAAG;AAC3C,UAAM,QAAQ,YAAY,CAAC;AAC3B,iBAAK,OACD,SAAS,UAAU,KAAK,SAAS,GACjC,MACI,6BAA6B,KAAK,kBAAkB,UAAU,CAAC,GAAG;;AAG5E,MAAI,aAAa;AAEjB,MAAI,aAAa,MAAM;AACrB,iBAAa;;AAGf,QAAM,cAAc,aAAK,cAAc,QAAQ,KAAK;AAEpD,QAAM,YAAY,qBAAa,aAAa,yBACxC,GAAG,SAAS,YAAY,UAAU;AAEtC,QAAM,WAAWC,SAAQ;IACvB,QAAQ,EAAC,EAAC;IACV,SAAAD;IACA,OAAO;MACL,OAAO;QACL,UAAU;QAAW,UAAU;QAAW,UAAU;QACpD,UAAU;;;GAGf;AAED,QAAM,eAAeC,SAAQ;IAC3B,QAAQ,EAAC,GAAG,QAAO;IACnB,SAAAD;IACA,OAAO,EAAC,OAAO,CAAC,UAAU,WAAW,cAAc,UAAU,SAAS,EAAC;GACxE;AAED,QAAM,qBAAqB;IACzB,UAAU;IAAW,UAAU;IAAW,cAAc,UAAU;IAClE,UAAU;;AAGZ,QAAM,aAAaA,SAAQ,WAAW,YAAY;AAClD,QAAM,OAAOA,SAAQ,WAAW,QAAQ;AACxC,QAAM,SAAS,aAAa,MAAM,YAAY,kBAAkB;AAEhE,EAAAA,SAAQ,8BAA8B,QAAQ;AAC9C,EAAAA,SAAQ,8BAA8B,YAAY;AAElD,SAAOA,SAAQ,eACX,UAAU,aAAa,OAAO,OAAO,OAAO,MAAM;AACxD;AAEO,IAAM,iBAA+B;EAC1C,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACvER,SAAUE,MAAK,MAAmD;AAEtE,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,OAAAC,OAAK,IAAI;AAEhB,QAAM,YAAY,aAAK,cAAcA,OAAM,KAAK;AAGhD,QAAM,qBAAqBA,OAAM,MAAMA,OAAM,MAAM,SAAS,CAAC;AAC7D,QAAM,QAAQ,YAAY;AAE1B,QAAM,UAAUC,SAAQ;IACtB,QAAQ,EAAC,GAAGD,OAAK;IACjB,SAAAD;IACA,OAAO,EAAC,OAAO,CAAC,OAAO,kBAAkB,EAAC;GAC3C;AAED,QAAM,SAAS,SAAS,SAAS,MAAMA,QAAO;AAE9C,QAAM,iBACFE,SAAQ,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAOC,OAAM,MAAK,EAAC,CAAC;AAEvE,EAAAD,SAAQ,8BAA8B,OAAO;AAC7C,EAAAA,SAAQ,8BAA8B,MAAM;AAE5C,SAAO;AACT;AAEO,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACjCP,IAAMI,YACT,gBAAgB,UAAU,CAAC,OAAO,OAAO,SAAS,EAAE,IAAI,IAAI,GAAG,MAAM;AAElE,IAAM,iBAA+B;EAC1C,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACNP,IAAMC,SACT,gBAAgB,OAAO,CAAC,OAAO,KAAK,IAAI,EAAE,MAAM,WAAW,IAAI,GAAG,MAAM;AAErE,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACNP,IAAMC,SACT,gBAAgB,OAAO,CAAC,OAAO,OAAO,MAAM,EAAE,IAAI,IAAI,GAAG,MAAM;AAE5D,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLR,SAAU,SAAS,MAAqD;AAE5E,QAAM,EAAC,SAAAC,UAAS,MAAK,IAAI;AACzB,QAAM,EAAC,OAAO,MAAM,IAAG,IAAI;AAE3B,QAAM,UAAU,aAAa,OAAO,MAAM,GAAG;AAE7C,SAAOA,SAAQ,eAAe,CAAC,QAAQ,MAAM,GAAG,WAAW,OAAO;AACpE;AAEO,IAAM,iBAA+B;EAC1C,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACdP,IAAMC,SAAQ,gBAAgB,OAAO,CAAC,OAAO,KAAK,MAAM,EAAE,CAAC;AAE3D,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACJP,IAAM,iBACT,6BAA6B,CAAC,GAAW,MAAc,KAAK,CAAC;AAC1D,IAAMC,cAAa,iBACtB,YAAY,gBAAgB,MAAwB,MAAM;AAEvD,IAAM,mBAAiC;EAC5C,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACTP,IAAMC,cACT,gBAAgB,YAAY,CAAC,OAAO,KAAK,IAAI,GAAG,MAAM;AAEnD,IAAM,mBAAiC;EAC5C,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLP,IAAM,gBACT,6BAA6B,CAAC,GAAW,MAAc,KAAK,CAAC;AAC1D,IAAMC,aACT,iBAAiB,WAAW,eAAe,MAAwB,MAAM;AAEtE,IAAM,kBAAgC;EAC3C,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACRR,SAAU,IACZ,MAAmE;AAErE,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,aAAa,MAAM,OAAO,KAAI,IAAI;AAEzC,mBAAiB,GAAG,KAAK;AAEzB,QAAM,WAAW,EAAE,MAAM,CAAC;AAC1B,QAAM,OAAO,WAAW;AACxB,QAAM,UAAUA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,OAAO,aAAK,cAAc,EAAE,KAAK;AACvC,QAAM,SAAS,IAAI,aAAa,IAAI;AAEpC,WAAS,kBAAkB,QAAc;AACvC,UAAM,iBAAiB,SAAS;AAChC,QAAI,iBACA,SAAS,iBAAiB,KAAK,IAAI,GAAG,iBAAiB,WAAW;AACtE,UAAM,eACF,SAAS,iBAAiB,KAAK,IAAI,iBAAiB,aAAa,IAAI;AAEzE,QAAIC,OAAM;AACV,WAAO,kBAAkB,cAAc,kBAAkB;AACvD,YAAM,IAAI,QAAQ,cAAc;AAChC,MAAAA,QAAO,IAAI;;AAEb,WAAOA;EACT;AAEA,WAAS,SAAS,GAAG,SAAS,MAAM,UAAU;AAC5C,UAAMA,OAAM,kBAAkB,MAAM;AACpC,UAAM,MAAM,QAAQ,MAAM,IAAI,KAAK,IAAI,OAAO,QAAQA,MAAK,CAAC,IAAI;AAChE,WAAO,MAAM,IAAI;;AAGnB,SAAOD,SAAQ,eAAe,EAAE,OAAO,EAAE,OAAO,MAAM;AACxD;AAGO,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC3CR,SAAU,QACZ,MACyE;AAE3E,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,GAAG,GAAE,IAAI;AACnB,QAAM,EAAC,aAAa,MAAM,OAAO,KAAI,IAAI;AAEzC,mBAAiB,IAAI,SAAS;AAE9B,QAAM,SAAS,aAAK,cAAc,GAAG,KAAK;AAE1C,QAAM,WAAW,GAAG,MAAM,CAAC;AAC3B,QAAM,WAAWA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC7C,QAAM,UAAUA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,UAAUA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,SAAS,IAAI,aAAa,MAAM;AACtC,QAAM,OAAO;AAEb,WAAS,SAAS,GAAG,SAAS,MAAM,UAAU;AAC5C,UAAM,iBAAiB,SAAS;AAChC,UAAM,aACD,SAAS,iBAAkB,KAAK,IAAI,GAAG,iBAAiB,WAAW;AACxE,UAAM,WAAY,SAAS,iBACvB,KAAK,IAAI,UAAU,iBAAiB,cAAc,CAAC;AAEvD,QAAIC,QAAO;AACX,aAAS,IAAI,YAAY,IAAI,UAAU,KAAK;AAC1C,MAAAA,SAAQ,KAAK,IAAI,QAAQ,CAAC,GAAG,CAAC;;AAEhC,IAAAA,QAAO,QAAQA,QAAO;AAEtB,aAAS,IAAI,YAAY,IAAI,UAAU,KAAK;AAC1C,UAAI,MAAM,KAAK,QAAQ,OAAO,QAAQ,CAAC,IAAI,QAAQ,MAAM,IAAIA;AAC7D,UAAI,WAAW,GAAG;AAChB,eAAO,KAAK,IAAIA,OAAM,CAAC,IAAI;;AAE7B,aAAO,SAAS,MAAM;AACtB,aAAO,CAAC,KAAK;;;AAIjB,SAAOD,SAAQ,eAAe,GAAG,OAAO,EAAE,OAAO,MAAM;AACzD;AAGO,IAAM,gBAA8B;EACzC,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC5CR,SAAUE,KACZ,MAAmE;AAErE,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,kBAAkB,SAAQ,IAAI;AACrC,QAAM,aAAaA;AACnB,MAAI,SAAS,EAAE;AACf,QAAM,QAAQ,OAAO;AAErB,QAAM,WAAW,aAAK,eAAe,kBAAkB,MAAM;AAC7D,MAAI,OAAO;AACX,QAAM,eAAe,qBAAa,mBAAmB,MAAM,KAAK;AAChE,MAAI,QAAQ,WAAW,KAAK,IAAI,EAAE,MAAM,EAAE;AAC1C,MAAI,gBAAgB,MAAM;AACxB,UAAM,WAAqB,IAAI,MAAM,KAAK;AAC1C,aAAS,IAAI,GAAG,IAAI,SAAS,QAAQ,KAAK;AACxC,eAAS,CAAC,IAAI,OAAO,aAAa,CAAC,CAAC;;AAGtC,YAAQ,cAAc,OAAO,QAAQ,EAAE,OAAO,cAAc,QAAQ;AACpE,WAAO,qBAAa,iBAAiB,KAAK,QAAQ,KAAK;AAEvD,aAAS;;AAGX,mBAAiB,GAAG,KAAK;AACzB,uBAAa,2BAA2B,OAAO,MAAM,KAAK;AAC1D,QAAM,CAAC,aAAa,WAAW,IAC3B,qBAAa,0BAA0B,QAAQ,IAAI;AAEvD,QAAM,aAAa,aAAK,cAAc,WAAW;AAEjD,QAAM,SAAS,QAAQ,OAAO,YAAY,aAAa,EAAE,KAAK;AAC9D,QAAM,SAAS,WAAW,MAAM,QAAQ,aAAa,EAAE,KAAK;AAE5D,MAAI,WAAW;AACf,MAAI,UAAU;AAEZ,UAAM,WAAW,qBAAa,qBAAqB,aAAa,QAAQ;AACxE,eAAW;;AAGb,SAAO,EAAC,QAAQ,OAAO,UAAU,OAAO,EAAE,MAAK;AACjD;AAEO,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACrDR,SAAUE,SACZ,MACyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,mBAAiB,GAAG,SAAS;AAC7B,QAAM,EAAC,YAAY,SAAS,KAAAC,MAAK,gBAAe,IAAI;AACpD,QAAM,YAAY;AAElB,eAAK,OACD,qBAAa,+BAA+B,SAAS,SAAS,GAC9D,MAAM,wEACa,OAAO,mBAAmB,SAAS,GAAG;AAE7D,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OAA2C,YAAY,SACzD,WAAWA,MAAK,eAAe;AACnC,MAAI;AAEJ,MAAI,SAAS,gBAAgB,KAAK,SAAS,iBAAiB,KACxD,aAAK,YAAY,SAAS,SAAS,SAAS,QAAQ,GAAG;AACzD,UAAM,SAAS,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,SAAO,CAAC;SAChC;AACL,UAAM,UAAUA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,UAAME,WAAU,aAAK,eAAe,EAAE,KAAK;AAC3C,UAAMC,UAASC,MAAK,SAAS,EAAE,OAAO,EAAE,OAAOF,UAAS,UAAU,KAAK;AACvE,UAAMF,SAAQ,eACV,SAAS,UAAU,EAAE,OAAOG,QAAO,MAAoB;;AAE7D,SAAO;AACT;AAEO,IAAM,gBAA8B;EACzC,YAAY;EACZ,aAAa;EACb,YAAYJ;;;;ACpCR,SAAU,UAAU,MAIzB;AACC,QAAM,EAAC,QAAQ,SAAAM,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,YAAY,SAAS,KAAAC,MAAK,iBAAiB,WAAU,IAAI;AAEhE,mBAAiB,GAAG,WAAW;AAE/B,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OAAmD,YAAY,SACjE,GAAmBA,MAAK,iBAAiB,UAAU;AAEvD,QAAM,UAAUD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,SAAS,OACX,SAAS,EAAE,OAAO,EAAE,OAAO,aAAK,eAAe,EAAE,KAAK,GAAG,UAAU,KAAK;AAE5E,SAAOA,SAAQ,eAAe,OAAO,OAAO,WAAW,OAAO,MAAM;AACtE;AAEO,IAAM,kBAAgC;EAC3C,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACzBR,SAAU,cAAc,MAI7B;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,IAAI,OAAAC,OAAK,IAAI;AACpB,QAAM,EAAC,YAAY,SAAS,KAAAC,MAAK,gBAAe,IAAI;AAEpD,mBAAiB,CAAC,IAAID,MAAK,GAAG,eAAe;AAE7C,QAAM,WAAW,qBAAa,kBAC1BA,OAAM,OAAmD,YACzD,SAAS,GAAmBC,MAAK,eAAe;AAEpD,QAAM,WAAWF,SAAQ,WAAWC,MAAK;AACzC,QAAM,YAAY,mBAAmB,UAAU,QAAQ;AACvD,QAAM,cAAc,SAAS;AAC7B,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,gBAAgB,SAAS;AAC/B,QAAM,iBAAiB,SAAS;AAChC,QAAM,gBAAgB,SAAS;AAC/B,QAAM,uBAAuB,SAAS;AACtC,QAAM,wBAAwB,SAAS;AACvC,QAAM,uBAAuB,SAAS;AACtC,QAAM,WAAW,uBAAuB,IAAI,SAAS,QAAQ;AAC7D,QAAM,UAAU,uBAAuB,IAAI,SAAS,QAAQ;AAC5D,QAAM,SAAS,wBAAwB,IAAI,SAAS,QAAQ;AAC5D,QAAM,KAAK,OAAOA,OAAM,OAAO,SAAS;AAExC,QAAM,QAAQD,SAAQ,WAA4B,EAAE;AAEpD,WAAS,QAAQ,GAAG,QAAQ,SAAS,WAAW,EAAE,OAAO;AACvD,aAAS,UAAU,GAAG,UAAU,SAAS,YAAY,EAAE,SAAS;AAC9D,eAAS,UAAU,GAAG,UAAU,SAAS,SAAS,EAAE,SAAS;AAC3D,iBAAS,QAAQ,GAAG,QAAQ,SAAS,UAAU,EAAE,OAAO;AACtD,mBAAS,QAAQ,GAAG,QAAQ,SAAS,SAAS,EAAE,OAAO;AAErD,kBAAM,gBAAgB,UAAU;AAChC,kBAAM,cAAc,QAAQ;AAC5B,kBAAM,cAAc,QAAQ;AAC5B,gBAAI,UAAU;AACd,qBAAS,SAAS,GAAG,SAAS,sBACzB,UAAU,eAAe;AAC5B,oBAAM,WAAW,gBAAgB,UAAU;AAC3C,kBAAI,UAAU,KAAK,WAAW,SAAS,YACnC,KAAK,MAAM,OAAO,MAAM,SAAS;AACnC;;AAEF,uBAAS,OAAO,GAAG,OAAO,uBACrB,QAAQ,gBAAgB;AAC3B,sBAAM,SAAS,cAAc,QAAQ;AACrC,oBAAI,QAAQ,KAAK,SAAS,SAAS,aAC/B,KAAK,MAAM,KAAK,MAAM,OAAO;AAC/B;;AAEF,yBAAS,OAAO,GAAG,OAAO,sBACrB,QAAQ,eAAe;AAC1B,wBAAM,SAAS,cAAc,QAAQ;AACrC,sBAAI,QAAQ,KAAK,SAAS,SAAS,YAC/B,KAAK,MAAM,KAAK,MAAM,OAAO;AAC/B;;AAGF,wBAAM,SAAS,uBAAuB,wBAC9B,uBACJ,IACC,UAAU,IAAI,OAAO,SAAS,OAAO,OAAO,OAAO;AAExD,wBAAM,SACF,SAAS,wBAAwB,uBACjC,OAAO,uBAAuB;AAElC,wBAAM,OAAO,WAAW,SAAS,IAAI;AACrC,sBAAI,SAAS,GAAG;AACd;;AAGF,wBAAM,QACF,MAAM,IAAI,OAAO,SAAS,OAAO,OAAO,OAAO;AACnD,6BAAW,QAAQ;;;;AAIzB,eAAG,IAAI,SAAS,OAAO,SAAS,OAAO,OAAO,OAAO;;;;;;AAO/D,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,sBAAoC;EAC/C,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACnGR,SAAU,YAAY,MAI3B;AACC,QAAM,EAAC,QAAQ,SAAAG,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,IAAI,OAAAC,QAAO,OAAM,IAAI;AAC5B,QAAM,IAAIA;AACV,mBAAiB,CAACA,QAAO,MAAM,GAAG,aAAa;AAC/C,QAAM,EAAC,YAAY,SAAS,KAAAC,MAAK,gBAAe,IAAI;AAEpD,QAAM,WAAW,qBAAa,kBAC1B,EAAE,OAA2C,YAAY,SACzD,GAAmBA,MAAK,eAAe;AAC3C,QAAM,UAAUF,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,YAAY,OACd,SAAS,UAAU,EAAE,OACrB,iBAAiB,SAAS,EAAE,OAAO,EAAE,OAAO,QAAQ,EAAE,MAAM;AAChE,QAAM,eAAe,SAAS;AAC9B,QAAM,cAAc,SAAS;AAC7B,QAAM,iBAAiB,SAAS;AAChC,QAAM,gBAAgB,SAAS;AAC/B,QAAM,wBAAwB,SAAS;AACvC,QAAM,uBAAuB,SAAS;AACtC,QAAM,UAAU,uBAAuB,IAAI,SAAS,QAAQ;AAC5D,QAAM,SAAS,wBAAwB,IAAI,SAAS,QAAQ;AAC5D,QAAM,KACF,OAAgB,EAAE,OAA2C,SAAS;AAE1E,QAAM,SAASA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC3C,QAAM,QAAQ,OACV,GAAG,OAA2C,WAAW,MAAM;AAEnE,WAAS,IAAI,GAAG,IAAI,SAAS,WAAW,EAAE,GAAG;AAC3C,aAAS,IAAI,GAAG,IAAI,SAAS,YAAY,EAAE,GAAG;AAC5C,eAAS,MAAM,GAAG,MAAM,SAAS,UAAU,EAAE,KAAK;AAChD,iBAAS,MAAM,GAAG,MAAM,SAAS,SAAS,EAAE,KAAK;AAE/C,gBAAM,YAAY,MAAM;AACxB,gBAAM,YAAY,MAAM;AACxB,cAAI,UAAU;AACd,mBAAS,KAAK,GAAG,KAAK,uBAAuB,MAAM,gBAAgB;AACjE,kBAAM,OAAO,YAAY,MAAM;AAC/B,gBAAI,MAAM,KAAK,OAAO,SAAS,aAC3B,KAAK,MAAM,GAAG,MAAM,KAAK;AAC3B;;AAEF,qBAAS,KAAK,GAAG,KAAK,sBAAsB,MAAM,eAAe;AAC/D,oBAAM,OAAO,YAAY,MAAM;AAC/B,kBAAI,MAAM,KAAK,OAAO,SAAS,YAC3B,KAAK,MAAM,GAAG,MAAM,KAAK;AAC3B;;AAEF,oBAAM,SAAS,wBAAwB,uBAAuB,IACzD,UAAU,IAAI,GAAG,KAAK,KAAK,CAAC;AACjC,oBAAM,SAAS,KAAK,uBAAuB;AAE3C,oBAAM,OAAO,WAAW,SAAS,IAAI;AACrC,kBAAI,SAAS,GAAG;AACd;;AAGF,oBAAM,QAAQ,MAAM,IAAI,GAAG,KAAK,KAAK,CAAC;AACtC,yBAAW,QAAQ;;;AAGvB,aAAG,IAAI,SAAS,GAAG,KAAK,KAAK,CAAC;;;;;AAKtC,SAAOA,SAAQ,eAAe,GAAG,OAAO,GAAG,OAAO,GAAG,MAAM;AAC7D;AAEO,IAAM,oBAAkC;EAC7C,YAAY;EACZ,aAAa;EACb,YAAY;;;;AChFR,SAAU,sBACZ,SAAqB,QAAkB,OACvC,qBAA8B,UAAiC;AACjE,QAAM,UAAU,aAAK,eAAe,MAAM;AAC1C,QAAM,WAAWG,MAAK,SAAS,QAAQ,OAAO,SAAS,UAAU,KAAK;AACtE,QAAM,eAAe,iBACjB,SAAS,QAAQ,OAAO,UAAU,MAAM,mBAAmB;AAE/D,SAAO,CAAC,SAAS,QAAQ,aAAa,MAAM;AAC9C;;;ACJO,IAAM,0BAAwC;EACnD,YAAY;EACZ,aAAa;EACb,YAAY,CAAC,EAAC,QAAQ,OAAO,SAAAC,SAAO,MAAK;AACvC,UAAM,EAAC,EAAC,IAAI;AACZ,UAAM,EAAC,YAAY,SAAS,KAAAC,MAAK,oBAAmB,IAChD;AACJ,UAAM,aAAaD;AACnB,qBAAiB,GAAG,mBAAmB;AAEvC,UAAM,SAAS,WAAW,KAAK,IAAI,EAAE,MAAM,EAAE;AAC7C,UAAM,WAAW,qBAAa,kBAC1B,EAAE,OAA2C,YAAY,SACzD,CAAC,GAAG,CAAC,GAAGC,IAAG;AACf,UAAM,CAAC,QAAQ,OAAO,IAAI,sBACtB,QAAQ,EAAE,OAAO,EAAE,OAAO,qBAAqB,QAAQ;AAE3D,UAAM,eACF,WAAW,MAAM,QAAwB,SAAS,UAAU,EAAE,KAAK;AACvE,UAAM,gBACF,WAAW,MAAM,SAAuB,SAAS,UAAU,EAAE,KAAK;AACtE,WAAO;MACL,EAAC,QAAQ,cAAc,OAAO,SAAS,UAAU,OAAO,EAAE,MAAK;MAC/D,EAAC,QAAQ,eAAe,OAAO,SAAS,UAAU,OAAO,QAAO;;EAEpE;;;;ACzBI,SAAUC,MACZ,MAAqE;AAEvE,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,MAAM,SAAQ,IAAI;AAEzB,QAAM,OAAO,aAAK,eAAe,MAAM,EAAE,KAAK;AAC9C,QAAM,SAAS,qBAAa,0BAA0B,EAAE,OAAO,IAAI;AACnE,QAAM,cAAc,OAAO,CAAC;AAC5B,QAAM,aAAa,aAAK,cAAc,WAAW;AACjD,QAAM,YAAY,CAAA;AAClB,QAAM,mBACFA,SAAQ,eAAe,CAAA,GAAI,WAAW,IAAI,aAAa,CAAC,UAAU,CAAC,CAAC;AACxE,YAAU,KAAK,gBAAgB;AAE/B,QAAM,KAAKC,MAAK,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,OAAO,UAAS,EAAC,CAAC;AACjE,YAAU,KAAK,EAAE;AAEjB,QAAM,MACFE,KAAI,EAAC,QAAQ,EAAC,GAAG,IAAI,GAAG,iBAAgB,GAAG,SAAAF,SAAO,CAAC;AACvD,YAAU,KAAK,GAAG;AAElB,QAAM,SAASG,KAAI,EAAC,QAAQ,EAAC,GAAG,IAAG,GAAG,SAAAH,UAAS,OAAO,EAAC,MAAM,SAAQ,EAAC,CAAC;AAEvE,YAAU,QAAQ,OAAKA,SAAQ,8BAA8B,CAAC,CAAC;AAE/D,SAAO;AACT;AAEO,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACjCR,SAAUK,KACZ,MAAmE;AAErE,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,MAAM,SAAQ,IAAI;AAEzB,mBAAiB,GAAG,KAAK;AAEzB,QAAM,WAAW,aAAK,eAAe,MAAM,EAAE,KAAK;AAClD,MAAI,OAAO;AACX,QAAM,eAAe,qBAAa,mBAAmB,MAAM,EAAE,MAAM,MAAM;AACzE,MAAI,KAAK;AACT,MAAI,gBAAgB,MAAM;AACxB,SAAKC,WAAU,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,MAAM,aAAY,EAAC,CAAC;AAClE,WAAO,qBAAa,iBAAiB,KAAK,QAAQ,EAAE,MAAM,MAAM;;AAGlE,uBAAa,2BAA2B,OAAO,MAAM,GAAG,MAAM,MAAM;AACpE,QAAM,CAAC,UAAU,WAAW,IACxB,qBAAa,0BAA0B,GAAG,OAAO,IAAI;AACzD,QAAM,aAAa,aAAK,cAAc,WAAW;AACjD,QAAM,OAAO,aAAK,oBAAoB,aAAK,cAAc,QAAQ,GAAG,GAAG,KAAK;AAE5E,QAAM,QAAQA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC1C,WAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,EAAE,GAAG;AACpC,UAAM,SAAS,IAAI;AACnB,QAAID,OAAM,MAAM,MAAM;AACtB,aAAS,IAAI,GAAG,IAAI,YAAY,EAAE,GAAG;AACnC,YAAM,QAAQ,MAAM,SAAS,CAAC;AAC9B,UAAI,OAAO,MAAM,KAAK,KAClB,QAAQA,MAAK;AACf,QAAAA,OAAM;;;AAGV,SAAK,CAAC,IAAIA;;AAGZ,MAAI,gBAAgB,MAAM;AACxB,IAAAC,SAAQ,8BAA8B,EAAE;;AAG1C,QAAM,SAASA,SAAQ,eAAe,UAAU,GAAG,OAAO,IAAI;AAE9D,MAAI,UAAU;AACZ,UAAM,gBAAgB,qBAAa,qBAAqB,UAAU,QAAQ;AAC1E,UAAM,iBACFE,SAAQ,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,cAAa,EAAC,CAAC;AAEzE,IAAAA,SAAQ,8BAA8B,MAAM;AAE5C,WAAO;;AAGT,SAAO;AACT;AAEO,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC9DR,SAAUI,WAAU,MAIzB;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,UAAU,KAAI,IAAI;AAEzB,mBAAiB,GAAG,WAAW;AAE/B,QAAM,WAAW,SAAS;IACtB,CAACC,IAAG,MAAMA,GAAE,CAAC,IAAoB,EAAE,MAAM,CAAC,IAAIA,GAAE,CAAC;;EAAgB;AAErE,QAAM,QAAQ,SAAS,IAAI,CAAAA,OAAKA,GAAE,CAAC,CAAC;AACpC,QAAM,MAAM,SAAS,IAAI,CAACA,IAAG,MAAMA,GAAE,CAAC,IAAI,EAAE,MAAM,CAAC,CAAC;AACpD,QAAM,SAAS,SAAS,YAAY,IAAI;AAExC,QAAM,QAAQD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,QAAQ,EAAE,MAAM;AACtB,QAAM,WAAW,aAAK,eAAe,EAAE,KAAK;AAE5C,QAAM,aAAa,aAAK,cAAc,QAAQ;AAC9C,QAAM,aAAa,SAAS;AAC5B,QAAM,gBAAgB,aAAK,eAAe,QAAQ;AAClD,QAAM,UACF,aAAK,uBAAuB,EAAE,OAA0B,UAAU;AAEtE,WAAS,IAAI,GAAG,IAAI,YAAY,KAAK;AACnC,QAAI,SAAS,aAAK,WAAW,GAAG,YAAY,aAAa;AACzD,aAASE,KAAI,GAAGA,KAAI,YAAYA,MAAK;AACnC,UAAI,OAAOA,EAAC,IAAI,MAAMA,EAAC,GAAG;AACxB,eAAOA,EAAC,IAAI,MAAMA,EAAC,IAAI,IAAI,OAAOA,EAAC,IAAI;iBAC9B,OAAOA,EAAC,KAAK,IAAIA,EAAC,GAAG;AAC9B,eAAOA,EAAC,KAAK,IAAIA,EAAC,IAAI,KAAK,IAAI,OAAOA,EAAC,IAAI;;;AAG/C,aAAS,OAAO,IAAI,CAAC,GAAGA,OAAM,IAAI,MAAMA,EAAC,CAAC;AAE1C,UAAM,UAAU,aAAK,WAAW,QAAQ,OAAO,QAAQ;AAEvD,YAAQ,CAAC,IAAI,MAAM,OAAO;;AAG5B,QAAM,QAAQF,SAAQ,MAAM,SAAS,UAAU,EAAE,KAAK;AAEtD,SAAO,EAAC,QAAQ,OAAO,OAAO,UAAU,OAAO,EAAE,MAAK;AACxD;AAEO,IAAM,kBAAgC;EAC3C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACpDP,IAAM,UACT,6BAA8B,CAAC,QAAgB,WAAkB;AAC/D,QAAM,MAAM,SAAS;AACrB,MAAK,SAAS,KAAK,SAAS,KAAO,UAAU,KAAK,UAAU,GAAI;AAC9D,WAAO;SACF;AACL,YAAQ,MAAM,UAAU;;AAE5B,CAAE;AAEC,IAAMI,OAAM,iBAAiB,KAAK,OAAO;AAEzC,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACnBd,iBAA4B;;;ACUtB,SAAUC,SACZ,MACyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAM,IAAI;AACjB,QAAM,EAAC,IAAG,IAAI;AAEd,QAAM,aAAa,OAAO,MAAM;AAEhC,MAAI,OAAO;AACX,MAAI,SAAS,IAAI;AACf,WAAO,aAAa;;AAEtB,MAAI,SAAS,aAAa,GAAG;AAC3B,UAAM,MACF,4EACmB,UAAU,gBAAgB,IAAI,EAAE;;AAGzD,QAAM,OAAO,aAAK,eAAe,CAAC,IAAI,GAAG,OAAO,KAAK;AACrD,QAAM,WAAWC,KAAI;IACnB,QAAQ,EAAC,GAAG,OAAM;IAClB,SAAAD;IACA,OAAO,EAAC,kBAAkB,MAAM,UAAU,MAAK;GAChD;AACD,QAAM,gBAAgB,qBAAa,qBAAqB,SAAS,OAAO,IAAI;AAE5E,QAAM,mBACFE,SAAQ,EAAC,QAAQ,EAAC,GAAG,SAAQ,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,cAAa,EAAC,CAAC;AAC3E,QAAM,IACFG,KAAI,EAAC,QAAQ,EAAC,GAAG,QAAQ,GAAG,iBAAgB,GAAG,SAAAH,SAAO,CAAC;AAC3D,QAAM,IAAII,KAAI,EAAC,QAAQ,EAAC,GAAG,EAAC,GAAG,SAAAJ,SAAO,CAAC;AACvC,QAAM,SACFK,KAAI,EAAC,QAAQ,EAAC,GAAG,EAAC,GAAG,SAAAL,UAAS,OAAO,EAAC,MAAM,MAAM,UAAU,MAAK,EAAC,CAAC;AACvE,QAAM,cACFE,SAAQ,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,cAAa,EAAC,CAAC;AAEzE,QAAM,SAASM,KAAI,EAAC,QAAQ,EAAC,GAAG,GAAG,GAAG,YAAW,GAAG,SAAAN,SAAO,CAAC;AAE5D,EAAAA,SAAQ,8BAA8B,QAAQ;AAC9C,EAAAA,SAAQ,8BAA8B,gBAAgB;AACtD,EAAAA,SAAQ,8BAA8B,CAAC;AACvC,EAAAA,SAAQ,8BAA8B,CAAC;AACvC,EAAAA,SAAQ,8BAA8B,MAAM;AAC5C,EAAAA,SAAQ,8BAA8B,WAAW;AAEjD,SAAO;AACT;AAEO,IAAM,gBAA8B;EACzC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ADxDR,SAAUQ,aAAY,MAI3B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAM,IAAI;AACjB,QAAM,EAAC,YAAY,MAAM,WAAU,IAAI;AAEvC,mBAAiB,QAAQ,aAAa;AAEtC,QAAM,gBAAgB,aAClB,SACAC,SAAQ,EAAC,QAAQ,EAAC,OAAM,GAAG,SAAAD,UAAS,OAAO,EAAC,KAAK,GAAE,EAAC,CAAC;AAEzD,QAAM,YAAY,cAAc,MAAM,CAAC;AACvC,QAAM,YAAY,cAAc,MAAM,CAAC;AACvC,QAAM,WAAWA,SAAQ,KAAK,IAAI,cAAc,MAAM,EAAE;AACxD,QAAM,WAAW,CAAC,WAAW,UAAU;AACvC,QAAM,UACF,aAAK,oBAAoB,aAAK,cAAc,QAAQ,GAAG,OAAO;AAElE,WAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,UAAM,SAAS,IAAI;AAGnB,UAAM,MAAM,IAAI,aAAa,YAAY,CAAC;AAC1C,QAAI,CAAC,IAAI,SAAS,MAAM;AACxB,aAAS,QAAQ,GAAG,QAAQ,IAAI,QAAQ,EAAE,OAAO;AAC/C,UAAI,KAAK,IAAI,IAAI,QAAQ,CAAC,IAAI,SAAS,SAAS,KAAK;;AAGvD,UAAM,SAAoB,gBAAK,KAAK,SAAQ,CAAE;AAC9C,UAAM,YAAY,IAAI;AACtB,aAAS,WAAW,GAAG,WAAW,YAAY,EAAE,UAAU;AACxD,YAAM,IAAI,OAAM;AAGhB,cAAQ,YAAY,QAAQ,IAAI,IAAI;AAEpC,eAAS,QAAQ,GAAG,QAAQ,IAAI,QAAQ,SAAS;AAC/C,YAAI,IAAI,IAAI,KAAK,GAAG;AAClB,kBAAQ,YAAY,QAAQ,IAAI;AAChC;;;;;AAMR,MAAI,CAAC,YAAY;AACf,IAAAA,SAAQ,8BAA8B,aAAa;;AAGrD,SAAOA,SAAQ,eAAe,UAAU,SAAS,OAAO;AAC1D;AAEO,IAAM,oBAAkC;EAC7C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AEjEd,IAAM,0BAA0B,qBAAa;AAKvC,SAAU,oBAAoB,MAInC;AACC,QAAM,EAAC,QAAQ,SAAAG,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAO,OAAM,IAAI;AACxB,QAAM,EAAC,eAAe,cAAc,eAAc,IAAI;AAEtD,mBAAiB,OAAO,mBAAmB;AAE3C,QAAM,YAAYA,SAAQ,KAAK,IAAI,MAAM,MAAM,EAAE;AACjD,QAAM,aAAaA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAEnD,QAAM,EAAC,gBAAe,IAAI,wBACtB,WAAW,YAAY,eAAe,cAAc,cAAc;AAEtE,SAAOA,SAAQ,eACX,CAAC,gBAAgB,MAAM,GAAG,SAAS,IAAI,WAAW,eAAe,CAAC;AACxE;AAEO,IAAM,4BAA0C;EACrD,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC7Bd,IAAM,0BAA0B,qBAAa;AAIvC,SAAU,oBAAoB,MAInC;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAO,OAAM,IAAI;AACxB,QAAM,EAAC,eAAe,cAAc,gBAAgB,mBAAkB,IAClE;AAEJ,mBAAiB,OAAO,yBAAyB;AAEjD,QAAM,YAAYA,SAAQ,KAAK,IAAI,MAAM,MAAM,EAAE;AACjD,QAAM,aAAaA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAEnD,QAAM,EAAC,iBAAiB,aAAY,IAAI,wBACpC,WAAW,YAAY,eAAe,cAAc,gBACpD,kBAAkB;AAEtB,SAAO;IACLA,SAAQ,eACJ,CAAC,gBAAgB,MAAM,GAAG,SAAS,IAAI,WAAW,eAAe,CAAC;IACtEA,SAAQ,eAAe,CAAA,GAAI,SAAS,IAAI,WAAW,CAAC,YAAY,CAAC,CAAC;;AAEtE;AACO,IAAM,4BAA0C;EACrD,YAAY;EACZ,aAAa;EACb,YAAY;;;;AChCd,IAAM,0BAA0B,qBAAa;AAIvC,SAAU,oBAAoB,MAInC;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAO,OAAM,IAAI;AACxB,QAAM,EAAC,eAAe,cAAc,gBAAgB,aAAY,IAAI;AAEpE,mBAAiB,OAAO,4BAA4B;AAEpD,QAAM,YAAYA,SAAQ,KAAK,IAAI,MAAM,MAAM,EAAE;AACjD,QAAM,aAAaA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAEnD,QAAM,mBAAmB;AACzB,QAAM,kBAAkB;AACxB,QAAM,oBAAoB;AAC1B,QAAM,kBAAkB;AAExB,QAAM,EAAC,iBAAiB,eAAc,IAAI,wBACtC,WAAW,YAAY,kBAAkB,iBACzC,mBAAmB,eAAe;AAEtC,SAAO;IACLA,SAAQ,eACJ,CAAC,gBAAgB,MAAM,GAAG,SAAS,IAAI,WAAW,eAAe,CAAC;IACtEA,SAAQ,eACJ,CAAC,eAAe,MAAM,GAAG,WAAW,IAAI,aAAa,cAAc,CAAC;;AAE5E;AAEO,IAAM,4BAA0C;EACrD,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACnCR,SAAUC,QACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,QAAO,IAAI;AAClB,QAAM,EAAC,OAAO,OAAO,SAAS,SAAQ,IAAI;AAE1C,mBAAiB,SAAS,QAAQ;AAElC,QAAM,cAAc,aAAK,cAAc,QAAQ,KAAK;AAEpD,QAAM,MAAM,IAAI,aAAa,cAAc,KAAK;AAChD,MAAI,KAAK,QAAQ;AACjB,QAAM,aAAaA,SAAQ,KAAK,IAAI,QAAQ,MAAM,EAAE;AAEpD,WAAS,QAAQ,GAAG,QAAQ,aAAa,EAAE,OAAO;AAChD,QAAI,WAAW,KAAK,KAAK,KAAK,WAAW,KAAK,IAAI,OAAO;AACvD,UAAI,QAAQ,QAAQ,WAAW,KAAK,CAAC,IAAI;;;AAI7C,SAAOA,SAAQ,eAAe,CAAC,GAAG,QAAQ,OAAO,KAAK,GAAG,OAAO,GAAG;AACrE;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACvBR,SAAUE,WACZ,MAAwD;AAC1D,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,EAAC,IAAI;AAEZ,MAAI,EAAE,UAAU,UAAU;AACxB,UAAM,IAAI,MAAM,+CAA+C;aACtD,EAAE,UAAU,aAAa;AAClC,UAAM,WAAWC,MAAK,EAAC,QAAQ,EAAC,OAAO,EAAC,GAAG,SAAAD,SAAO,CAAC;AACnD,UAAM,IAAID,WAAU,EAAC,QAAQ,EAAC,GAAG,SAAQ,GAAG,SAAAC,SAAO,CAAC;AACpD,UAAM,WAAWE,MAAK,EAAC,QAAQ,EAAC,OAAO,EAAC,GAAG,SAAAF,SAAO,CAAC;AACnD,UAAM,IAAID,WAAU,EAAC,QAAQ,EAAC,GAAG,SAAQ,GAAG,SAAAC,SAAO,CAAC;AAEpD,UAAM,SAASG,SAAQ,EAAC,QAAQ,EAAC,MAAM,GAAG,MAAM,EAAC,GAAG,SAAAH,SAAO,CAAC;AAE5D,IAAAA,SAAQ,8BAA8B,QAAQ;AAC9C,IAAAA,SAAQ,8BAA8B,CAAC;AACvC,IAAAA,SAAQ,8BAA8B,QAAQ;AAC9C,IAAAA,SAAQ,8BAA8B,CAAC;AAEvC,WAAO;SACF;AACL,WAAOI,MAAK,EAAC,SAAAJ,UAAS,OAAO,EAAC,OAAO,EAAE,OAAO,OAAO,GAAG,OAAO,EAAE,MAAK,EAAC,CAAC;;AAE5E;AAEO,IAAM,kBAAgC;EAC3C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC7BR,SAAUM,UACZ,MAAuD;AACzD,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,EAAC,IAAI;AAEZ,MAAI,EAAE,UAAU,UAAU;AACxB,UAAM,IAAI,MAAM,8CAA8C;aACrD,EAAE,UAAU,aAAa;AAClC,UAAM,WAAWC,MAAK,EAAC,QAAQ,EAAC,OAAO,EAAC,GAAG,SAAAD,SAAO,CAAC;AACnD,UAAM,IAAID,UAAS,EAAC,QAAQ,EAAC,GAAG,SAAQ,GAAG,SAAAC,SAAO,CAAC;AACnD,UAAM,WAAWE,MAAK,EAAC,QAAQ,EAAC,OAAO,EAAC,GAAG,SAAAF,SAAO,CAAC;AACnD,UAAM,IAAIG,WAAU,EAAC,QAAQ,EAAC,GAAG,SAAQ,GAAG,SAAAH,SAAO,CAAC;AAEpD,UAAM,SAASI,SAAQ,EAAC,QAAQ,EAAC,MAAM,GAAG,MAAM,EAAC,GAAG,SAAAJ,SAAO,CAAC;AAE5D,IAAAA,SAAQ,8BAA8B,QAAQ;AAC9C,IAAAA,SAAQ,8BAA8B,CAAC;AACvC,IAAAA,SAAQ,8BAA8B,QAAQ;AAC9C,IAAAA,SAAQ,8BAA8B,CAAC;AAEvC,WAAO;SACF;AACL,WAAOK,MAAK,EAAC,SAAAL,UAAS,OAAO,EAAC,OAAO,EAAE,OAAO,OAAO,GAAG,OAAO,EAAE,MAAK,EAAC,CAAC;;AAE5E;AAEO,IAAM,iBAA+B;EAC1C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AChCR,SAAU,KACZ,MAAqE;AAEvE,QAAM,EAAC,QAAQ,SAAAO,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,KAAI,IAAI;AAEf,MAAI,OAAO,WAAW,GAAG;AACvB,WAAOC,YACH,EAAC,QAAQ,EAAC,OAAO,OAAO,CAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,KAAK,KAAI,EAAC,CAAC;;AAG/D,QAAM,QAAQ,OAAO,CAAC,EAAE;AACxB,QAAM,QAAQ,OAAO,CAAC,EAAE;AAExB,SAAO,QAAQ,OAAI;AACjB,iBAAK,kBACD,OAAO,EAAE,OACT,uDAAuD;AAC3D,iBAAK,OACD,UAAU,EAAE,OACZ,MAAM,uDAAuD;EACnE,CAAC;AAED,QAAM,0BAAwC,CAAA;AAC9C,QAAM,kBAAkB,OAAO,IAAI,OAAI;AACrC,UAAM,YACFC,YAAW,EAAC,QAAQ,EAAC,OAAO,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,KAAK,KAAI,EAAC,CAAC;AAChE,4BAAwB,KAAK,SAAS;AACtC,WAAO;EACT,CAAC;AAED,QAAM,SAASE,QAAO,EAAC,QAAQ,iBAAiB,SAAAF,UAAS,OAAO,EAAC,KAAI,EAAC,CAAC;AAEvE,0BAAwB,QACpB,OAAKA,SAAQ,8BAA8B,CAAC,CAAC;AAEjD,SAAO;AACT;AAEO,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC3CR,SAAU,MACZ,MAAuE;AAEzE,QAAM,EAAC,QAAQ,SAAAG,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,UAAU,cAAa,IAAI;AAElC,mBAAiB,GAAG,KAAK;AAEzB,QAAM,WAAW,SAAS;IACtB,CAACC,IAAG,MAAMA,GAAE,CAAC,IAAoB,EAAE,MAAM,CAAC,IAAIA,GAAE,CAAC;;EAAgB;AAErE,QAAM,QAAQ,SAAS,IAAI,CAAAA,OAAKA,GAAE,CAAC,CAAC;AAEpC,QAAM,QAAQD,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,QAAQ,aAAK,cAAc,EAAE,KAAK;AACxC,QAAM,QAAQ,EAAE,MAAM;AACtB,QAAM,WAAW,aAAK,eAAe,EAAE,KAAK;AAE5C,QAAM,aAAa,aAAK,cAAc,QAAQ;AAC9C,QAAM,aAAa,SAAS;AAC5B,QAAM,gBAAgB,aAAK,eAAe,QAAQ;AAClD,QAAM,UACF,aAAK,uBAAuB,EAAE,OAA0B,UAAU;AAEtE,MAAI,kBAAkB,GAAG;AACvB,YAAQ,KAAK,aAAa;;AAG5B,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,SAAS,aAAK,WAAW,GAAG,OAAO,QAAQ;AACjD,UAAM,YAAY,OAAO,IAAI,CAAC,GAAGE,OAAM,IAAI,MAAMA,EAAC,CAAC;AACnD,UAAM,WAAW,aAAK,WAAW,WAAW,YAAY,aAAa;AAErE,YAAQ,QAAQ,IAAI,MAAM,CAAC;;AAG7B,QAAM,QAAQF,SAAQ,MAAM,SAAS,UAAU,EAAE,KAAK;AAEtD,SAAO,EAAC,QAAQ,OAAO,OAAO,UAAU,OAAO,EAAE,MAAK;AACxD;AAEO,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC7CP,IAAM,UACT,6BAA6B,CAAC,GAAW,MAAc,KAAK,IAAI,GAAG,CAAC,CAAC;AAClE,IAAMG,OAAM,iBAAiB,KAAK,OAAO;AAEzC,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACNR,SAAUC,cAAa,MAI5B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,oBAAoB,mBAAmB,QAAO,IAAI;AACzD,QAAM,EAAC,iBAAgB,IAAI;AAE3B,QAAM,sBAAsB,mBAAmB,IAC3C,OAAKA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE,MAAoB;AACxD,QAAM,4BAA4B,mBAAmB,IAAI,OAAK,EAAE,KAAK;AACrE,QAAM,qBACFA,SAAQ,KAAK,IAAI,kBAAkB,MAAM,EAAE;AAC/C,QAAM,WAAWA,SAAQ,KAAK,IAAI,QAAQ,MAAM,EAAE;AAElD,QAAM,CAAC,oBAAoB,mBAAmB,sBAAsB,IAChE,iBACI,qBAAqB,2BAA2B,oBAChD,kBAAkB,OAAO,kBAAkB,OAAO,UAClD,QAAQ,OAAO,gBAAgB;AAEvC,QAAM,4BAA4B,mBAAmB,IACjD,CAAC,WAAWA,SAAQ,eAAe,CAAC,OAAO,MAAM,GAAG,SAAS,MAAM,CAAC;AAExE,QAAM,0BAA0BA,SAAQ,eACpC,wBAAwB,kBAAkB,OAAO,iBAAiB;AAEtE,SAAO,0BAA0B,OAAO,CAAC,uBAAuB,CAAC;AACnE;AAEO,IAAM,qBAAmC;EAC9C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AClCR,SAAUE,sBAAqB,MAIpC;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAO,QAAQ,cAAc,oBAAmB,IAAI;AAC3D,QAAM,EAAC,kBAAiB,IAAI;AAE5B,QAAM,SAASA,SAAQ,KAAK,IAAI,MAAM,MAAM,EAAE;AAC9C,QAAM,UAAUA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAChD,QAAM,gBACFA,SAAQ,KAAK,IAAI,aAAa,MAAM,EAAE;AAC1C,QAAM,sBAAsB,oBAAoB,IAC5C,OAAKA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE,MAAoB;AACxD,QAAM,2BAA2B,oBAAoB,IAAI,OAAK,EAAE,KAAK;AAErE,QAAM,CAAC,aAAa,MAAM,IAAI,yBAC1B,QAAQ,MAAM,OAAO,SAAS,OAAO,OAAO,OAAO,OAAO,eAC1D,aAAa,OAAO,qBAAqB,0BACzC,iBAAiB;AACrB,SAAOA,SAAQ,eAAe,aAAa,OAAO,OAAO,MAAM;AACjE;AAEO,IAAM,6BAA2C;EACtD,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC5BR,SAAUE,OAAM,MAAkD;AAEtE,QAAM,EAAC,SAAAC,UAAS,MAAK,IAAI;AACzB,QAAM,EAAC,OAAO,MAAM,OAAO,MAAAC,MAAI,IAAI;AAEnC,QAAM,SAAS,UAAU,OAAO,MAAMA,OAAM,KAAK;AACjD,SAAOD,SAAQ,eAAe,CAAC,OAAO,MAAM,GAAG,OAAO,MAAM;AAC9D;AAEO,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACbP,IAAMG,cAAa,gBAAgB,YAAY,CAAC,OAAO,IAAI,EAAE;AAE7D,IAAM,mBAAiC;EAC5C,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACJR,SAAUC,gBAAe,MAI9B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAM,IAAI;AACjB,QAAM,EAAC,cAAc,kBAAkB,KAAI,IAAI;AAE/C,mBAAiB,QAAQ,gBAAgB;AAEzC,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AACtD,QAAM,CAAC,WAAW,QAAQ,IAAI;AAE9B,QAAM,CAAC,OAAO,WAAW,UAAU,WAAW,IAAI,OAAO;AACzD,QAAM,UAAUA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAChD,QAAM,SAAS,IAAI,aACf,aAAK,cAAc,CAAC,OAAO,WAAW,UAAU,WAAW,CAAC,CAAC;AAEjE,QAAM,qBAAuC;IAC1C,gBAAgB,YAAY,IAAK,YAAY,IAAI;IACjD,gBAAgB,WAAW,IAAK,WAAW,IAAI;;AAGlD,QAAM,sBAAwC;IAC3C,gBAAgB,YAAY,IAAK,YAAY,IAAI;IACjD,gBAAgB,WAAW,IAAK,WAAW,IAAI;;AAElD,MAAI,YAAY;AAChB,QAAM,wBAAwB,mBAAmB,CAAC,IAAI,oBAAoB,CAAC;AAC3E,QAAM,wBAAwB,mBAAmB,CAAC,IAAI,oBAAoB,CAAC;AAC3E,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,aAAS,IAAI,GAAG,IAAI,WAAW,KAAK;AAClC,UAAI;AACJ,UAAI,kBAAkB;AACpB,wBAAgB,yBAAyB,IAAI,OAAO;aAC/C;AACL,wBAAgB,wBAAwB;;AAG1C,YAAM,iBAAiB,KAAK,IAAI,GAAG,KAAK,MAAM,aAAa,CAAC;AAC5D,YAAM,UAAU,gBAAgB;AAChC,YAAM,gBAAgB,KAAK,IAAI,YAAY,GAAG,KAAK,KAAK,aAAa,CAAC;AACtE,YAAM,eACF,IAAI,cAAc,CAAC,IAAI,iBAAiB,cAAc,CAAC;AAC3D,YAAM,eACF,IAAI,cAAc,CAAC,IAAI,gBAAgB,cAAc,CAAC;AAC1D,eAAS,IAAI,GAAG,IAAI,UAAU,KAAK;AACjC,YAAI;AACJ,YAAI,kBAAkB;AACpB,0BAAgB,yBAAyB,IAAI,OAAO;eAC/C;AACL,0BAAgB,wBAAwB;;AAE1C,cAAM,iBAAiB,KAAK,IAAI,GAAG,KAAK,MAAM,aAAa,CAAC;AAC5D,cAAM,UAAU,gBAAgB;AAChC,cAAM,gBAAgB,KAAK,IAAI,WAAW,GAAG,KAAK,KAAK,aAAa,CAAC;AACrE,cAAM,gBAAgB,eAAe,iBAAiB,cAAc,CAAC;AACrE,cAAM,gBAAgB,eAAe,iBAAiB,cAAc,CAAC;AACrE,cAAM,iBAAiB,eAAe,gBAAgB,cAAc,CAAC;AACrE,cAAM,iBAAiB,eAAe,gBAAgB,cAAc,CAAC;AACrE,iBAAS,IAAI,GAAG,IAAI,aAAa,KAAK;AAIpC,gBAAM,UAAU,QAAQ,gBAAgB,CAAC;AACzC,gBAAM,aAAa,QAAQ,gBAAgB,CAAC;AAC5C,gBAAM,WAAW,QAAQ,iBAAiB,CAAC;AAC3C,gBAAM,cAAc,QAAQ,iBAAiB,CAAC;AAE9C,gBAAM,MAAM,WAAW,WAAW,WAAW;AAC7C,gBAAM,SAAS,cAAc,cAAc,cAAc;AACzD,gBAAM,WAAW,OAAO,SAAS,OAAO;AAExC,iBAAO,WAAW,IAAI;;;;;AAM9B,SAAOA,SAAQ,eACX,CAAC,OAAO,WAAW,UAAU,WAAW,GAAG,WAAW,MAAM;AAClE;AAEO,IAAM,uBAAqC;EAChD,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACvFR,SAAU,mBAAmB,MAIlC;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,QAAQ,GAAE,IAAI;AACrB,QAAM,EAAC,aAAY,IAAI;AAEvB,mBAAiB,CAAC,IAAI,MAAM,GAAG,oBAAoB;AAEnD,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AAEtD,QAAM,CAAC,OAAO,SAAS,QAAQ,KAAK,IAAI,OAAO;AAC/C,QAAM,CAAC,EAAE,SAAS,MAAM,IAAI,GAAG;AAE/B,QAAM,SAAS,IAAI,aAAa,QAAQ,UAAU,SAAS,KAAK;AAOhE,QAAM,iBAAmC;IACtC,gBAAgB,UAAU,IAAK,UAAU,IAAI;IAC7C,gBAAgB,SAAS,IAAK,SAAS,IAAI;;AAG9C,QAAM,iBAAmC;IACtC,gBAAgB,UAAU,IAAK,UAAU,IAAI;IAC7C,gBAAgB,SAAS,IAAK,SAAS,IAAI;;AAG9C,QAAM,cAAc,eAAe,CAAC,IAAI,eAAe,CAAC;AACxD,QAAM,aAAa,eAAe,CAAC,IAAI,eAAe,CAAC;AAKvD,QAAM,WAAWA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAC7C,MAAI,SAAS;AACb,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,UAAU,IAAI,cAAc,CAAC;AACnC,aAAS,IAAI,GAAG,IAAI,SAAS,KAAK;AAChC,YAAM,MAAM,IAAI;AAChB,YAAM,cAAc,KAAK,MAAM,GAAG;AAClC,YAAM,iBAAiB,KAAK,IAAI,KAAK,KAAK,GAAG,GAAG,UAAU,CAAC;AAE3D,YAAM,eAAe,UAAU,cAAc,cAAc,CAAC;AAC5D,YAAM,kBAAkB,UAAU,iBAAiB,cAAc,CAAC;AAElE,YAAM,UAAU,MAAM;AACtB,YAAM,iBAAiB,IAAM;AAC7B,eAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAC/B,cAAM,MAAM,IAAI;AAChB,cAAM,eAAe,KAAK,MAAM,GAAG;AACnC,cAAM,gBAAgB,KAAK,IAAI,KAAK,KAAK,GAAG,GAAG,SAAS,CAAC;AACzD,cAAM,UAAU,MAAM;AACtB,cAAM,iBAAiB,IAAM;AAE7B,cAAM,kBAAkB,eAAe,eAAe,cAAc,CAAC;AACrE,cAAM,mBACF,eAAe,gBAAgB,cAAc,CAAC;AAClD,cAAM,qBACF,kBAAkB,eAAe,cAAc,CAAC;AACpD,cAAM,sBACF,kBAAkB,gBAAgB,cAAc,CAAC;AAErD,cAAM,oCACF,iBAAiB;AACrB,cAAM,6BAA6B,iBAAiB;AACpD,cAAM,6BAA6B,UAAU;AAC7C,cAAM,sBAAsB,UAAU;AACtC,iBAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,gBAAM,QAAQ,SAAS,QAAQ;AAC/B,iBAAO,kBAAkB,CAAC,KACtB,QAAQ;AACZ,iBAAO,mBAAmB,CAAC,KAAK,QAAQ;AACxC,iBAAO,qBAAqB,CAAC,KAAK,QAAQ;AAC1C,iBAAO,sBAAsB,CAAC,KAAK,QAAQ;;;;;AAMnD,SAAOA,SAAQ,eACX,CAAC,OAAO,QAAQ,SAAS,KAAK,GAAG,WAAW,MAAM;AACxD;AAEO,IAAM,2BAAyC;EACpD,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC5FR,SAAUC,uBAAsB,MAIrC;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,OAAM,IAAI;AACjB,QAAM,EAAC,cAAc,kBAAkB,KAAI,IAAI;AAE/C,mBAAiB,QAAQ,uBAAuB;AAEhD,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AACtD,QAAM,CAAC,WAAW,QAAQ,IAAI;AAE9B,QAAM,CAAC,OAAO,WAAW,UAAU,WAAW,IAAI,OAAO;AACzD,QAAM,UAAUA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAChD,QAAM,SAAS,IAAI,aAAa,QAAQ,YAAY,WAAW,WAAW;AAE1E,QAAM,qBAAuC;IAC1C,gBAAgB,YAAY,IAAK,YAAY,IAAI;IACjD,gBAAgB,WAAW,IAAK,WAAW,IAAI;;AAGlD,QAAM,sBAAwC;IAC3C,gBAAgB,YAAY,IAAK,YAAY,IAAI;IACjD,gBAAgB,WAAW,IAAK,WAAW,IAAI;;AAGlD,QAAM,wBAAwB,mBAAmB,CAAC,IAAI,oBAAoB,CAAC;AAC3E,QAAM,wBAAwB,mBAAmB,CAAC,IAAI,oBAAoB,CAAC;AAE3E,MAAI,eAAe;AACnB,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,cAAc,IAAI,cAAc,CAAC;AACvC,aAAS,IAAI,GAAG,IAAI,WAAW,KAAK;AAClC,YAAM,gBAAgB,mBAClB,yBAAyB,IAAI,OAC7B,wBAAwB;AAC5B,UAAI,mBAAmB,KAAK,IACxB,YAAY,GACZ,eAAe,KAAK,MAAM,aAAa,IAAI,KAAK,MAAM,aAAa,CAAC;AACxE,UAAI,kBAAkB;AACpB,2BAAmB,KAAK,IAAI,GAAG,gBAAgB;;AAEjD,YAAM,YAAY,cAAc,mBAAmB,cAAc,CAAC;AAClE,eAAS,IAAI,GAAG,IAAI,UAAU,KAAK;AACjC,cAAM,gBAAgB,mBAClB,yBAAyB,IAAI,OAC7B,wBAAwB;AAC5B,YAAI,mBAAmB,KAAK,IACxB,WAAW,GACX,eAAe,KAAK,MAAM,aAAa,IACxB,KAAK,MAAM,aAAa,CAAC;AAC5C,YAAI,kBAAkB;AACpB,6BAAmB,KAAK,IAAI,GAAG,gBAAgB;;AAEjD,cAAM,YAAY,YAAY,mBAAmB,cAAc,CAAC;AAChE,iBAAS,IAAI,GAAG,IAAI,aAAa,KAAK;AAGpC,gBAAM,SAAS,QAAQ,YAAY,CAAC;AACpC,iBAAO,cAAc,IAAI;;;;;AAMjC,SAAOA,SAAQ,eACX,CAAC,OAAO,WAAW,UAAU,WAAW,GAAG,OAAO,OAAO,MAAM;AACrE;AAEO,IAAM,8BAA4C;EACvD,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC1ER,SAAU,0BAA0B,MAIzC;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,QAAQ,GAAE,IAAI;AACrB,QAAM,EAAC,aAAY,IAAI;AAEvB,mBAAiB,CAAC,IAAI,MAAM,GAAG,2BAA2B;AAE1D,QAAM,gBAAgB,aAAK,eAAe,OAAO,KAAK;AACtD,QAAM,YAAY,aAAK,eAAe,GAAG,KAAK;AAC9C,QAAM,CAAC,OAAO,SAAS,QAAQ,KAAK,IAAI,OAAO;AAC/C,QAAM,CAAC,EAAE,SAAS,MAAM,IAAI,GAAG;AAE/B,QAAM,SAAS,IAAI,aAAa,QAAQ,UAAU,SAAS,KAAK;AAChE,QAAM,WAAWA,SAAQ,KAAK,IAAI,GAAG,MAAM,EAAE;AAK7C,QAAM,iBAAmC;IACtC,gBAAgB,UAAU,IAAK,UAAU,IAAI;IAC7C,gBAAgB,SAAS,IAAK,SAAS,IAAI;;AAG9C,QAAM,iBAAmC;IACtC,gBAAgB,UAAU,IAAK,UAAU,IAAI;IAC7C,gBAAgB,SAAS,IAAK,SAAS,IAAI;;AAG9C,QAAM,cAAc,eAAe,CAAC,IAAI,eAAe,CAAC;AACxD,QAAM,aAAa,eAAe,CAAC,IAAI,eAAe,CAAC;AAEvD,QAAM,iBAAiB,IAAI;AAC3B,QAAM,gBAAgB,IAAI;AAI1B,QAAM,YAAa,KAAK,KAAK,cAAc,IAAI,IAAK;AACpD,QAAM,WAAY,KAAK,KAAK,aAAa,IAAI,IAAK;AAGlD,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,cAAc,IAAI,cAAc,CAAC;AACvC,aAAS,IAAI,GAAG,IAAI,SAAS,KAAK;AAChC,YAAM,YAAY,cAAc,IAAI,cAAc,CAAC;AAGnD,YAAM,aAAa,KAAK,MAAM,IAAI,cAAc;AAChD,YAAM,WAAW,KAAK,MAAM,aAAc,YAAY,CAAE;AACxD,eAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAC/B,cAAM,YAAY,YAAY,IAAI,cAAc,CAAC;AAGjD,cAAM,aAAa,KAAK,MAAM,IAAI,aAAa;AAC/C,cAAM,WAAW,KAAK,MAAM,aAAc,WAAW,CAAE;AAEvD,iBAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,cAAI,QAAQ;AAGZ,mBAAS,WAAW,GAAG,WAAW,WAAW,YAAY;AACvD,kBAAM,MAAM,WAAW;AAEvB,gBAAI,MAAM,KAAK,OAAO,SAAS;AAC7B;;AAGF,kBAAM,YAAY,cAAc,MAAM,UAAU,CAAC;AACjD,kBAAM,gBAAgB,MAAM;AAC5B,kBAAM,mBAAmB,KAAK,IAC1B,UAAU,GACV,eAAe,KAAK,MAAM,aAAa,IACxB,KAAK,MAAM,aAAa,CAAC;AAC5C,gBAAI,MAAM,kBAAkB;AAC1B;;AAEF,qBAAS,WAAW,GAAG,WAAW,UAAU,YAAY;AACtD,oBAAM,MAAM,WAAW;AAEvB,kBAAI,MAAM,KAAK,OAAO,QAAQ;AAC5B;;AAGF,oBAAM,YAAY,YAAY,MAAM,UAAU,CAAC;AAC/C,oBAAM,gBAAgB,MAAM;AAC5B,oBAAM,mBAAmB,KAAK,IAC1B,SAAS,GACT,eAAe,KAAK,MAAM,aAAa,IACxB,KAAK,MAAM,aAAa,CAAC;AAE5C,kBAAI,MAAM,kBAAkB;AAC1B,yBAAS,SAAS,YAAY,CAAC;;;;AAIrC,iBAAO,YAAY,CAAC,IAAI;;;;;AAMhC,SAAOA,SAAQ,eAAe,OAAO,OAAO,OAAO,OAAO,MAAM;AAClE;AAEO,IAAM,kCAAgD;EAC3D,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC7GR,SAAUC,SACZ,MACyE;AAE3E,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,KAAI,IAAI;AAEf,mBAAiB,GAAG,SAAS;AAE7B,QAAM,QAAQ,EAAE,MAAM;AAEtB,QAAM,QAAQ,aAAK,eAAe,MAAM,EAAE,KAAK;AAC/C,MAAI,UAAU,GAAG;AACf,WAAO,SAAS,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAA,SAAO,CAAC;;AAGxC,QAAM,SAAS,IAAI,aAAa,EAAE,OAAO,EAAE,KAAK;AAChD,QAAM,OAAOA,SAAQ,WAAW,CAAC;AAEjC,WAAS,IAAI,GAAG,IAAI,OAAO,MAAM,KAAK;AACpC,UAAM,SAAS,OAAO,WAAW,CAAC;AAClC,UAAM,QAAQ,OAAO,MAAK;AAC1B,UAAM,QAAQ,OAAK,MAAM,CAAC,IAAI,EAAE,MAAM,CAAC,IAAI,IAAI,MAAM,CAAC,CAAC;AACvD,WAAO,IAAI,KAAK,IAAI,GAAG,KAAK,GAAG,GAAG,MAAM;;AAG1C,SAAOA,SAAQ,eAAe,OAAO,OAAO,OAAO,OAAO,OAAO,MAAM;AACzE;AAEO,IAAM,gBAA8B;EACzC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AClCP,IAAM,yBAAuC;EAClD,YAAY;EACZ,aAAa;EACb,YAAY,CAAC,EAAC,QAAQ,OAAO,SAAAE,SAAO,MAAK;AACvC,UAAM,EAAC,OAAAC,OAAK,IAAI;AAChB,UAAM,EAAC,SAAS,WAAW,OAAM,IAAI;AACrC,UAAM,aAAaD;AAEnB,UAAM,SAAS,aAAK,uBAChBC,OAAM,OAA0B,aAAK,cAAcA,OAAM,KAAK,CAAC;AACnE,UAAM,CAAC,OAAO,aAAa,YAAY,WAAW,IAAIA,OAAM;AAE5D,UAAM,CAAC,SAAS,OAAO,IACnB,qBAAa,eAAe,QAAQ,aAAa,UAAU;AAC/D,UAAM,mBAAmB;AAEzB,UAAM,YAAY,KAAK,IAAI,OAAO;AAClC,UAAM,YAAY,KAAK,IAAI,OAAO;AAClC,UAAM,YAAY,WAAW,KAAK,IAAIA,OAAM,MAAM,EAAE;AAEpD,aAAS,WAAW,GAAG,WAAW,OAAO,YAAY;AACnD,YAAM,cAAc,WAAW,aAAa,cAAc;AAE1D,eAAS,MAAM,GAAG,MAAM,aAAa,OAAO;AAC1C,cAAM,YAAY,OAAO,aAAa;AAEtC,iBAAS,MAAM,GAAG,MAAM,YAAY,OAAO;AACzC,gBAAM,YAAY,MAAM;AAExB,mBAAS,UAAU,GAAG,UAAU,aAAa,WAAW;AACtD,kBAAM,SAAS,CAAC,OAAO,KAAK,KAAK,OAAO;AAExC,kBAAM,IAAI,OAAO,CAAC;AAClB,kBAAM,IAAI,OAAO,CAAC;AAGlB,gBAAI,UAAU,IAAI,WAAW,aAAa,IAAI,WAAW;AACzD,gBAAI,UAAU,IAAI,WAAW,aAAa,IAAI,WAAW;AACzD,qBAAS,KAAK,MAAM,SAAS,OAAO;AACpC,qBAAS,KAAK,MAAM,SAAS,OAAO;AAEpC,gBAAI,cAAc;AAClB,gBAAI,OAAO,cAAc,UAAU;AACjC,kBAAI,YAAY,GAAG;AACjB,8BAAc;qBACT;AACL,8BAAc,UAAU,OAAO;;;AAKnC,gBAAI,UAAU,KAAK,SAAS,cAAc,UAAU,KAChD,SAAS,aAAa;AAExB,oBAAM,mBAAmB,UAAU,aAAa;AAChD,oBAAM,mBAAmB,SAAS;AAClC,oBAAM,WACF,cAAc,mBAAmB,mBAAmB;AACxD,4BAAc,UAAU,QAAQ;;AAGlC,kBAAM,SAAS,cAAc,YAAY,YAAY;AACrD,mBAAO,MAAM,IAAI;;;;;AAMzB,UAAM,SAAS,WAAW,MAAM,QAAQA,OAAM,OAAOA,OAAM,KAAK;AAChE,WAAO,EAAC,QAAQ,OAAOA,OAAM,OAAO,OAAOA,OAAM,MAAK;EACxD;;;;ACvEK,IAAMC,SAAQ,gBAAgB,OAAO,CAAC,OAAM;AAEjD,QAAM,OAAO,KAAK,MAAM,EAAE;AAC1B,MAAI,KAAK,OAAO,KAAK;AACnB,WAAO,KAAK,MAAM,EAAE;aACX,KAAK,OAAO,KAAK;AAC1B,WAAO,KAAK,KAAK,EAAE;SACd;AACL,QAAI,OAAO,MAAQ,GAAK;AACtB,aAAO;WACF;AACL,aAAO,OAAO;;;AAGpB,CAAC;AAEM,IAAM,cAA4B;EACvC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;AClBR,SAAU,UAAU,MAIzB;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,SAAS,QAAO,IAAI;AAC3B,QAAM,EAAC,MAAK,IAAI;AAEhB,QAAM,EAAC,WAAW,YAAY,WAAW,SAAS,WAAU,IACxD,qBAAa,gBAAgB,SAAS,SAAS,KAAK;AACxD,QAAM,iBAAiB;AAEvB,QAAM,aAAaA,SAAQ,WAA0B,OAAO;AAC5D,QAAM,aAAaA,SAAQ,WAAoC,OAAO;AAEtE,QAAM,SAAS,YACX,YAAY,YAAY,OAAO,YAAY,WAAW,YACtD,WAAW,SAAS,GAAsB,cAAc;AAE5D,SAAOA,SAAQ,eAAe,OAAO,OAAO,OAAO,OAAO,MAAM;AAClE;AAEO,IAAM,kBAAgC;EAC3C,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC7Bd,SAASC,YAAW,OAAmB,OAAa;AAClD,MAAI,OAAO;AACX,MAAI,QAAQ,MAAM;AAClB,MAAI,MAAM;AACV,SAAO,OAAO,OAAO;AACnB,UAAM,KAAK,OAAO,OAAO,SAAS,CAAC;AACnC,QAAI,MAAM,GAAG,IAAI,OAAO;AACtB,aAAO,MAAM;WACR;AACL,cAAQ;;;AAGZ,SAAO;AACT;AAEA,SAASC,YAAW,OAAmB,OAAa;AAClD,MAAI,OAAO;AACX,MAAI,QAAQ,MAAM;AAClB,MAAI,MAAM;AACV,SAAO,OAAO,OAAO;AACnB,UAAM,KAAK,OAAO,OAAO,SAAS,CAAC;AACnC,QAAI,MAAM,GAAG,KAAK,OAAO;AACvB,aAAO,MAAM;WACR;AACL,cAAQ;;;AAGZ,SAAO;AACT;AAEM,SAAU,iBACZ,cAA0B,QAAoB,WAC9C,WAAmB,WAAmB,MAAoB;AAC5D,QAAM,SACF,aAAK,kBAAkB,SAAS,YAAY,SAAS;AACzD,WAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,UAAM,oBACF,aAAa,MAAM,IAAI,YAAY,IAAI,KAAK,SAAS;AACzD,UAAM,eAAe,IAAI;AACzB,aAAS,IAAI,GAAG,IAAI,WAAW,EAAE,GAAG;AAClC,aAAO,eAAe,CAAC,IAAI,SAAS,SAChCD,YAAW,mBAAmB,OAAO,IAAI,YAAY,CAAC,IACtDC,YAAW,mBAAmB,OAAO,IAAI,YAAY,CAAC;;;AAG9D,SAAO;AACT;;;AC1CM,SAAUC,cAAa,MAI5B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,gBAAgB,OAAM,IAAI;AACjC,QAAM,EAAC,KAAI,IAAI;AAEf,QAAM,kBACFA,SAAQ,KAAK,IAAI,eAAe,MAAM,EAAE;AAC5C,QAAM,UAAUA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAEhD,QAAM,SAAS,iBACX,iBAAiB,SAAS,eAAe,MAAM,CAAC,GAChD,eAAe,MAAM,CAAC,GAAG,OAAO,MAAM,CAAC,GAAG,IAAI;AAClD,SAAOA,SAAQ,eAAe,OAAO,OAAO,SAAS,MAAM;AAC7D;AAEO,IAAM,qBAAmC;EAC9C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACvBR,SAAU,OAAO,MAAqD;AAE1E,QAAM,EAAC,QAAQ,SAAAE,SAAO,IAAI;AAC1B,QAAM,EAAC,WAAW,GAAG,EAAC,IAAI;AAE1B,mBAAiB,CAAC,WAAW,GAAG,CAAC,GAAG,QAAQ;AAC5C,QAAM,gBAAgB,UAAU,MAAM;AAEtC,QAAM,SAASA,SAAQ,KAAK,IAAI,UAAU,MAAM,EAAE;AAClD,QAAM,UAAUA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,UAAUA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC3C,QAAM,cAAc,WAAW,EAAE,OAAO,EAAE,KAAK;AAC/C,QAAM,YACF,aAAK,oBAAoB,aAAK,cAAc,EAAE,KAAK,GAAG,WAAW;AAErE,MAAI,QAAQ;AACZ,QAAM,SACF,kBAAkB,KAAK,gBAAgB,KAAK,EAAE,MAAM,WAAW,IAC/D,IACA,aAAK,cAAc,EAAE,MAAM,MAAM,CAAC,CAAC;AAEvC,WAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;AACtC,aAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAC/B,UAAI,OAAO,CAAC,MAAM,GAAG;AACnB,kBAAU,OAAO,IAAI,QAAQ,CAAC;aACzB;AACL,kBAAU,OAAO,IAAI,QAAQ,CAAC;;;;AAKpC,SAAOA,SAAQ,eAAe,EAAE,OAAO,aAAa,SAAS;AAC/D;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACtCd,IAAM,aAAa,qBAAa;AAChC,IAAM,QAAQ,qBAAa;AAEpB,IAAMC,QAAO,gBAAgB,MAAM,CAAC,OAAM;AAC/C,MAAI,MAAM,GAAG;AACX,WAAO,QAAQ;SACV;AACL,WAAO,cAAc,KAAK,IAAI,EAAE,IAAI;;AAExC,CAAC;AAEM,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACdP,IAAMC,QAAO,gBAAgB,MAAM,CAAC,OAAM;AAC/C,MAAI,KAAK,GAAG;AACV,WAAO;aACE,KAAK,GAAG;AACjB,WAAO;SACF;AACL,WAAO;;AAEX,CAAC;AAEM,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACbP,IAAMC,OAAM,gBAAgB,KAAK,CAAC,OAAO,KAAK,IAAI,EAAE,CAAC;AAErD,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLP,IAAMC,QAAO,gBAAgB,MAAM,CAAC,OAAO,KAAK,KAAK,EAAE,CAAC;AAExD,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACAd,IAAM,UAAU;AAChB,IAAM,YAAY,KAAK,IAAI,OAAO,IAAI;AAE/B,IAAMC,YAAW,gBAAgB,UAAU,CAAC,OAAM;AAGvD,QAAM,WAAW,KAAK,CAAC;AAIvB,QAAM,WAAW,KAAK;AAEtB,QAAM,OAAO,KAAK,IAAI,EAAE;AACxB,MAAI;AAEJ,MAAI,UAAU;AACZ,aAAS;aACA,UAAU;AACnB,aAAS;SACJ;AACL,aAAS,KAAK,IAAI,IAAM,IAAI;;AAE9B,SAAO;AACT,CAAC;AAEM,IAAM,iBAA+B;EAC1C,YAAY;EACZ,aAAa;EACb,YAAYA;;;;AC5BR,SAAUC,gBAAe,MAI9B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,YAAY,SAAQ,IAAI;AAE/B,mBAAiB,CAAC,CAAC,GAAG,gBAAgB;AAEtC,QAAMC,QAAO,aAAK,cAAc,UAAU;AAE1C,QAAM,mBAA4C,CAAC,CAAC,GAAG,CAAC,CAAC;AACzD,mBAAiB,KAAK,GAAI,QAAoC;AAE9D,WAAS,IAAI,IAAI,WAAW,QAAQ,IAAI,EAAE,MAAM,QAAQ,EAAE,GAAG;AAC3D,qBAAiB,KAAK,CAAC,GAAG,CAAC,CAAC;;AAG9B,QAAM,UAAU,YAAY,WAAW;IACrC,QAAQ,EAAC,EAAC;IACV,SAAAD;IACA,OAAO,EAAC,UAAU,kBAAkB,eAAe,EAAC;GACrD;AAED,QAAM,sBACF,qBAAa,YAAY,QAAQ,OAAO,YAAYC,OAAM,KAAK;AAEnE,QAAM,oCAAoC,qBAAa,YACnD,oBAAoB,QAAQ,WAAW,QAAQ,KAAK;AAExD,QAAM,eACF,qBAAa,oBAAoB,QAAQ,OAAO,YAAYA,OAAM,KAAK;AAE3E,QAAM,gBAA+B,EAAC,GAAG,QAAO;AAChD,QAAM,eAA6B,EAAC,OAAO,oBAAmB;AAC9D,QAAM,kBACFC,SAAQ,EAAC,QAAQ,eAAe,SAAAF,UAAS,OAAO,aAAY,CAAC;AAEjE,QAAM,kBAAmC,EAAC,GAAG,gBAAe;AAC5D,QAAM,iBACe,EAAC,MAAM,kCAAiC;AAC7D,QAAM,WACFG,WAAU,EAAC,QAAQ,iBAAiB,SAAAH,UAAS,OAAO,eAAc,CAAC;AAEvE,QAAM,sBAAqC,EAAC,GAAG,SAAQ;AACvD,QAAM,qBAAmC,EAAC,OAAO,aAAY;AAC7D,QAAM,SAASE,SACX,EAAC,QAAQ,qBAAqB,SAAAF,UAAS,OAAO,mBAAkB,CAAC;AAErE,EAAAA,SAAQ,8BAA8B,OAAO;AAC7C,EAAAA,SAAQ,8BAA8B,eAAe;AACrD,EAAAA,SAAQ,8BAA8B,QAAQ;AAE9C,SAAO;AACT;AAEO,IAAM,uBAAqC;EAChD,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AChER,SAAU,oBAAoB,MAGnC;AACC,QAAM,EAAC,QAAQ,SAAAK,SAAO,IAAI;AAC1B,QAAM,EAAC,SAAS,QAAQ,YAAY,aAAY,IAAI;AACpD,MAAI,WAAW,MAAM,WAAW,GAAG;AACjC,UAAM,IAAI,MAAM;UACV,WAAW,KAAK,EAAE;;AAE1B,MAAI,QAAQ,MAAM,WAAW,GAAG;AAC9B,UAAM,IAAI,MAAM;UACV,QAAQ,KAAK,EAAE;;AAEvB,MAAI,OAAO,MAAM,WAAW,GAAG;AAC7B,UAAM,IAAI,MAAM;UACV,OAAO,KAAK,EAAE;;AAEtB,MAAI,aAAa,MAAM,WAAW,GAAG;AACnC,UAAM,IAAI,MAAM;UACV,aAAa,KAAK,EAAE;;AAG5B,QAAM,WAAWA,SAAQ,KAAK,IAAI,QAAQ,MAAM,EAAE;AAClD,QAAM,UAAUA,SAAQ,KAAK,IAAI,OAAO,MAAM,EAAE;AAChD,QAAM,cAAcA,SAAQ,KAAK,IAAI,WAAW,MAAM,EAAE;AACxD,QAAM,gBACFA,SAAQ,KAAK,IAAI,aAAa,MAAM,EAAE,OAAO,CAAC;AAElD,QAAM,CAAC,eAAe,oBAAoB,cACnC,mBAAmB,eAAe,IACrC,wBACI,UAAU,QAAQ,OAAO,QAAQ,OAAO,SAAS,OAAO,OACxD,aAAa,aAAa;AAClC,SAAO;IACLA,SAAQ,eAAe,oBAAoB,QAAQ,OAAO,aAAa;IACvEA,SAAQ,eACJ,CAAC,mBAAmB,CAAC,CAAC,GAAG,OAAO,OAAO,YAAY;IACvDA,SAAQ,eACJ,CAAC,kBAAkB,MAAM,GAAG,QAC5B,IAAI,WACA,kBAAkB,IAAI,CAAC,UAAmB,OAAO,KAAK,CAAC,CAAC,CAAC;IACjEA,SAAQ,eACJ,CAAC,gBAAgB,MAAM,GAAG,QAAQ,OAClC,IAAI,WAAW,eAAe,CAAC;;AAEvC;AAEO,IAAM,4BAA0C;EACrD,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACnDR,SAAU,cACZ,MAA4D;AAE9D,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,cAAc,YAAY,SAAQ,IAAI;AAC7C,MAAI,aAAa,MAAM,WAAW,GAAG;AACnC,UAAM,IAAI,MAAM;UACV,aAAa,KAAK,EAAE;;AAE5B,MAAI,WAAW,MAAM,WAAW,GAAG;AACjC,UAAM,IAAI,MAAM;UACV,WAAW,KAAK,EAAE;;AAG1B,MAAI,SAAS,MAAM,WAAW,GAAG;AAC/B,UAAM,IAAI,MACN,sDAAsD,SAAS,KAAK,EAAE;;AAG5E,QAAM,cACF,MAAM,KAAKA,SAAQ,KAAK,IAAI,WAAW,MAAM,EAAE,MAAoB;AACvE,QAAM,gBACFA,SAAQ,KAAK,IAAI,aAAa,MAAM,EAAE;AAC1C,QAAM,cACF,MAAM,KAAKA,SAAQ,KAAK,IAAI,SAAS,MAAM,EAAE,MAAoB;AAErE,QAAM,CAAC,YAAY,cAAc,WAAW,IAAI,kBAC5C,eAAe,aAAa,OAAO,aAAa,OAAO,aACvD,WAAW;AACf,SAAO;IACLA,SAAQ,eAAe,cAAc,aAAa,OAAO,UAAU;IACnEA,SAAQ,eACJ,CAAC,YAAY,MAAM,GAAG,SAAS,OAAO,IAAI,WAAW,WAAW,CAAC;;AAEzE;AAEO,IAAM,sBAAoC;EAC/C,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACvCR,SAAU,kBACZ,MAAgE;AAElE,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,MAAM,SAAS,WAAU,IAAI;AACpC,MAAI,KAAK,MAAM,SAAS,GAAG;AACzB,UAAM,IAAI,MACN,2DAA2D;;AAEjE,MAAI,QAAQ,MAAM,WAAW,GAAG;AAC9B,UAAM,IAAI,MAAM;YACR,QAAQ,KAAK,EAAE;;AAEzB,MAAI,WAAW,MAAM,WAAW,GAAG;AACjC,UAAM,IAAI,MAAM;YACR,WAAW,KAAK,EAAE;;AAE5B,MAAI,QAAQ,MAAM,CAAC,MAAM,WAAW,MAAM,CAAC,GAAG;AAC5C,UAAM,IAAI,MAAM,+CAA+C;;AAGjE,QAAM,QAAQA,SAAQ,KAAK,IAAI,KAAK,MAAM,EAAE;AAC5C,QAAM,WAAWA,SAAQ,KAAK,IAAI,QAAQ,MAAM,EAAE;AAClD,QAAM,cAAcA,SAAQ,KAAK,IAAI,WAAW,MAAM,EAAE;AAExD,QAAM,CAAC,YAAY,eAAe,IAAI,2BAClC,OAAO,KAAK,OAAO,KAAK,OAAO,UAAU,aAAa,IAAI;AAC9D,SAAOA,SAAQ,eAAe,iBAAiB,KAAK,OAAO,UAAU;AACvE;AAEO,IAAM,0BAAwC;EACnD,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACjCR,SAAU,iBACZ,MAA+D;AAEjE,QAAM,EAAC,QAAQ,SAAAC,SAAO,IAAI;AAC1B,QAAM,EAAC,MAAM,SAAS,WAAU,IAAI;AACpC,MAAI,KAAK,MAAM,SAAS,GAAG;AACzB,UAAM,IAAI,MACN,2DAA2D;;AAEjE,MAAI,QAAQ,MAAM,WAAW,GAAG;AAC9B,UAAM,IAAI,MAAM;WACT,QAAQ,KAAK,EAAE;;AAExB,MAAI,WAAW,MAAM,WAAW,GAAG;AACjC,UAAM,IAAI,MAAM;WACT,WAAW,KAAK,EAAE;;AAE3B,MAAI,QAAQ,MAAM,CAAC,MAAM,WAAW,MAAM,CAAC,GAAG;AAC5C,UAAM,IAAI,MAAM,+CAA+C;;AAGjE,QAAM,QAAQA,SAAQ,KAAK,IAAI,KAAK,MAAM,EAAE;AAC5C,QAAM,WAAWA,SAAQ,KAAK,IAAI,QAAQ,MAAM,EAAE;AAClD,QAAM,cAAcA,SAAQ,KAAK,IAAI,WAAW,MAAM,EAAE;AAExD,QAAM,CAAC,YAAY,eAAe,IAAI,2BAClC,OAAO,KAAK,OAAO,KAAK,OAAO,UAAU,WAAW;AACxD,SAAOA,SAAQ,eAAe,iBAAiB,KAAK,OAAO,UAAU;AACvE;AAEO,IAAM,yBAAuC;EAClD,YAAY;EACZ,aAAa;EACb,YAAY;;;;AClCR,SAAUC,eAAc,MAI7B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,eAAe,cAAc,aAAY,IAAI;AACpD,QAAM,EAAC,YAAW,IAAI;AAEtB,QAAM,EAAC,WAAW,YAAY,WAAW,SAAS,WAAU,IACxD,qBAAa,gBAAgB,cAAc,eAAe,WAAW;AACzE,QAAM,iBAAiB;AAEvB,QAAM,aAAaA,SAAQ,WAA0B,aAAa;AAElE,MAAI;AACJ,UAAQ,aAAa,OAAO;IAC1B,KAAK,QAAQ;AACX,YAAM,aAAaA,SAAQ,WAAyB,YAAY;AAChE,YAAM,gBACF,QAAQA,SAAQ,KAAK,IAAI,aAAa,MAAM,EAAE,OAAO,CAAC,CAAC;AAC3D,eAAS,YACL,YAAY,YAAY,aAAa,YAAY,WACjD,YAAY,WAAW,SAAS,eAAe,cAAc;AACjE;;IAEF,KAAK,WAAW;AACd,YAAM,aAAaA,SAAQ,WAA4B,YAAY;AACnE,YAAM,gBACFA,SAAQ,KAAK,IAAI,aAAa,MAAM,EAAE,OAAO,CAAC;AAClD,eAAS,YACL,YAAY,YAAY,aAAa,YAAY,WACjD,YAAY,WAAW,SAAS,eAAe,cAAc;AACjE;;IAEF,KAAK,SAAS;AACZ,YAAM,aAAaA,SAAQ,WAA0B,YAAY;AACjE,YAAM,gBACFA,SAAQ,KAAK,IAAI,aAAa,MAAM,EAAE,OAAO,CAAC;AAClD,eAAS,YACL,YAAY,YAAY,aAAa,YAAY,WACjD,YAAY,WAAW,SAAS,eAAe,cAAc;AACjE;;IAEF,KAAK,UAAU;AACb,YAAM,aAAaA,SAAQ,WAA2B,YAAY;AAClE,YAAM,gBAAgB,aAAK,aACvBA,SAAQ,KAAK,IAAI,aAAa,MAAM,EAAE,OAAO,CAAC,CAAe;AACjE,eAAS,YACL,YAAY,YAAY,aAAa,YAAY,WACjD,YAAY,WAAW,SAAS,eAAe,cAAc;AACjE;;IAEF;AACE,YAAM,IAAI,MAAM,oBAAoB,aAAa,KAAK,EAAE;;AAE5D,SAAOA,SAAQ,eAAe,aAAa,OAAO,OAAO,OAAO,MAAM;AACxE;AAEO,IAAM,sBAAoC;EAC/C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AC7DR,SAAU,OACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,iBAAiB,KAAI,IAAI;AAEhC,QAAM,QAAQ,aAAK,eAAe,MAAM,EAAE,KAAK,EAAE,CAAC;AAClD,QAAM,aAAa,qBAAa,iBAAiB,GAAG,iBAAiB,KAAK;AAE1E,QAAM,QAAQ,IAAI,MAAM,EAAE,MAAM,MAAM,EAAE,KAAK,CAAC;AAC9C,QAAM,OAAO,EAAE,MAAM,MAAK;AAC1B,SAAO,WAAW,IAAI,OAAI;AACxB,UAAM,YAAY,CAAC,GAAG,IAAI;AAC1B,cAAU,KAAK,IAAI;AACnB,UAAM,SACFC,OAAM,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,OAAO,MAAM,UAAS,EAAC,CAAC;AACjE,UAAM,KAAK,KAAK;AAChB,WAAO;EACT,CAAC;AACH;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC1BP,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAY,CAAC,EAAC,QAAQ,SAAAE,SAAO,MAAK;AAChC,UAAM,EAAC,EAAC,IAAI;AACZ,UAAM,aAAaA;AACnB,qBAAiB,GAAG,QAAQ;AAE5B,UAAM,SAAS,WAAW,KAAK,IAAI,EAAE,MAAM,EAAE;AAC7C,UAAM,YAAY,IAAI,aAAa,OAAO,MAAM;AAChD,aAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,EAAE,GAAG;AACtC,YAAM,QAAQ,OAAO,CAAC;AACtB,gBAAU,CAAC,IAAI,QAAQ;;AAEzB,UAAM,SAAS,WAAW,MAAM,WAAW,EAAE,OAAO,EAAE,KAAK;AAC3D,WAAO,EAAC,QAAQ,OAAO,EAAE,OAAO,OAAO,EAAE,MAAK;EAChD;;;;ACjBK,IAAMC,QAAO,gBAAgB,MAAM,CAAC,IAAI,UAAS;AACtD,QAAM,YAAY;AAClB,MAAI,MAAM,EAAE,GAAG;AACb,WAAO;SACF;AACL,WAAO,KAAK,IAAI,IAAI,UAAU;;AAElC,CAAC;AAEM,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACRR,SAAUC,cAAa,MAI5B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EACJ,OACA,KACA,SACA,WACA,SACA,cACA,aACA,eAAc,IACZ;AAEJ,mBAAiB,GAAG,cAAc;AAElC,QAAM,EACJ,kBACA,YACA,YACA,WACA,eACA,OAAO,QACP,KAAK,MACL,SAAS,SAAQ,IAEf,mBAAW,UACP,EAAE,OAAO,OAAO,KAAK,SAAS,WAAW,SAAS,cAClD,aAAa,cAAc;AAEnC,MAAI;AAIJ,MAAI,YAAY;AAEd,aAASC,SAAQ,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAD,UAAS,OAAO,EAAC,OAAO,WAAU,EAAC,CAAC;aAC1D,aAAa,eAAe;AAErC,iBAAK,OACD,EAAE,MAAM,UAAU,GAClB,MAAM,yCAAyC,EAAE,MAAM,MAAM,EAAE;AAEnE,UAAM,OAAO,mBAAW,gBAAgB,QAAQ,MAAM,QAAQ;AAE9D,UAAM,SAASE,OAAM,EAAC,QAAQ,EAAC,EAAC,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,QAAQ,KAAI,EAAC,CAAC;AACzE,aACIC,SAAQ,EAAC,QAAQ,EAAC,GAAG,OAAM,GAAG,SAAAD,UAAS,OAAO,EAAC,OAAO,WAAU,EAAC,CAAC;AACtE,IAAAA,SAAQ,8BAA8B,MAAM;SACvC;AACL,UAAM,OAAOA,SAAQ,WAA4B,CAAC;AAClD,UAAM,SAAS,iBAAiB,kBAAkB,MAAM,UAAU,MAAM;AAExE,aAASA,SAAQ,eAAe,YAAY,OAAO,OAAO,OAAO,MAAM;;AAGzE,SAAO;AACT;AAEO,IAAM,qBAAmC;EAC9C,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACpER,SAAU,aAAa,MAI5B;AACC,QAAM,EAAC,QAAQ,SAAAI,UAAS,MAAK,IAAI;AACjC,QAAM,EACJ,WACA,aACA,SACA,UACA,UACA,uBAAsB,IACpB;AACJ,QAAM,EAAC,MAAM,WAAU,IAAI;AAC3B,QAAM,QAAQA,SAAQ,KAAK,IAAI,KAAK,MAAM,EAAE;AAC5C,QAAM,cAAcA,SAAQ,KAAK,IAAI,WAAW,MAAM,EAAE;AAExD,QAAM,CAAC,QAAQ,YAAY,IAAI,iBAC3B,OAAO,aAAa,WAAW,aAAa,SAAS,UAAU,UAC/D,sBAAsB;AAC1B,SAAO;IACLA,SAAQ,eAAe,CAAC,OAAO,MAAM,GAAG,UAAU,MAAM;IACxDA,SAAQ,eAAe,WAAW,OAAO,SAAS,YAAY;;AAElE;AAEO,IAAM,qBAAmC;EAC9C,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC9BR,SAAU,YAAY,MAI3B;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,UAAS,IAAI;AACpB,QAAM,EAAC,OAAAC,QAAO,UAAS,IAAI;AAE3B,MAAIA,OAAM,UAAU,UAAU;AAC5B,UAAM,IAAI,MAAM,kCAAkC;;AAEpD,MAAIA,OAAM,MAAM,WAAW,GAAG;AAC5B,UAAM,IAAI,MAAM,sCAAsCA,OAAM,KAAK,EAAE;;AAErE,MAAI,UAAU,MAAM,WAAW,GAAG;AAChC,UAAM,IAAI,MACN,0CAA0C,UAAU,KAAK,EAAE;;AAGjE,QAAM,SAASD,SAAQ,KAAK,IAAIC,OAAM,MAAM,EAAE;AAC9C,QAAM,aAAaD,SAAQ,KAAK,IAAI,UAAU,MAAM,EAAE,OAAO,CAAC;AAE9D,QAAM,CAAC,SAAS,QAAQ,KAAK,IACzB,gBAAgB,QAAQ,YAAY,SAAS;AACjD,QAAM,aAAa,OAAO;AAC1B,SAAO;IACLA,SAAQ,eAAe,CAAC,YAAY,CAAC,GAAG,SAAS,OAAO;IACxDA,SAAQ,eAAe,CAAC,UAAU,GAAG,UAAU,MAAM;IACrDA,SAAQ,eAAe,CAAC,CAAC,GAAG,SAAS,IAAI,WAAW,KAAK,CAAC;;AAE9D;AAEO,IAAM,oBAAkC;EAC7C,YAAY;EACZ,aAAa;EACb,YAAY;;;;ACpCR,SAAU,uBAAuB,MAItC;AACC,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,WAAU,IAAI;AACrB,QAAM,EAAC,OAAAC,OAAK,IAAI;AAEhB,MAAIA,OAAM,UAAU,UAAU;AAC5B,UAAM,IAAI,MAAM,kCAAkC;;AAEpD,MAAI,cAAc,GAAG;AACnB,UAAM,IAAI,MAAM,sCAAsC;;AAGxD,QAAM,SAASD,SAAQ,KAAK,IAAIC,OAAM,MAAM,EAAE;AAE9C,QAAM,SAAS,2BAA2B,QAAQ,UAAU;AAC5D,SAAOD,SAAQ,eAAeC,OAAM,OAAO,SAAS,MAAM;AAC5D;AAEO,IAAM,+BAA6C;EACxD,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC3BP,IAAMC,OAAM,gBAAgB,KAAK,CAAC,OAAO,KAAK,IAAI,EAAE,CAAC;AAErD,IAAM,YAA0B;EACrC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACLP,IAAMC,QAAO,gBAAgB,MAAM,CAAC,OAAO,KAAK,KAAK,EAAE,CAAC;AAExD,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYA;;;;ACHR,SAAUC,MACZ,MAAqE;AAEvE,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,KAAI,IAAI;AAEf,mBAAiB,GAAG,MAAM;AAC1B,QAAM,SAAS,SAASA,SAAQ,WAAW,CAAC,GAAG,IAAI;AAEnD,SAAOA,SAAQ,eAAe,OAAO,OAAO,OAAO,OAAO,OAAO,MAAM;AACzE;AAEO,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;AChBR,SAAU,KACZ,MAAqE;AAEvE,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,EAAC,IAAI;AACZ,QAAM,EAAC,GAAG,OAAM,IAAI;AAEpB,mBAAiB,GAAG,MAAM;AAE1B,QAAM,QAAQA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AACzC,QAAM,CAAC,aAAa,cAAc,IAC9B,SAAS,OAAO,EAAE,OAAO,EAAE,OAA0B,GAAG,MAAM;AAElE,SAAO;IACLA,SAAQ,eACJ,YAAY,OAAO,YAAY,OAAO,YAAY,MAAM;IAC5DA,SAAQ,eACJ,eAAe,OAAO,eAAe,OAAO,eAAe,MAAM;;AAEzE;AAEO,IAAM,aAA2B;EACtC,YAAY;EACZ,aAAa;EACb,YAAY;;;;AC1BR,SAAU,UAAU,MAIzB;AACC,QAAM,EAAC,QAAQ,OAAO,SAAAC,SAAO,IAAI;AACjC,QAAM,EAAC,OAAAC,QAAO,WAAU,IAAI;AAC5B,QAAM,EAAC,eAAe,UAAU,WAAW,YAAW,IAAI;AAE1D,QAAM,CAAC,OAAO,aAAa,YAAY,WAAW,IAAIA,OAAM;AAC5D,QAAM,CAAC,WAAW,QAAQ,IACtB,eAAe,OAAO,cAAc,CAAC,aAAa,UAAU;AAChE,QAAM,WAAW,CAAC,OAAO,WAAW,UAAU,WAAW;AAEzD,QAAM,YAAY,aAAK,eAAeA,OAAM,KAAK;AACjD,QAAM,gBAAgB,UAAU,CAAC;AACjC,QAAM,cAAc,UAAU,CAAC;AAC/B,QAAM,cAAc,UAAU,CAAC;AAE/B,QAAM,aAAa,aAAK,eAAe,QAAQ;AAC/C,QAAM,iBAAiB,WAAW,CAAC;AACnC,QAAM,eAAe,WAAW,CAAC;AACjC,QAAM,eAAe,WAAW,CAAC;AAEjC,QAAM,UAAU,aAAK,uBACjBA,OAAM,OAA0B,aAAK,cAAc,QAAQ,CAAC;AAEhE,UAAQ,KAAK,SAAS;AAEtB,QAAM,YAAYD,SAAQ,KAAK,IAAIC,OAAM,MAAM,EAAE;AACjD,QAAM,gBACFD,SAAQ,KAAK,IAAI,WAAW,MAAM,EAAE;AAIxC,WAAS,IAAI,GAAG,IAAI,OAAO,EAAE,GAAG;AAC9B,UAAME,aAAY,WAAW,MAAM,CAAC,MAAM,IACtC,gBACA,cAAc,SAAS,IAAI,GAAG,IAAI,IAAI,CAAC;AAE3C,aAAS,OAAO,GAAG,OAAO,WAAW,EAAE,MAAM;AAC3C,eAAS,OAAO,GAAG,OAAO,UAAU,EAAE,MAAM;AAC1C,iBAAS,UAAU,GAAG,UAAU,aAAa,EAAE,SAAS;AACtD,cAAI;AAEJ,gBAAM,aAAaA,WAAU,CAAC,IAAI,OAAOA,WAAU,CAAC,IAAI,OAAO;AAE/D,cAAI,eAAe,GAAG;AAGpB;;AAGF,gBAAM,OACDA,WAAU,CAAC,IAAI,OAAOA,WAAU,CAAC,IAAI,OAAOA,WAAU,CAAC,KACxD;AACJ,gBAAM,OACDA,WAAU,CAAC,IAAI,OAAOA,WAAU,CAAC,IAAI,OAAOA,WAAU,CAAC,KACxD;AAEJ,gBAAM,IAAI,SAAS,KAAK,YAAY,QAAQ;AAC5C,gBAAM,IAAI,SAAS,KAAK,aAAa,QAAQ;AAE7C,kBAAQ,eAAe;YACrB,KAAK;AACH,oBAAM,qBACF,WAAW,aAAa,YAAY,eACpC,aAAa,aAAa,GAAG,GAAG,GAAG,SAAS,SAAS;AACzD;YACF,KAAK;AACH,oBAAM,sBACF,WAAW,aAAa,YAAY,eACpC,aAAa,aAAa,GAAG,GAAG,GAAG,SAAS,SAAS;AACzD;YACF;AACE,oBAAM,IAAI,MACN,+DACuB,aAAa,EAAE;;AAG9C,gBAAM,MACF,IAAI,iBAAiB,OAAO,eAC5B,OAAO,eAAe;AAE1B,kBAAQ,GAAG,IAAI;;;;AAKrB,WAAOF,SAAQ,eAAe,UAAUC,OAAM,OAAO,OAAO;;AAG9D,QAAM,SAASD,SAAQ,MAAM,SAAS,UAAUC,OAAM,KAAK;AAC3D,SAAO,EAAC,QAAQ,OAAOA,OAAM,OAAO,OAAOA,OAAM,MAAK;AACxD;AAEO,IAAM,kBAAgC;EAC3C,YAAY;EACZ,aAAa;EACb,YAAY;;AAGd,SAAS,SACL,UAAkB,KAClB,MAA2C;AAC7C,UAAQ,MAAM;IACZ,KAAK;AACH,aAAO,gBAAgB,UAAU,GAAG;IACtC,KAAK;AACH,aAAO,aAAa,UAAU,GAAG;IACnC,KAAK;AACH,aAAO,gBAAgB,UAAU,GAAG;IACtC,KAAK;IACL;AACE,aAAO,iBAAiB,UAAU,GAAG;;AAE3C;AAEA,SAAS,gBAAgB,UAAkB,KAAW;AAEpD,MAAI,UAAU;AACd,MAAI,UAAU,GAAG;AACf,QAAI,OAAO,GAAG;AACZ,gBAAU;WACL;AACL,YAAM,MAAM,IAAI;AAChB,UAAI,UAAU,KAAK;AACjB,kBAAU,MAAM,KAAK,MAAM,CAAC,UAAU,GAAG,IAAI;;AAE/C,gBAAU,UAAU,CAAC,MAAM,UAAU,MAAM,CAAC,UAAU;;aAE/C,UAAU,MAAM,GAAG;AAC5B,QAAI,OAAO,GAAG;AACZ,gBAAU;WACL;AACL,YAAM,MAAM,IAAI;AAChB,iBAAW,MAAM,KAAK,MAAM,UAAU,GAAG;AACzC,UAAI,WAAW,KAAK;AAClB,kBAAU,MAAM,UAAU;;;;AAMhC,SAAO,aAAK,MAAM,GAAG,SAAS,MAAM,CAAC;AACvC;AAEA,SAAS,aAAa,UAAkB,KAAW;AAEjD,MAAI,UAAU;AACd,MAAI,UAAU,GAAG;AACf,QAAI,OAAO,GAAG;AACZ,gBAAU;WACL;AACL,YAAM,KAAK,MAAM;AACjB,iBAAW,OAAO,KAAK,MAAM,CAAC,UAAU,EAAE,IAAI;;aAEvC,UAAU,MAAM,GAAG;AAC5B,QAAI,OAAO,GAAG;AACZ,gBAAU;WACL;AACL,YAAM,KAAK,MAAM;AACjB,iBAAW,MAAM,KAAK,MAAM,UAAU,EAAE;;;AAK5C,SAAO,aAAK,MAAM,GAAG,SAAS,MAAM,CAAC;AACvC;AAEA,SAAS,iBAAiB,UAAkB,KAAW;AACrD,SAAO;AACT;AAEA,SAAS,gBAAgB,UAAkB,KAAW;AACpD,SAAO,aAAK,MAAM,GAAG,UAAU,MAAM,CAAC;AACxC;AAEA,SAAS,kBACL,WAAuB,aAAqB,YAC5C,aAAqB,WAAmB,WAAmB,OAC3D,GAAW,GAAW,SAAiB,WAAiB;AAC1D,QAAM,MAAM,QAAQ,cAAc,IAAI,YAAY,IAAI,YAAY;AAClE,MAAI,KAAK,KAAK,IAAI,eAAe,KAAK,KAAK,IAAI,YAAY;AACzD,WAAO,UAAU,GAAG;SACf;AACL,WAAO;;AAEX;AAEA,SAAS,qBACL,WAAuB,aAAqB,YAC5C,aAAqB,WAAmB,WAAmB,OAC3D,GAAW,GAAW,SAAiB,WAAiB;AAC1D,QAAM,KAAK,KAAK,MAAM,CAAC;AACvB,QAAM,KAAK,KAAK,MAAM,CAAC;AAEvB,SAAO,kBACH,WAAW,aAAa,YAAY,aAAa,WAAW,WAC5D,OAAO,IAAI,IAAI,SAAS,SAAS;AACvC;AAEA,SAAS,sBACL,WAAuB,aAAqB,YAC5C,aAAqB,WAAmB,WAAmB,OAC3D,GAAW,GAAW,SAAiB,WAAiB;AAC1D,QAAM,SAAS,KAAK,MAAM,CAAC;AAC3B,QAAM,SAAS,KAAK,MAAM,CAAC;AAC3B,QAAM,QAAQ,SAAS;AACvB,QAAM,QAAQ,SAAS;AAGvB,QAAM,eACD,QAAQ,KACL,kBACI,WAAW,aAAa,YAAY,aAAa,WACjD,WAAW,OAAO,QAAQ,QAAQ,SAAS,SAAS,KAC3D,IAAI,UACD,kBACI,WAAW,aAAa,YAAY,aAAa,WACjD,WAAW,OAAO,QAAQ,OAAO,SAAS,SAAS;AAG/D,QAAM,cACD,QAAQ,KACL,kBACI,WAAW,aAAa,YAAY,aAAa,WACjD,WAAW,OAAO,OAAO,QAAQ,SAAS,SAAS,KAC1D,IAAI,UACD,kBACI,WAAW,aAAa,YAAY,aAAa,WACjD,WAAW,OAAO,OAAO,OAAO,SAAS,SAAS;AAG9D,UAAQ,QAAQ,KAAK,eAAe,IAAI,UAAU;AACpD;;;ACxOM,SAAUE,QACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,OAAO,SAAAC,SAAO,IAAI;AACjC,QAAM,EAAC,KAAI,IAAI;AACf,QAAM,EAAC,EAAC,IAAI;AACZ,mBAAiB,GAAG,QAAQ;AAE5B,QAAM,SAASA,SAAQ,KAAK,IAAI,EAAE,MAAM,EAAE;AAC1C,QAAM,EAAC,cAAc,aAAa,QAAO,IACrC,WAAW,QAAQ,MAAM,EAAE,OAAO,EAAE,KAAK;AAC7C,SAAO;IACLA,SAAQ,eAAe,aAAa,EAAE,OAAO,YAAY;IACzDA,SAAQ,eAAe,CAAC,QAAQ,MAAM,GAAG,SAAS,OAAO;;AAE7D;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACrBR,SAAU,OACZ,MAAyE;AAE3E,QAAM,EAAC,QAAQ,SAAAE,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,MAAK,IAAI;AAChB,MAAI,EAAC,KAAI,IAAI;AAEb,MAAI,OAAO,GAAG;AACZ,YAAQ,MAAM,MAAM;;AAGtB,QAAM,YAAY,MAAM,MAAM;AAE9B,QAAM,MAAM,MAAM,MAAM,IAAI;AAC5B,QAAM,WAAqB,IAAI,MAAM,YAAY,CAAC;AAClD,MAAI,WAAW;AACf,WAAS,IAAI,GAAG,IAAI,WAAW,KAAK;AAClC,QAAI,MAAM,MAAM;AACd,eAAS,UAAU,IAAI,MAAM,MAAM,CAAC;;;AAIxC,QAAM,QAAQ,IAAI,MAAM,SAAS,EAAE,KAAK,CAAC;AACzC,QAAM,OAAO,MAAM,MAAM,MAAK;AAC9B,OAAK,IAAI,IAAI;AACb,QAAM,MAAM,IAAI,MAAM,GAAG;AACzB,WAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK;AACnC,UAAM,IAAI,IAAI;AACd,UAAM,UAAUC,OAAM,EAAC,QAAQ,EAAC,GAAG,MAAK,GAAG,SAAAD,UAAS,OAAO,EAAC,OAAO,KAAI,EAAC,CAAC;AACzE,QAAI,CAAC,IAAIE,SAAQ,EAAC,QAAQ,EAAC,GAAG,QAAO,GAAG,SAAAF,UAAS,OAAO,EAAC,OAAO,SAAQ,EAAC,CAAC;AAC1E,IAAAA,SAAQ,8BAA8B,OAAO;;AAG/C,SAAO;AACT;AAEO,IAAM,eAA6B;EACxC,YAAY;EACZ,aAAa;EACb,YAAY;;;;AClCR,SAAUG,oBAAmB,MAIlC;AACC,QAAM,EAAC,QAAQ,SAAAC,UAAS,MAAK,IAAI;AACjC,QAAM,EAAC,GAAG,WAAU,IAAI;AACxB,QAAM,EAAC,YAAW,IAAI;AAEtB,mBAAiB,GAAG,oBAAoB;AAExC,QAAM,QAAQ,EAAE,MAAM;AACtB,QAAM,iBAAiB,WAAW,MAAM;AACxC,QAAM,MAAM,CAAA;AACZ,QAAM,gBAA8B,CAAA;AAIpC,QAAM,WAAW,QAAQ;AACzB,MAAI,cAAc;AAElB,WAAS,IAAI,GAAG,IAAI,UAAU,EAAE,GAAG;AACjC,UAAM,WAAWC,YACb,EAAC,QAAQ,EAAC,OAAO,YAAW,GAAG,SAAAD,UAAS,OAAO,EAAC,KAAK,IAAI,EAAC,EAAC,CAAC;AAChE,kBAAc;AACd,kBAAc,KAAK,QAAQ;;AAG7B,WAAS,IAAI,GAAG,IAAI,aAAa,EAAE,GAAG;AACpC,UAAM,cAAc,aAAK,kBAAkB,GAAoB,OAAO;AACtE,UAAM,YAAYA,SAAQ,eAAe,CAAA,GAAI,SAAS,WAAW;AACjE,UAAM,OACFE,OAAM,EAAC,QAAQ,EAAC,GAAG,WAAW,GAAG,YAAW,GAAG,SAAAF,SAAO,CAAC;AAC3D,UAAM,aACFG,MAAK,EAAC,QAAQ,EAAC,GAAG,KAAI,GAAG,SAAAH,UAAS,OAAO,EAAC,OAAO,UAAS,EAAC,CAAC;AAChE,UAAMI,OACF,SAAS,EAAC,QAAQ,EAAC,GAAG,YAAY,GAAG,EAAC,GAAG,SAAAJ,SAAO,CAAC;AACrD,UAAM,gBACFK,KAAI,EAAC,QAAQ,EAAC,GAAGD,KAAG,GAAG,SAAAJ,UAAS,OAAO,EAAC,MAAM,GAAG,UAAU,MAAK,EAAC,CAAC;AACtE,QAAI,KAAK,aAAa;AACtB,kBAAc,KAAK,SAAS;AAC5B,kBAAc,KAAK,IAAI;AACvB,kBAAc,KAAK,UAAU;AAC7B,kBAAc,KAAKI,IAAG;AACtB,kBAAc,KAAK,aAAa;;AAGlC,QAAM,SAAS,KAAK,EAAC,QAAQ,KAAK,SAAAJ,UAAS,OAAO,EAAC,MAAM,EAAC,EAAC,CAAC;AAE5D,gBAAc,QAAQ,OAAKA,SAAQ,8BAA8B,CAAC,CAAC;AAEnE,SAAO;AACT;AAEO,IAAM,2BAAyC;EACpD,YAAY;EACZ,aAAa;EACb,YAAYD;;;;ACyGd,IAAM,gBAAgC;EACpC;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;AAGF,WAAW,gBAAgB,eAAe;AACxC,iBAAe,YAAY;;;;ACtW7B,IAAMO,WAAU;;;ACyCT,IAAMC,WAAU;EACrB,aAAa;EACb,oBAAoBA;EACpB,sBAAsBA;EACtB,aAAaA;EACb,eAAeA;EACf,kBAAkBA;EAClB,QAAQA;;",
  "names": ["pad", "mean", "scale", "min", "max", "pad", "pad", "pad", "reverse", "reverse", "pad", "pad", "isNaN", "pad", "exp", "pad", "version", "elu", "leakyRelu", "backend", "prelu", "backend", "relu", "relu6", "backend", "relu", "elu", "relu6", "prelu", "leakyRelu", "sigmoid", "reshape", "backend", "real", "imag", "backend", "reshape", "sum", "backend", "add", "acos", "acosh", "addN", "backend", "all", "backend", "transpose", "reshape", "any", "backend", "transpose", "reshape", "argMax", "backend", "transpose", "max", "argMin", "backend", "transpose", "min", "asin", "asinh", "atan", "atan2", "atanh", "pool", "avgPool", "backend", "pad", "strides", "buffer", "pool", "backend", "pad", "backend", "input", "pad", "backend", "input", "pad", "batchNorm", "backend", "scale", "mean", "batchToSpaceND", "backend", "prod", "reshape", "transpose", "slice", "bincount", "backend", "broadcastArgs", "backend", "clipByValue", "real", "imag", "imag", "backend", "input", "concat", "backend", "real", "imag", "complex", "reshape", "backend", "pad", "backend", "pad", "backend", "pad", "backend", "pad", "backend", "pad", "backend", "pad", "cos", "cosh", "backend", "image", "cumprod", "backend", "reverse", "transpose", "cumsum", "backend", "reverse", "transpose", "denseBincount", "backend", "depthToSpace", "backend", "backend", "pad", "backend", "pad", "backend", "pad", "diag", "backend", "backend", "pad", "backend", "pad", "backend", "pad", "sum", "backend", "cast", "transpose", "zeros", "reshape", "einsum", "backend", "transpose", "reshape", "sum", "backend", "erf", "sign", "expandDims", "backend", "input", "reshape", "div", "input", "slice", "complex", "real", "imag", "add", "sub", "concat", "fft", "backend", "input", "reshape", "fill", "backend", "backend", "image", "floorDiv", "backend", "pad", "reshape", "add", "backend", "pad", "add", "backend", "backend", "reshape", "ifft", "backend", "input", "reshape", "isFinite", "isInf", "isNaN", "backend", "log1p", "logicalAnd", "logicalNot", "logicalOr", "backend", "sum", "backend", "norm", "max", "backend", "maxPool", "backend", "pad", "strides", "buffer", "pool", "backend", "pad", "backend", "input", "pad", "backend", "input", "pad", "pool", "backend", "pad", "mean", "backend", "cast", "div", "sum", "min", "backend", "transpose", "reshape", "mirrorPad", "backend", "p", "i", "mod", "softmax", "backend", "max", "reshape", "sub", "exp", "sum", "div", "multinomial", "backend", "softmax", "backend", "backend", "backend", "oneHot", "backend", "zerosLike", "backend", "real", "imag", "complex", "fill", "onesLike", "backend", "real", "imag", "zerosLike", "complex", "fill", "backend", "expandDims", "concat", "backend", "p", "i", "pow", "raggedGather", "backend", "raggedTensorToTensor", "backend", "range", "backend", "step", "reciprocal", "resizeBilinear", "backend", "backend", "resizeNearestNeighbor", "backend", "backend", "reverse", "backend", "backend", "image", "round", "backend", "lowerBound", "upperBound", "searchSorted", "backend", "backend", "selu", "sign", "sin", "sinh", "softplus", "spaceToBatchND", "backend", "prod", "reshape", "transpose", "backend", "backend", "backend", "backend", "sparseToDense", "backend", "backend", "slice", "backend", "step", "stridedSlice", "backend", "reshape", "slice", "backend", "backend", "input", "backend", "input", "tan", "tanh", "tile", "backend", "backend", "backend", "image", "transform", "unique", "backend", "backend", "slice", "reshape", "unsortedSegmentSum", "backend", "expandDims", "equal", "cast", "mul", "sum", "version", "version"]
}

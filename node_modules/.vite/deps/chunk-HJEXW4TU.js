import {
  Abs,
  Acos,
  Acosh,
  Add,
  AddN,
  ArgMax,
  ArgMin,
  Asin,
  Asinh,
  Atan,
  Atan2,
  Atanh,
  AvgPool,
  AvgPool3D,
  AvgPool3DGrad,
  AvgPoolGrad,
  BatchMatMul,
  BatchToSpaceND,
  BroadcastTo,
  Cast,
  Ceil,
  ClipByValue,
  ComplexAbs,
  Concat,
  Conv2D,
  Conv2DBackpropInput,
  Conv3D,
  Conv3DBackpropFilterV2,
  Cos,
  Cosh,
  Cumsum,
  DepthwiseConv2dNative,
  Dilation2D,
  Dilation2DBackpropFilter,
  Dilation2DBackpropInput,
  ENGINE,
  Elu,
  EluGrad,
  Erf,
  Exp,
  ExpandDims,
  Expm1,
  Floor,
  FloorDiv,
  FusedBatchNorm,
  GatherV2,
  GreaterEqual,
  Identity,
  IsFinite,
  IsInf,
  IsNan,
  LRN,
  LRNGrad,
  LeakyRelu,
  Log,
  Log1p,
  LogSoftmax,
  Max,
  MaxPool,
  MaxPool3D,
  MaxPool3DGrad,
  MaxPoolGrad,
  Maximum,
  Mean,
  Min,
  Minimum,
  MirrorPad,
  Mod,
  Multiply,
  Neg,
  OneHot,
  OnesLike,
  Optimizer,
  Pack,
  PadV2,
  Pow,
  Prelu,
  Prod,
  RealDiv,
  Reciprocal,
  Relu,
  Relu6,
  Reshape,
  ResizeBilinear,
  ResizeBilinearGrad,
  ResizeNearestNeighbor,
  ResizeNearestNeighborGrad,
  Reverse,
  Round,
  Rsqrt,
  SELU_SCALE,
  SELU_SCALEALPHA,
  Select,
  Selu,
  Sigmoid,
  Sign,
  Sin,
  Sinh,
  Slice,
  Softmax,
  Softplus,
  SpaceToBatchND,
  SplitV,
  Sqrt,
  Square,
  SquaredDifference,
  Step,
  Sub,
  Sum,
  Tan,
  Tanh,
  Tensor,
  Tile,
  Transpose,
  Unpack,
  UnsortedSegmentSum,
  ZerosLike,
  abs,
  add,
  all,
  any,
  argMax,
  assert,
  assertAndGetBroadcastShape,
  avgPool,
  avgPool3d,
  backend,
  backend_util_exports,
  batchNorm2d,
  batchNorm3d,
  batchNorm4d,
  batchToSpaceND,
  cast,
  checkPadOnDimRoundingMode,
  clipByValue,
  clone,
  computeOutAndReduceShapes,
  concat,
  concat1d,
  concat2d,
  concat3d,
  concat4d,
  conv1d,
  conv2DBackpropFilter,
  conv2DBackpropInput,
  conv2d,
  conv2dTranspose,
  conv3DBackpropInput,
  conv3d,
  conv3dTranspose,
  convertToTensor,
  cos,
  cosh,
  cumprod,
  cumsum,
  depthwiseConv2d,
  depthwiseConv2dNativeBackpropFilter,
  depthwiseConv2dNativeBackpropInput,
  dispose,
  div,
  dropout,
  eitherStridesOrDilationsAreOne,
  elu,
  env,
  equal,
  exp,
  expandDims,
  expandShapeToKeepDim,
  eye,
  fill,
  floor,
  fused_ops_exports,
  gather,
  getAxesPermutation,
  getReductionAxes,
  getUndoAxesPermutation,
  greater,
  greaterEqual,
  image,
  io_exports,
  keep,
  leakyRelu,
  less,
  lessEqual,
  linalg,
  log,
  log1p,
  logSoftmax,
  logicalAnd,
  logicalNot,
  matMul,
  max,
  maxPool,
  maxPool3d,
  maximum,
  mean,
  memory,
  minimum,
  moments,
  mul,
  neg,
  nextFrame,
  notEqual,
  oneHot,
  ones,
  onesLike,
  op,
  pad,
  parseAxisParam,
  parseSliceParams,
  pow,
  prelu,
  randomNormal,
  randomUniform,
  registerGradient,
  relu,
  reshape,
  reverse,
  rsqrt,
  scalar,
  selu,
  separableConv2d,
  serialization_exports,
  sigmoid,
  sin,
  sinh,
  sizeFromShape,
  slice,
  slice1d,
  slice2d,
  slice3d,
  slice4d,
  softmax,
  softplus,
  spaceToBatchND,
  split,
  sqrt,
  square,
  squeeze,
  stack,
  step,
  sub,
  sum,
  tanh,
  tensor1d,
  tidy,
  tile,
  train,
  transpose,
  truncatedNormal,
  tupleValuesAreOne,
  unsortedSegmentSum,
  unstack,
  util_exports,
  variable,
  where,
  zeros,
  zerosLike
} from "./chunk-SW6JV2BF.js";
import {
  __export
} from "./chunk-V4OQ3NZ2.js";

// node_modules/@tensorflow/tfjs-layers/dist/errors.js
var AttributeError = class _AttributeError extends Error {
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _AttributeError.prototype);
  }
};
var RuntimeError = class _RuntimeError extends Error {
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _RuntimeError.prototype);
  }
};
var ValueError = class _ValueError extends Error {
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _ValueError.prototype);
  }
};
var NotImplementedError = class _NotImplementedError extends Error {
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _NotImplementedError.prototype);
  }
};
var AssertionError = class _AssertionError extends Error {
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _AssertionError.prototype);
  }
};

// node_modules/@tensorflow/tfjs-layers/dist/utils/executor_utils.js
var LruCache = class {
  constructor(maxEntries) {
    this.maxEntries = maxEntries || 100;
    this.cache = /* @__PURE__ */ new Map();
  }
  /**
   * Get the entry for the key and mark it as used recently.
   */
  get(key) {
    let entry;
    if (this.cache.has(key)) {
      entry = this.cache.get(key);
      this.cache.delete(key);
      this.cache.set(key, entry);
    }
    return entry;
  }
  /**
   * Put the entry into the cache. If the key already existed, mark the key as
   * used recently.
   */
  put(key, value) {
    if (this.cache.has(key)) {
      this.cache.delete(key);
    } else if (this.cache.size >= this.maxEntries) {
      const keyToDelete = this.cache.keys().next().value;
      this.cache.delete(keyToDelete);
    }
    this.cache.set(key, value);
  }
  /**
   * Get the MaxEntries of the cache.
   */
  getMaxEntries() {
    return this.maxEntries;
  }
  /**
   * Set the MaxEntries of the cache. If the maxEntries is decreased, reduce
   * entries in the cache.
   */
  setMaxEntries(maxEntries) {
    if (maxEntries < 0) {
      throw new Error(`The maxEntries of LRU caches must be at least 0, but got ${maxEntries}.`);
    }
    if (this.maxEntries > maxEntries) {
      for (let i = 0; i < this.maxEntries - maxEntries; i++) {
        const keyToDelete = this.cache.keys().next().value;
        this.cache.delete(keyToDelete);
      }
    }
    this.maxEntries = maxEntries;
  }
};

// node_modules/@tensorflow/tfjs-layers/dist/utils/generic_utils.js
function pyListRepeat(value, numValues) {
  if (Array.isArray(value)) {
    let newArray = [];
    for (let i = 0; i < numValues; i++) {
      newArray = newArray.concat(value);
    }
    return newArray;
  } else {
    const newArray = new Array(numValues);
    newArray.fill(value);
    return newArray;
  }
}
function assert2(val, message) {
  if (!val) {
    throw new AssertionError(message);
  }
}
function count(array, refernce) {
  let counter = 0;
  for (const item of array) {
    if (item === refernce) {
      counter++;
    }
  }
  return counter;
}
function singletonOrArray(xs) {
  if (xs.length === 1) {
    return xs[0];
  }
  return xs;
}
function toList(x) {
  if (Array.isArray(x)) {
    return x;
  }
  return [x];
}
function toSnakeCase(name) {
  const intermediate = name.replace(/(.)([A-Z][a-z0-9]+)/g, "$1_$2");
  const insecure = intermediate.replace(/([a-z])([A-Z])/g, "$1_$2").toLowerCase();
  if (insecure[0] !== "_") {
    return insecure;
  }
  return "private" + insecure;
}
function toCamelCase(identifier) {
  if (identifier.length <= 1) {
    return identifier;
  }
  if (identifier.indexOf("_") === -1) {
    return identifier;
  }
  return identifier.replace(/[_]+(\w|$)/g, (m, p1) => p1.toUpperCase());
}
var _GLOBAL_CUSTOM_OBJECTS = {};
function serializeKerasObject(instance) {
  if (instance === null || instance === void 0) {
    return null;
  }
  const dict = {};
  dict["className"] = instance.getClassName();
  dict["config"] = instance.getConfig();
  return dict;
}
function convertNDArrayScalarsInConfig(config) {
  if (config == null || typeof config !== "object") {
    return;
  } else if (Array.isArray(config)) {
    config.forEach((configItem) => convertNDArrayScalarsInConfig(configItem));
  } else {
    const fields = Object.keys(config);
    for (const field of fields) {
      const value = config[field];
      if (value != null && typeof value === "object") {
        if (!Array.isArray(value) && value["type"] === "ndarray" && typeof value["value"] === "number") {
          config[field] = value["value"];
        } else {
          convertNDArrayScalarsInConfig(value);
        }
      }
    }
  }
}
function deserializeKerasObject(identifier, moduleObjects = {}, customObjects = {}, printableModuleName = "object", fastWeightInit = false) {
  if (typeof identifier === "string") {
    const functionName = identifier;
    let fn;
    if (functionName in customObjects) {
      fn = customObjects[functionName];
    } else if (functionName in _GLOBAL_CUSTOM_OBJECTS) {
      fn = _GLOBAL_CUSTOM_OBJECTS[functionName];
    } else {
      fn = moduleObjects[functionName];
      if (fn == null) {
        throw new ValueError(`Unknown ${printableModuleName}: ${identifier}. This may be due to one of the following reasons:
1. The ${printableModuleName} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.
2. The custom ${printableModuleName} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);
      }
    }
    return fn;
  } else {
    const config = identifier;
    if (config["className"] == null || config["config"] == null) {
      throw new ValueError(`${printableModuleName}: Improper config format: ${JSON.stringify(config)}.
'className' and 'config' must set.`);
    }
    const className = config["className"];
    let cls, fromConfig;
    if (className in customObjects) {
      [cls, fromConfig] = customObjects[className];
    } else if (className in _GLOBAL_CUSTOM_OBJECTS) {
      [cls, fromConfig] = _GLOBAL_CUSTOM_OBJECTS["className"];
    } else if (className in moduleObjects) {
      [cls, fromConfig] = moduleObjects[className];
    }
    if (cls == null) {
      throw new ValueError(`Unknown ${printableModuleName}: ${className}. This may be due to one of the following reasons:
1. The ${printableModuleName} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.
2. The custom ${printableModuleName} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);
    }
    if (fromConfig != null) {
      const customObjectsCombined = {};
      for (const key of Object.keys(_GLOBAL_CUSTOM_OBJECTS)) {
        customObjectsCombined[key] = _GLOBAL_CUSTOM_OBJECTS[key];
      }
      for (const key of Object.keys(customObjects)) {
        customObjectsCombined[key] = customObjects[key];
      }
      const nestedConfig = config["config"];
      nestedConfig["customObjects"] = customObjectsCombined;
      const backupCustomObjects = Object.assign({}, _GLOBAL_CUSTOM_OBJECTS);
      for (const key of Object.keys(customObjects)) {
        _GLOBAL_CUSTOM_OBJECTS[key] = customObjects[key];
      }
      convertNDArrayScalarsInConfig(config["config"]);
      const returnObj = fromConfig(cls, config["config"], customObjects, fastWeightInit);
      _GLOBAL_CUSTOM_OBJECTS = Object.assign({}, backupCustomObjects);
      return returnObj;
    } else {
      const backupCustomObjects = Object.assign({}, _GLOBAL_CUSTOM_OBJECTS);
      for (const key of Object.keys(customObjects)) {
        _GLOBAL_CUSTOM_OBJECTS[key] = customObjects[key];
      }
      const returnObj = new cls(config["config"]);
      _GLOBAL_CUSTOM_OBJECTS = Object.assign({}, backupCustomObjects);
      return returnObj;
    }
  }
}
function numberCompare(a, b) {
  return a < b ? -1 : a > b ? 1 : 0;
}
function reverseNumberCompare(a, b) {
  return -1 * numberCompare(a, b);
}
function unique(xs) {
  if (xs == null) {
    return xs;
  }
  const out = [];
  for (const x of xs) {
    if (out.indexOf(x) === -1) {
      out.push(x);
    }
  }
  return out;
}
function isObjectEmpty(obj) {
  if (obj == null) {
    throw new ValueError(`Invalid value in obj: ${JSON.stringify(obj)}`);
  }
  for (const key in obj) {
    if (obj.hasOwnProperty(key)) {
      return false;
    }
  }
  return true;
}
function checkStringTypeUnionValue(values, label, value) {
  if (value == null) {
    return;
  }
  if (values.indexOf(value) < 0) {
    throw new ValueError(`${value} is not a valid ${label}.  Valid values are ${values} or null/undefined.`);
  }
}
function checkArrayTypeAndLength(x, expectedType, minLength = 0, maxLength = Infinity) {
  assert2(minLength >= 0);
  assert2(maxLength >= minLength);
  return Array.isArray(x) && x.length >= minLength && x.length <= maxLength && x.every((e) => typeof e === expectedType);
}
function assertPositiveInteger(value, name) {
  if (Array.isArray(value)) {
    util_exports.assert(value.length > 0, () => `${name} is unexpectedly an empty array.`);
    value.forEach((v, i) => assertPositiveInteger(v, `element ${i + 1} of ${name}`));
  } else {
    util_exports.assert(Number.isInteger(value) && value > 0, () => `Expected ${name} to be a positive integer, but got ${formatAsFriendlyString(value)}.`);
  }
}
function formatAsFriendlyString(value) {
  if (value === null) {
    return "null";
  } else if (Array.isArray(value)) {
    return "[" + value.map((v) => formatAsFriendlyString(v)).join(",") + "]";
  } else if (typeof value === "string") {
    return `"${value}"`;
  } else {
    return `${value}`;
  }
}
function debounce(f, waitMs, nowFunc) {
  let lastTime = nowFunc != null ? nowFunc() : util_exports.now();
  let lastResult;
  const f2 = (...args) => {
    const now = nowFunc != null ? nowFunc() : util_exports.now();
    if (now - lastTime < waitMs) {
      return lastResult;
    }
    lastTime = now;
    lastResult = f(...args);
    return lastResult;
  };
  return f2;
}
function mapActivationToFusedKernel(activationName) {
  if (activationName === "relu") {
    return "relu";
  }
  if (activationName === "linear") {
    return "linear";
  }
  if (activationName === "elu") {
    return "elu";
  }
  return null;
}

// node_modules/@tensorflow/tfjs-layers/dist/backend/state.js
var _nextUniqueTensorId = 0;
function getNextUniqueTensorId() {
  return _nextUniqueTensorId++;
}
var _uidPrefixes = {};
function getUid(prefix = "") {
  if (!(prefix in _uidPrefixes)) {
    _uidPrefixes[prefix] = 0;
  }
  _uidPrefixes[prefix] += 1;
  return prefix + _uidPrefixes[prefix].toString();
}

// node_modules/@tensorflow/tfjs-layers/dist/keras_format/common.js
var VALID_DATA_FORMAT_VALUES = ["channelsFirst", "channelsLast"];
var VALID_INTERPOLATION_FORMAT_VALUES = ["nearest", "bilinear"];
var VALID_PADDING_MODE_VALUES = ["valid", "same", "causal"];
var VALID_POOL_MODE_VALUES = ["max", "avg"];
var VALID_BIDIRECTIONAL_MERGE_MODES = ["sum", "mul", "concat", "ave"];

// node_modules/@tensorflow/tfjs-layers/dist/common.js
var nameMap = /* @__PURE__ */ new Map();
function checkDataFormat(value) {
  checkStringTypeUnionValue(VALID_DATA_FORMAT_VALUES, "DataFormat", value);
}
function checkInterpolationFormat(value) {
  checkStringTypeUnionValue(VALID_INTERPOLATION_FORMAT_VALUES, "InterpolationFormat", value);
}
function checkPaddingMode(value) {
  checkStringTypeUnionValue(VALID_PADDING_MODE_VALUES, "PaddingMode", value);
}
function checkPoolMode(value) {
  checkStringTypeUnionValue(VALID_POOL_MODE_VALUES, "PoolMode", value);
}
var _nameScopeStack = [];
var _nameScopeDivider = "/";
function nameScope(name, fn) {
  _nameScopeStack.push(name);
  try {
    const val = fn();
    _nameScopeStack.pop();
    return val;
  } catch (e) {
    _nameScopeStack.pop();
    throw e;
  }
}
function currentNameScopePrefix() {
  if (_nameScopeStack.length === 0) {
    return "";
  } else {
    return _nameScopeStack.join(_nameScopeDivider) + _nameScopeDivider;
  }
}
function getScopedTensorName(tensorName) {
  if (!isValidTensorName(tensorName)) {
    throw new Error("Not a valid tensor name: '" + tensorName + "'");
  }
  return currentNameScopePrefix() + tensorName;
}
function getUniqueTensorName(scopedName) {
  if (!isValidTensorName(scopedName)) {
    throw new Error("Not a valid tensor name: '" + scopedName + "'");
  }
  if (!nameMap.has(scopedName)) {
    nameMap.set(scopedName, 0);
  }
  const index = nameMap.get(scopedName);
  nameMap.set(scopedName, nameMap.get(scopedName) + 1);
  if (index > 0) {
    const result = `${scopedName}_${index}`;
    nameMap.set(result, 1);
    return result;
  } else {
    return scopedName;
  }
}
var tensorNameRegex = new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);
function isValidTensorName(name) {
  return !!name.match(tensorNameRegex);
}

// node_modules/@tensorflow/tfjs-layers/dist/utils/math_utils.js
function isInteger(x) {
  return x === parseInt(x.toString(), 10);
}
function arrayProd(array, begin, end) {
  if (begin == null) {
    begin = 0;
  }
  if (end == null) {
    end = array.length;
  }
  let prod = 1;
  for (let i = begin; i < end; ++i) {
    prod *= array[i];
  }
  return prod;
}
function min(array) {
  if (array.length === 0) {
    return Number.NaN;
  }
  let min2 = Number.POSITIVE_INFINITY;
  for (let i = 0; i < array.length; i++) {
    const value = array[i];
    if (value < min2) {
      min2 = value;
    }
  }
  return min2;
}
function max2(array) {
  if (array.length === 0) {
    return Number.NaN;
  }
  let max3 = Number.NEGATIVE_INFINITY;
  for (let i = 0; i < array.length; i++) {
    const value = array[i];
    if (value > max3) {
      max3 = value;
    }
  }
  return max3;
}
function range(begin, end) {
  if (end < begin) {
    throw new ValueError(`end (${end}) < begin (${begin}) is forbidden.`);
  }
  const out = [];
  for (let i = begin; i < end; ++i) {
    out.push(i);
  }
  return out;
}

// node_modules/@tensorflow/tfjs-layers/dist/backend/common.js
var _epsilon;
function epsilon() {
  if (_epsilon == null) {
    _epsilon = backend().epsilon();
  }
  return _epsilon;
}
function imageDataFormat() {
  return "channelsLast";
}

// node_modules/@tensorflow/tfjs-layers/dist/backend/tfjs_backend.js
function cast2(x, dtype) {
  return cast(x, dtype);
}
function expandDims2(x, axis = -1) {
  const outShape = x.shape.slice();
  if (axis < 0) {
    axis = outShape.length + axis + 1;
  }
  outShape.splice(axis, 0, 1);
  return reshape(x, outShape);
}
function repeat(x, n) {
  return tidy(() => {
    if (x.shape.length !== 2) {
      throw new ValueError(`repeat() expects a rank-2 tensor, but received a rank-${x.shape.length} tensor.`);
    }
    const y = expandDims2(x, 1);
    return tile2(y, [1, n, 1]);
  });
}
function flatten(x) {
  const newShape = [arrayProd(x.shape)];
  return reshape(x, newShape);
}
function batchFlatten(x) {
  if (x.rank <= 1) {
    throw new ValueError(`batchFlatten requires a minimum rank of 2. Got rank: ${x.rank}.`);
  }
  const newShape = [x.shape[0], arrayProd(x.shape, 1)];
  return reshape(x, newShape);
}
function sliceAlongFirstAxis(array, start, size) {
  return tidy(() => {
    switch (array.rank) {
      case 1:
        return slice1d(array, start, size);
      case 2:
        return slice2d(array, [start, 0], [size, array.shape[1]]);
      case 3:
        return slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);
      case 4:
        return slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);
      case 5:
        return slice(array, [start, 0, 0, 0, 0], [
          size,
          array.shape[1],
          array.shape[2],
          array.shape[3],
          array.shape[4]
        ]);
      case 6:
        return slice(array, [start, 0, 0, 0, 0, 0], [
          size,
          array.shape[1],
          array.shape[2],
          array.shape[3],
          array.shape[4],
          array.shape[5]
        ]);
      default:
        throw new ValueError(`sliceAlongFirstAxis() received an unsupported tensor rank: ${array.rank}`);
    }
  });
}
function sliceAlongLastAxis(array, start, size) {
  return tidy(() => {
    switch (array.rank) {
      case 1:
        return slice1d(array, start, size);
      case 2:
        return slice2d(array, [0, start], [array.shape[0], size]);
      case 3:
        return slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);
      case 4:
        return slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);
      default:
        throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ${array.rank}`);
    }
  });
}
function sliceAlongAxis(array, start, size, axis) {
  return tidy(() => {
    switch (array.rank) {
      case 1:
        return slice1d(array, start, size);
      case 2:
        switch (axis) {
          case 1:
            return sliceAlongFirstAxis(array, start, size);
          case 2:
            return sliceAlongLastAxis(array, start, size);
          default:
            throw new ValueError(`The axis is not within the rank of the tensor ${axis}`);
        }
      case 3:
        switch (axis) {
          case 1:
            return sliceAlongFirstAxis(array, start, size);
          case 2:
            return slice3d(array, [0, start, 0], [array.shape[0], size, array.shape[2]]);
          case 3:
            return sliceAlongLastAxis(array, start, size);
          default:
            throw new ValueError(`The axis is not within the rank of the tensor ${axis}`);
        }
      case 4:
        switch (axis) {
          case 1:
            return sliceAlongFirstAxis(array, start, size);
          case 2:
            return slice4d(array, [0, start, 0, 0], [array.shape[0], size, array.shape[2], array.shape[3]]);
          case 3:
            return slice4d(array, [0, 0, start, 0], [array.shape[0], array.shape[1], size, array.shape[3]]);
          case 4:
            return sliceAlongLastAxis(array, start, size);
          default:
            throw new ValueError(`The axis is not within the rank of the tensor ${axis}`);
        }
      default:
        throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ${array.rank}`);
    }
  });
}
function concatenate(tensors, axis = -1) {
  let rank;
  if (axis < 0) {
    rank = tensors[0].rank;
    if (rank !== 0) {
      axis = rank;
    } else {
      axis = 0;
    }
  }
  if (axis === tensors[0].rank) {
    axis = -1;
  }
  return concat(tensors, axis);
}
function concatAlongFirstAxis(a, b) {
  switch (a.rank) {
    case 1:
      return concat1d([a, b]);
    case 2:
      return concat2d([a, b], 0);
    case 3:
      return concat3d([a, b], 0);
    case 4:
      return concat4d([a, b], 0);
    default:
      throw new ValueError(`concatAlongFirstAxis() received an unsupported tensor rank: ${a.rank}`);
  }
}
function tile2(x, n) {
  if (!Array.isArray(n)) {
    n = [n];
  }
  if (x.rank !== n.length) {
    throw new ValueError(`The length of input n (${n.length}) does not match the number of dimensions in input x (${x.rank})`);
  }
  return tile(x, n);
}
function randomNormal2(shape, mean2 = 0, stddev = 1, dtype, seed) {
  return randomNormal(shape, mean2, stddev, dtype, seed);
}
function dot(a, b, activation2, bias) {
  if (a.rank < 2 || b.rank < 2) {
    throw new NotImplementedError(`dot requires both inputs to be rank >= 2 but got x shape = ${a.shape} and y shape = ${b.shape}`);
  }
  if (b.rank >= 3) {
    const xLastDim = a.shape.slice(-1)[0];
    const ySecondLastDim = b.shape.slice(-2)[0];
    if (xLastDim !== ySecondLastDim) {
      throw new NotImplementedError(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${a.shape} and  y shape = ${b.shape}`);
    }
  }
  if (a.rank === 2 && b.rank === 2) {
    const transposeA = false;
    const transposeB = false;
    return fused_ops_exports.matMul({
      a,
      b,
      transposeA,
      transposeB,
      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,
      activation: activation2
    });
  } else {
    const aFirstDims = a.shape.slice();
    const aLastDim = aFirstDims.pop();
    a = reshape(a, [-1, aLastDim]);
    const bShape = b.shape.slice();
    const bLastDim = bShape.pop();
    const ySecondLastDim = bShape.pop();
    const yOtherDims = [...bShape, bLastDim];
    const perm = Array.from({ length: b.rank }, (_, i) => {
      if (i === 0) {
        return b.rank - 2;
      } else if (i <= b.rank - 2) {
        return i - 1;
      }
      return i;
    });
    b = reshape(transpose(b, perm), [ySecondLastDim, -1]);
    const outputShape = [...aFirstDims, ...yOtherDims];
    const transposeA = false;
    const transposeB = false;
    return reshape(fused_ops_exports.matMul({
      a,
      b,
      transposeA,
      transposeB,
      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,
      activation: activation2
    }), outputShape);
  }
}
function gather2(reference, indices, axis) {
  return tidy(() => {
    if (Array.isArray(indices)) {
      indices = tensor1d(indices, "int32");
    } else {
      indices = cast(indices, "int32");
    }
    return gather(reference, indices, axis);
  });
}
function square2(x) {
  return mul(x, x);
}
function reshapeBias(xRank, bias, dataFormat) {
  const biasShape = bias.shape;
  if (bias.rank !== 1 && bias.rank !== xRank) {
    throw new ValueError(`Unexpected bias dimensions: ${bias.rank}; expected it to be 1 or ${xRank}`);
  }
  if (xRank === 5) {
    if (dataFormat === "channelsFirst") {
      if (biasShape.length === 1) {
        return reshape(bias, [1, biasShape[0], 1, 1, 1]);
      } else {
        return reshape(bias, [1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);
      }
    } else if (dataFormat === "channelsLast") {
      if (biasShape.length === 1) {
        return reshape(bias, [1, 1, 1, 1, biasShape[0]]);
      } else {
        return reshape(bias, [1].concat(biasShape));
      }
    }
  } else if (xRank === 4) {
    if (dataFormat === "channelsFirst") {
      if (biasShape.length === 1) {
        return reshape(bias, [1, biasShape[0], 1, 1]);
      } else {
        return reshape(bias, [1, biasShape[2], biasShape[0], biasShape[1]]);
      }
    } else if (dataFormat === "channelsLast") {
      if (biasShape.length === 1) {
        return reshape(bias, [1, 1, 1, biasShape[0]]);
      } else {
        return reshape(bias, [1].concat(biasShape));
      }
    }
  } else if (xRank === 3) {
    if (dataFormat === "channelsFirst") {
      if (biasShape.length === 1) {
        return reshape(bias, [1, biasShape[0], 1]);
      } else {
        return reshape(bias, [1, biasShape[1], biasShape[0]]);
      }
    } else if (dataFormat === "channelsLast") {
      if (biasShape.length === 1) {
        return reshape(bias, [1, 1, biasShape[0]]);
      } else {
        return reshape(bias, [1].concat(biasShape));
      }
    }
  } else if (xRank < 3) {
    return bias;
  }
  throw new ValueError(`Unsupported input rank by biasAdd: ${bias.rank}`);
}
function biasAdd(x, bias, dataFormat) {
  return tidy(() => {
    if (dataFormat == null) {
      dataFormat = imageDataFormat();
    }
    checkDataFormat(dataFormat);
    return add(x, reshapeBias(x.rank, bias, dataFormat));
  });
}
function elu2(x, alpha = 1) {
  if (alpha !== 1) {
    throw new NotImplementedError(`Support for alpha values other than 1 (${alpha}) is not implemented yet.`);
  }
  return elu(x);
}
function softsign(x) {
  return tidy(() => div(x, add(abs(x), 1)));
}
function dropout2(x, level, noiseShape, seed) {
  return tidy(() => dropout(x, level, noiseShape, seed));
}
function hardSigmoid(x) {
  return tidy(() => {
    const y = add(0.5, mul(0.2, x));
    return clipByValue(y, 0, 1);
  });
}
function inTrainPhase(x, alt, training = false) {
  return training ? x() : alt();
}

// node_modules/@tensorflow/tfjs-layers/dist/keras_format/initializer_config.js
var VALID_FAN_MODE_VALUES = ["fanIn", "fanOut", "fanAvg"];
var VALID_DISTRIBUTION_VALUES = ["normal", "uniform", "truncatedNormal"];

// node_modules/@tensorflow/tfjs-layers/dist/initializers.js
function checkFanMode(value) {
  checkStringTypeUnionValue(VALID_FAN_MODE_VALUES, "FanMode", value);
}
function checkDistribution(value) {
  checkStringTypeUnionValue(VALID_DISTRIBUTION_VALUES, "Distribution", value);
}
var Initializer = class extends serialization_exports.Serializable {
  fromConfigUsesCustomObjects() {
    return false;
  }
  getConfig() {
    return {};
  }
};
var Zeros = class extends Initializer {
  apply(shape, dtype) {
    return zeros(shape, dtype);
  }
};
Zeros.className = "Zeros";
serialization_exports.registerClass(Zeros);
var Ones = class extends Initializer {
  apply(shape, dtype) {
    return ones(shape, dtype);
  }
};
Ones.className = "Ones";
serialization_exports.registerClass(Ones);
var Constant = class extends Initializer {
  constructor(args) {
    super();
    if (typeof args !== "object") {
      throw new ValueError(`Expected argument of type ConstantConfig but got ${args}`);
    }
    if (args.value === void 0) {
      throw new ValueError(`config must have value set but got ${args}`);
    }
    this.value = args.value;
  }
  apply(shape, dtype) {
    return tidy(() => mul(scalar(this.value), ones(shape, dtype)));
  }
  getConfig() {
    return {
      value: this.value
    };
  }
};
Constant.className = "Constant";
serialization_exports.registerClass(Constant);
var RandomUniform = class extends Initializer {
  constructor(args) {
    super();
    this.DEFAULT_MINVAL = -0.05;
    this.DEFAULT_MAXVAL = 0.05;
    this.minval = args.minval || this.DEFAULT_MINVAL;
    this.maxval = args.maxval || this.DEFAULT_MAXVAL;
    this.seed = args.seed;
  }
  apply(shape, dtype) {
    return randomUniform(shape, this.minval, this.maxval, dtype);
  }
  getConfig() {
    return { minval: this.minval, maxval: this.maxval, seed: this.seed };
  }
};
RandomUniform.className = "RandomUniform";
serialization_exports.registerClass(RandomUniform);
var RandomNormal = class extends Initializer {
  constructor(args) {
    super();
    this.DEFAULT_MEAN = 0;
    this.DEFAULT_STDDEV = 0.05;
    this.mean = args.mean || this.DEFAULT_MEAN;
    this.stddev = args.stddev || this.DEFAULT_STDDEV;
    this.seed = args.seed;
  }
  apply(shape, dtype) {
    dtype = dtype || "float32";
    if (dtype !== "float32" && dtype !== "int32") {
      throw new NotImplementedError(`randomNormal does not support dType ${dtype}.`);
    }
    return randomNormal2(shape, this.mean, this.stddev, dtype, this.seed);
  }
  getConfig() {
    return { mean: this.mean, stddev: this.stddev, seed: this.seed };
  }
};
RandomNormal.className = "RandomNormal";
serialization_exports.registerClass(RandomNormal);
var TruncatedNormal = class extends Initializer {
  constructor(args) {
    super();
    this.DEFAULT_MEAN = 0;
    this.DEFAULT_STDDEV = 0.05;
    this.mean = args.mean || this.DEFAULT_MEAN;
    this.stddev = args.stddev || this.DEFAULT_STDDEV;
    this.seed = args.seed;
  }
  apply(shape, dtype) {
    dtype = dtype || "float32";
    if (dtype !== "float32" && dtype !== "int32") {
      throw new NotImplementedError(`truncatedNormal does not support dType ${dtype}.`);
    }
    return truncatedNormal(shape, this.mean, this.stddev, dtype, this.seed);
  }
  getConfig() {
    return { mean: this.mean, stddev: this.stddev, seed: this.seed };
  }
};
TruncatedNormal.className = "TruncatedNormal";
serialization_exports.registerClass(TruncatedNormal);
var Identity2 = class extends Initializer {
  constructor(args) {
    super();
    this.gain = args.gain != null ? args.gain : 1;
  }
  apply(shape, dtype) {
    return tidy(() => {
      if (shape.length !== 2 || shape[0] !== shape[1]) {
        throw new ValueError("Identity matrix initializer can only be used for 2D square matrices.");
      } else {
        return mul(this.gain, eye(shape[0]));
      }
    });
  }
  getConfig() {
    return { gain: this.gain };
  }
};
Identity2.className = "Identity";
serialization_exports.registerClass(Identity2);
function computeFans(shape, dataFormat = "channelsLast") {
  let fanIn;
  let fanOut;
  checkDataFormat(dataFormat);
  if (shape.length === 2) {
    fanIn = shape[0];
    fanOut = shape[1];
  } else if ([3, 4, 5].indexOf(shape.length) !== -1) {
    if (dataFormat === "channelsFirst") {
      const receptiveFieldSize = arrayProd(shape, 2);
      fanIn = shape[1] * receptiveFieldSize;
      fanOut = shape[0] * receptiveFieldSize;
    } else if (dataFormat === "channelsLast") {
      const receptiveFieldSize = arrayProd(shape, 0, shape.length - 2);
      fanIn = shape[shape.length - 2] * receptiveFieldSize;
      fanOut = shape[shape.length - 1] * receptiveFieldSize;
    }
  } else {
    const shapeProd = arrayProd(shape);
    fanIn = Math.sqrt(shapeProd);
    fanOut = Math.sqrt(shapeProd);
  }
  return [fanIn, fanOut];
}
var VarianceScaling = class extends Initializer {
  /**
   * Constructor of VarianceScaling.
   * @throws ValueError for invalid value in scale.
   */
  constructor(args) {
    super();
    if (args.scale < 0) {
      throw new ValueError(`scale must be a positive float. Got: ${args.scale}`);
    }
    this.scale = args.scale == null ? 1 : args.scale;
    this.mode = args.mode == null ? "fanIn" : args.mode;
    checkFanMode(this.mode);
    this.distribution = args.distribution == null ? "normal" : args.distribution;
    checkDistribution(this.distribution);
    this.seed = args.seed;
  }
  apply(shape, dtype) {
    const fans = computeFans(shape);
    const fanIn = fans[0];
    const fanOut = fans[1];
    let scale = this.scale;
    if (this.mode === "fanIn") {
      scale /= Math.max(1, fanIn);
    } else if (this.mode === "fanOut") {
      scale /= Math.max(1, fanOut);
    } else {
      scale /= Math.max(1, (fanIn + fanOut) / 2);
    }
    if (this.distribution === "normal") {
      const stddev = Math.sqrt(scale);
      dtype = dtype || "float32";
      if (dtype !== "float32" && dtype !== "int32") {
        throw new NotImplementedError(`${this.getClassName()} does not support dType ${dtype}.`);
      }
      return truncatedNormal(shape, 0, stddev, dtype, this.seed);
    } else {
      const limit = Math.sqrt(3 * scale);
      return randomUniform(shape, -limit, limit, dtype);
    }
  }
  getConfig() {
    return {
      scale: this.scale,
      mode: this.mode,
      distribution: this.distribution,
      seed: this.seed
    };
  }
};
VarianceScaling.className = "VarianceScaling";
serialization_exports.registerClass(VarianceScaling);
var GlorotUniform = class extends VarianceScaling {
  /**
   * Constructor of GlorotUniform
   * @param scale
   * @param mode
   * @param distribution
   * @param seed
   */
  constructor(args) {
    super({
      scale: 1,
      mode: "fanAvg",
      distribution: "uniform",
      seed: args == null ? null : args.seed
    });
  }
  getClassName() {
    return VarianceScaling.className;
  }
};
GlorotUniform.className = "GlorotUniform";
serialization_exports.registerClass(GlorotUniform);
var GlorotNormal = class extends VarianceScaling {
  /**
   * Constructor of GlorotNormal.
   * @param scale
   * @param mode
   * @param distribution
   * @param seed
   */
  constructor(args) {
    super({
      scale: 1,
      mode: "fanAvg",
      distribution: "normal",
      seed: args == null ? null : args.seed
    });
  }
  getClassName() {
    return VarianceScaling.className;
  }
};
GlorotNormal.className = "GlorotNormal";
serialization_exports.registerClass(GlorotNormal);
var HeNormal = class extends VarianceScaling {
  constructor(args) {
    super({
      scale: 2,
      mode: "fanIn",
      distribution: "normal",
      seed: args == null ? null : args.seed
    });
  }
  getClassName() {
    return VarianceScaling.className;
  }
};
HeNormal.className = "HeNormal";
serialization_exports.registerClass(HeNormal);
var HeUniform = class extends VarianceScaling {
  constructor(args) {
    super({
      scale: 2,
      mode: "fanIn",
      distribution: "uniform",
      seed: args == null ? null : args.seed
    });
  }
  getClassName() {
    return VarianceScaling.className;
  }
};
HeUniform.className = "HeUniform";
serialization_exports.registerClass(HeUniform);
var LeCunNormal = class extends VarianceScaling {
  constructor(args) {
    super({
      scale: 1,
      mode: "fanIn",
      distribution: "normal",
      seed: args == null ? null : args.seed
    });
  }
  getClassName() {
    return VarianceScaling.className;
  }
};
LeCunNormal.className = "LeCunNormal";
serialization_exports.registerClass(LeCunNormal);
var LeCunUniform = class extends VarianceScaling {
  constructor(args) {
    super({
      scale: 1,
      mode: "fanIn",
      distribution: "uniform",
      seed: args == null ? null : args.seed
    });
  }
  getClassName() {
    return VarianceScaling.className;
  }
};
LeCunUniform.className = "LeCunNormal";
serialization_exports.registerClass(LeCunUniform);
var Orthogonal = class extends Initializer {
  constructor(args) {
    super();
    this.DEFAULT_GAIN = 1;
    this.gain = args.gain == null ? this.DEFAULT_GAIN : args.gain;
    this.seed = args.seed;
    if (this.seed != null) {
      throw new NotImplementedError("Random seed is not implemented for Orthogonal Initializer yet.");
    }
  }
  apply(shape, dtype) {
    return tidy(() => {
      if (shape.length < 2) {
        throw new NotImplementedError("Shape must be at least 2D.");
      }
      if (shape[0] * shape[1] > 2e3) {
        console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${shape[0] * shape[1]}) elements: Slowness may result.`);
      }
      const normalizedShape = shape[0] > shape[1] ? [shape[1], shape[0]] : shape;
      const a = randomNormal2(normalizedShape, 0, 1, "float32");
      let q = linalg.gramSchmidt(a);
      if (shape[0] > shape[1]) {
        q = transpose(q);
      }
      return mul(this.gain, q);
    });
  }
  getConfig() {
    return {
      gain: this.gain,
      seed: this.seed
    };
  }
};
Orthogonal.className = "Orthogonal";
serialization_exports.registerClass(Orthogonal);
var INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP = {
  "constant": "Constant",
  "glorotNormal": "GlorotNormal",
  "glorotUniform": "GlorotUniform",
  "heNormal": "HeNormal",
  "heUniform": "HeUniform",
  "identity": "Identity",
  "leCunNormal": "LeCunNormal",
  "leCunUniform": "LeCunUniform",
  "ones": "Ones",
  "orthogonal": "Orthogonal",
  "randomNormal": "RandomNormal",
  "randomUniform": "RandomUniform",
  "truncatedNormal": "TruncatedNormal",
  "varianceScaling": "VarianceScaling",
  "zeros": "Zeros"
};
function deserializeInitializer(config, customObjects = {}) {
  return deserializeKerasObject(config, serialization_exports.SerializationMap.getMap().classNameMap, customObjects, "initializer");
}
function serializeInitializer(initializer) {
  return serializeKerasObject(initializer);
}
function getInitializer(identifier) {
  if (typeof identifier === "string") {
    const className = identifier in INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ? INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[identifier] : identifier;
    if (className === "GlorotNormal") {
      return new GlorotNormal();
    } else if (className === "GlorotUniform") {
      return new GlorotUniform();
    } else if (className === "HeNormal") {
      return new HeNormal();
    } else if (className === "HeUniform") {
      return new HeUniform();
    } else if (className === "LeCunNormal") {
      return new LeCunNormal();
    } else if (className === "LeCunUniform") {
      return new LeCunUniform();
    } else {
      const config = {};
      config["className"] = className;
      config["config"] = {};
      return deserializeInitializer(config);
    }
  } else if (identifier instanceof Initializer) {
    return identifier;
  } else {
    return deserializeInitializer(identifier);
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/utils/types_utils.js
function isArrayOfShapes(x) {
  return Array.isArray(x) && Array.isArray(x[0]);
}
function normalizeShapeList(x) {
  if (x.length === 0) {
    return [];
  }
  if (!Array.isArray(x[0])) {
    return [x];
  }
  return x;
}
function getExactlyOneTensor(xs) {
  let x;
  if (Array.isArray(xs)) {
    if (xs.length !== 1) {
      throw new ValueError(`Expected Tensor length to be 1; got ${xs.length}`);
    }
    x = xs[0];
  } else {
    x = xs;
  }
  return x;
}
function getExactlyOneShape(shapes) {
  if (Array.isArray(shapes) && Array.isArray(shapes[0])) {
    if (shapes.length === 1) {
      shapes = shapes;
      return shapes[0];
    } else {
      throw new ValueError(`Expected exactly 1 Shape; got ${shapes.length}`);
    }
  } else {
    return shapes;
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/utils/variable_utils.js
function countParamsInWeights(weights) {
  let count2 = 0;
  for (const weight of weights) {
    if (weight.shape.length === 0) {
      count2 += 1;
    } else {
      count2 += weight.shape.reduce((a, b) => a * b);
    }
  }
  return count2;
}

// node_modules/@tensorflow/tfjs-layers/dist/variables.js
var DEFAULT_VARIABLE_NAME_PREFIX = "Variable";
var LayerVariable = class {
  /**
   * Construct Variable from a `tf.Tensor`.
   *
   * If not explicitly named, the Variable will be given a name with the
   * prefix 'Variable'. Variable names are unique. In the case of name
   * collision, suffixies '_<num>' will be added to the name.
   *
   * @param val Initial value of the Variable.
   * @param name Name of the variable. If `null` or `undefined` is provided, it
   *   will default a name with the prefix 'Variable'.
   * @param constraint Optional, projection function to be applied to the
   * variable after optimize updates
   * @throws ValueError if `name` is `null` or `undefined`.
   */
  constructor(val, dtype = "float32", name = DEFAULT_VARIABLE_NAME_PREFIX, trainable = true, constraint = null) {
    this.dtype = dtype == null ? "float32" : dtype;
    this.shape = val.shape;
    this.id = getNextUniqueTensorId();
    name = name == null ? DEFAULT_VARIABLE_NAME_PREFIX : name;
    this.originalName = getScopedTensorName(name);
    this.name = getUniqueTensorName(this.originalName);
    this.trainable_ = trainable;
    this.constraint = constraint;
    this.val = variable(val, this.trainable_, this.name, this.dtype);
  }
  /**
   * Get a snapshot of the Variable's value.
   *
   * The returned value is a snapshot of the Variable's value at the time of
   * the invocation. Future mutations in the value of the tensor will only
   * be reflected by future calls to this method.
   */
  read() {
    this.assertNotDisposed();
    return this.val;
  }
  /**
   * Update the value of the Variable.
   *
   * @param newVal: The new value to update to. Must be consistent with the
   *   dtype and shape of the Variable.
   * @return This Variable.
   */
  write(newVal) {
    this.assertNotDisposed();
    checkShapesMatch(this.val, newVal);
    if (this.val.id !== newVal.id) {
      this.val.assign(newVal);
      if (this.constraint != null) {
        this.val.assign(this.constraint.apply(this.val));
      }
    }
    return this;
  }
  /**
   * Dispose this LayersVariable instance from memory.
   */
  dispose() {
    this.assertNotDisposed();
    this.val.dispose();
  }
  assertNotDisposed() {
    if (this.val.isDisposed) {
      throw new Error(`LayersVariable ${this.name} is already disposed.`);
    }
  }
  get trainable() {
    return this.trainable_;
  }
  set trainable(trainable) {
    this.trainable_ = trainable;
    this.val.trainable = trainable;
  }
};
function checkShapesMatch(x, y) {
  if (x.shape.toString() !== y.shape.toString()) {
    throw new Error("Shape mismatch: " + JSON.stringify(x.shape) + " vs. " + JSON.stringify(y.shape));
  }
}
function batchGetValue(xs) {
  return xs.map((x) => x.read());
}
function batchSetValue(variablesAndValues) {
  variablesAndValues.forEach((variableAndValue) => {
    const variable2 = variableAndValue[0];
    variable2.write(variableAndValue[1]);
  });
}

// node_modules/@tensorflow/tfjs-layers/dist/engine/topology.js
var InputSpec = class {
  constructor(args) {
    this.dtype = args.dtype;
    this.shape = args.shape;
    if (args.shape != null) {
      this.ndim = args.shape.length;
    } else {
      this.ndim = args.ndim;
    }
    this.maxNDim = args.maxNDim;
    this.minNDim = args.minNDim;
    this.axes = args.axes || {};
  }
};
var SymbolicTensor = class {
  /**
   *
   * @param dtype
   * @param shape
   * @param sourceLayer The Layer that produced this symbolic tensor.
   * @param inputs The inputs passed to sourceLayer's __call__() method.
   * @param nodeIndex
   * @param tensorIndex
   * @param callArgs The keyword arguments passed to the __call__() method.
   * @param name
   * @param outputTensorIndex The index of this tensor in the list of outputs
   *   returned by apply().
   */
  constructor(dtype, shape, sourceLayer, inputs, callArgs, name, outputTensorIndex) {
    this.dtype = dtype;
    this.shape = shape;
    this.sourceLayer = sourceLayer;
    this.inputs = inputs;
    this.callArgs = callArgs;
    this.outputTensorIndex = outputTensorIndex;
    this.id = getNextUniqueTensorId();
    if (name != null) {
      this.originalName = getScopedTensorName(name);
      this.name = getUniqueTensorName(this.originalName);
    }
    this.rank = shape.length;
  }
};
var _nextNodeID = 0;
var Node = class {
  constructor(args, callArgs) {
    this.callArgs = callArgs;
    this.id = _nextNodeID++;
    this.outboundLayer = args.outboundLayer;
    this.inboundLayers = args.inboundLayers;
    this.nodeIndices = args.nodeIndices;
    this.tensorIndices = args.tensorIndices;
    this.inputTensors = args.inputTensors;
    this.outputTensors = args.outputTensors;
    this.inputMasks = args.inputMasks;
    this.outputMasks = args.outputMasks;
    this.inputShapes = args.inputShapes;
    this.outputShapes = args.outputShapes;
    for (const layer of args.inboundLayers) {
      if (layer != null) {
        layer.outboundNodes.push(this);
      }
    }
    args.outboundLayer.inboundNodes.push(this);
  }
  getConfig() {
    const inboundNames = [];
    for (const layer of this.inboundLayers) {
      if (layer != null) {
        inboundNames.push(layer.name);
      } else {
        inboundNames.push(null);
      }
    }
    return {
      outboundLayer: this.outboundLayer ? this.outboundLayer.name : null,
      inboundLayers: inboundNames,
      nodeIndices: this.nodeIndices,
      tensorIndices: this.tensorIndices
    };
  }
};
var _nextLayerID = 0;
var Layer = class extends serialization_exports.Serializable {
  constructor(args = {}) {
    super();
    this._callHook = null;
    this._addedWeightNames = [];
    this._stateful = false;
    this.id = _nextLayerID++;
    this.activityRegularizer = null;
    this.inputSpec = null;
    this.supportsMasking = false;
    this._trainableWeights = [];
    this._nonTrainableWeights = [];
    this._losses = [];
    this._updates = [];
    this._built = false;
    this.inboundNodes = [];
    this.outboundNodes = [];
    let name = args.name;
    if (!name) {
      const prefix = this.getClassName();
      name = toSnakeCase(prefix) + "_" + getUid(prefix);
    }
    this.name = name;
    this.trainable_ = args.trainable == null ? true : args.trainable;
    if (args.inputShape != null || args.batchInputShape != null) {
      let batchInputShape;
      if (args.batchInputShape != null) {
        batchInputShape = args.batchInputShape;
      } else if (args.inputShape != null) {
        let batchSize = null;
        if (args.batchSize != null) {
          batchSize = args.batchSize;
        }
        batchInputShape = [batchSize].concat(args.inputShape);
      }
      this.batchInputShape = batchInputShape;
      let dtype = args.dtype;
      if (dtype == null) {
        dtype = args.inputDType;
      }
      if (dtype == null) {
        dtype = "float32";
      }
      this.dtype = dtype;
    }
    if (args.weights != null) {
      this.initialWeights = args.weights;
    } else {
      this.initialWeights = null;
    }
    this._refCount = null;
    this.fastWeightInitDuringBuild = false;
  }
  /**
   * Converts a layer and its index to a unique (immutable type) name.
   * This function is used internally with `this.containerNodes`.
   * @param layer The layer.
   * @param nodeIndex The layer's position (e.g. via enumerate) in a list of
   *   nodes.
   *
   * @returns The unique name.
   */
  static nodeKey(layer, nodeIndex) {
    return layer.name + "_ib-" + nodeIndex.toString();
  }
  /**
   * Returns this.inboundNode at index nodeIndex.
   *
   * Porting note: This is a replacement for _get_node_attribute_at_index()
   * @param nodeIndex
   * @param attrName The name of the attribute related to request for this node.
   */
  getNodeAtIndex(nodeIndex, attrName) {
    if (this.inboundNodes.length === 0) {
      throw new RuntimeError(`The layer has never been called and thus has no defined ${attrName}.`);
    }
    if (this.inboundNodes.length <= nodeIndex) {
      throw new ValueError(`Asked to get ${attrName} at node ${nodeIndex}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);
    }
    return this.inboundNodes[nodeIndex];
  }
  /**
   * Retrieves the input tensor(s) of a layer at a given node.
   *
   * @param nodeIndex Integer, index of the node from which to retrieve the
   *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer
   *   was called.
   *
   * @return A tensor (or list of tensors if the layer has multiple inputs).
   */
  getInputAt(nodeIndex) {
    return singletonOrArray(this.getNodeAtIndex(nodeIndex, "input").inputTensors);
  }
  /**
   * Retrieves the output tensor(s) of a layer at a given node.
   *
   * @param nodeIndex Integer, index of the node from which to retrieve the
   *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer
   *   was called.
   *
   * @return A tensor (or list of tensors if the layer has multiple outputs).
   */
  getOutputAt(nodeIndex) {
    return singletonOrArray(this.getNodeAtIndex(nodeIndex, "output").outputTensors);
  }
  // Properties
  /**
   * Retrieves the input tensor(s) of a layer.
   *
   * Only applicable if the layer has exactly one inbound node,
   * i.e. if it is connected to one incoming layer.
   *
   * @return Input tensor or list of input tensors.
   *
   * @exception AttributeError if the layer is connected to more than one
   *   incoming layers.
   */
  get input() {
    if (this.inboundNodes.length > 1) {
      throw new AttributeError(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);
    } else if (this.inboundNodes.length === 0) {
      throw new AttributeError(`Layer ${this.name} is not connected, no input to return.`);
    }
    return singletonOrArray(this.getNodeAtIndex(0, "input").inputTensors);
  }
  /**
   * Retrieves the output tensor(s) of a layer.
   *
   * Only applicable if the layer has exactly one inbound node,
   * i.e. if it is connected to one incoming layer.
   *
   * @return Output tensor or list of output tensors.
   *
   * @exception AttributeError if the layer is connected to more than one
   *   incoming layers.
   */
  get output() {
    if (this.inboundNodes.length === 0) {
      throw new AttributeError(`Layer ${this.name} has no inbound nodes.`);
    }
    if (this.inboundNodes.length > 1) {
      throw new AttributeError(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);
    }
    return singletonOrArray(this.getNodeAtIndex(0, "output").outputTensors);
  }
  get losses() {
    return this._losses;
  }
  /**
   * Retrieves the Layer's current loss values.
   *
   * Used for regularizers during training.
   */
  calculateLosses() {
    return this.losses.map((lossFn) => lossFn());
  }
  get updates() {
    return this._updates;
  }
  get built() {
    return this._built;
  }
  set built(built) {
    this._built = built;
  }
  get trainable() {
    return this.trainable_;
  }
  set trainable(trainable) {
    this._trainableWeights.forEach((w) => w.trainable = trainable);
    this.trainable_ = trainable;
  }
  get trainableWeights() {
    if (this.trainable_) {
      return this._trainableWeights.filter((w) => w.trainable);
    } else {
      return [];
    }
  }
  set trainableWeights(weights) {
    this._trainableWeights = weights;
  }
  get nonTrainableWeights() {
    if (this.trainable) {
      return this._trainableWeights.filter((w) => !w.trainable).concat(this._nonTrainableWeights);
    } else {
      return this._trainableWeights.concat(this._nonTrainableWeights);
    }
  }
  set nonTrainableWeights(weights) {
    this._nonTrainableWeights = weights;
  }
  /**
   * The concatenation of the lists trainableWeights and nonTrainableWeights
   * (in this order).
   */
  get weights() {
    return this.trainableWeights.concat(this.nonTrainableWeights);
  }
  get stateful() {
    return this._stateful;
  }
  /**
   * Reset the states of the layer.
   *
   * This method of the base Layer class is essentially a no-op.
   * Subclasses that are stateful (e.g., stateful RNNs) should override this
   * method.
   */
  resetStates() {
    if (!this.stateful) {
      throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.");
    }
  }
  /**
   * Checks compatibility between the layer and provided inputs.
   *
   * This checks that the tensor(s) `input`
   * verify the input assumptions of the layer
   * (if any). If not, exceptions are raised.
   *
   * @param inputs Input tensor or list of input tensors.
   *
   * @exception ValueError in case of mismatch between
   *   the provided inputs and the expectations of the layer.
   */
  assertInputCompatibility(inputs) {
    inputs = toList(inputs);
    if (this.inputSpec == null || this.inputSpec.length === 0) {
      return;
    }
    const inputSpec = toList(this.inputSpec);
    if (inputs.length !== inputSpec.length) {
      throw new ValueError(`Layer ${this.name} expects ${inputSpec.length} inputs, but it received ${inputs.length} input tensors. Input received: ${inputs}`);
    }
    for (let inputIndex = 0; inputIndex < inputs.length; inputIndex++) {
      const x = inputs[inputIndex];
      const spec = inputSpec[inputIndex];
      if (spec == null) {
        continue;
      }
      const ndim = x.rank;
      if (spec.ndim != null) {
        if (ndim !== spec.ndim) {
          throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name}: expected ndim=${spec.ndim}, found ndim=${ndim}`);
        }
      }
      if (spec.maxNDim != null) {
        if (ndim > spec.maxNDim) {
          throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name}: expected max_ndim=${spec.maxNDim}, found ndim=${ndim}`);
        }
      }
      if (spec.minNDim != null) {
        if (ndim < spec.minNDim) {
          throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name}: expected min_ndim=${spec.minNDim}, found ndim=${ndim}.`);
        }
      }
      if (spec.dtype != null) {
        if (x.dtype !== spec.dtype) {
          throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name} : expected dtype=${spec.dtype}, found dtype=${x.dtype}.`);
        }
      }
      if (spec.axes) {
        const xShape = x.shape;
        for (const key in spec.axes) {
          const axis = Number(key);
          const value = spec.axes[key];
          const xShapeAtAxis = axis >= 0 ? xShape[axis] : xShape[xShape.length + axis];
          if (value != null && [value, null].indexOf(xShapeAtAxis) === -1) {
            throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name}: expected axis ${axis} of input shape to have value ${value} but got shape ${xShape}.`);
          }
        }
      }
      if (spec.shape != null) {
        for (let i = 0; i < spec.shape.length; ++i) {
          const specDim = spec.shape[i];
          const dim = x.shape[i];
          if (specDim != null && dim != null) {
            if (specDim !== dim) {
              throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name}: expected shape=${spec.shape}, found shape=${x.shape}.`);
            }
          }
        }
      }
    }
  }
  /**
   * This is where the layer's logic lives.
   *
   * @param inputs Input tensor, or list/tuple of input tensors.
   * @param kwargs Additional keyword arguments.
   *
   * @return A tensor or list/tuple of tensors.
   */
  call(inputs, kwargs) {
    return inputs;
  }
  invokeCallHook(inputs, kwargs) {
    if (this._callHook != null) {
      this._callHook(inputs, kwargs);
    }
  }
  /**
   * Set call hook.
   * This is currently used for testing only.
   * @param callHook
   */
  setCallHook(callHook) {
    this._callHook = callHook;
  }
  /**
   * Clear call hook.
   * This is currently used for testing only.
   */
  clearCallHook() {
    this._callHook = null;
  }
  /**
   * Builds or executes a `Layer`'s logic.
   *
   * When called with `tf.Tensor`(s), execute the `Layer`'s computation and
   * return Tensor(s). For example:
   *
   * ```js
   * const denseLayer = tf.layers.dense({
   *   units: 1,
   *   kernelInitializer: 'zeros',
   *   useBias: false
   * });
   *
   * // Invoke the layer's apply() method with a `tf.Tensor` (with concrete
   * // numeric values).
   * const input = tf.ones([2, 2]);
   * const output = denseLayer.apply(input);
   *
   * // The output's value is expected to be [[0], [0]], due to the fact that
   * // the dense layer has a kernel initialized to all-zeros and does not have
   * // a bias.
   * output.print();
   * ```
   *
   * When called with `tf.SymbolicTensor`(s), this will prepare the layer for
   * future execution.  This entails internal book-keeping on shapes of
   * expected Tensors, wiring layers together, and initializing weights.
   *
   * Calling `apply` with `tf.SymbolicTensor`s are typically used during the
   * building of non-`tf.Sequential` models. For example:
   *
   * ```js
   * const flattenLayer = tf.layers.flatten();
   * const denseLayer = tf.layers.dense({units: 1});
   *
   * // Use tf.layers.input() to obtain a SymbolicTensor as input to apply().
   * const input = tf.input({shape: [2, 2]});
   * const output1 = flattenLayer.apply(input);
   *
   * // output1.shape is [null, 4]. The first dimension is the undetermined
   * // batch size. The second dimension comes from flattening the [2, 2]
   * // shape.
   * console.log(JSON.stringify(output1.shape));
   *
   * // The output SymbolicTensor of the flatten layer can be used to call
   * // the apply() of the dense layer:
   * const output2 = denseLayer.apply(output1);
   *
   * // output2.shape is [null, 1]. The first dimension is the undetermined
   * // batch size. The second dimension matches the number of units of the
   * // dense layer.
   * console.log(JSON.stringify(output2.shape));
   *
   * // The input and output can be used to construct a model that consists
   * // of the flatten and dense layers.
   * const model = tf.model({inputs: input, outputs: output2});
   * ```
   *
   * @param inputs a `tf.Tensor` or `tf.SymbolicTensor` or an Array of them.
   * @param kwargs Additional keyword arguments to be passed to `call()`.
   *
   * @return Output of the layer's `call` method.
   *
   * @exception ValueError error in case the layer is missing shape information
   *   for its `build` call.
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  // Porting Note: This is a replacement for __call__() in Python.
  apply(inputs, kwargs) {
    kwargs = kwargs || {};
    this.assertNotDisposed();
    const inputsList = toList(inputs);
    let allAreSymbolic = true;
    for (const input2 of inputsList) {
      if (!(input2 instanceof SymbolicTensor)) {
        allAreSymbolic = false;
        break;
      }
    }
    let noneAreSymbolic = true;
    for (const input2 of inputsList) {
      if (input2 instanceof SymbolicTensor) {
        noneAreSymbolic = false;
        break;
      }
    }
    if (allAreSymbolic === noneAreSymbolic) {
      throw new ValueError("Arguments to apply() must be all SymbolicTensors or all Tensors");
    }
    return nameScope(this.name, () => {
      if (!this.built) {
        this.assertInputCompatibility(inputs);
        const inputShapes = [];
        for (const xElem of toList(inputs)) {
          inputShapes.push(xElem.shape);
        }
        this.build(singletonOrArray(inputShapes));
        this.built = true;
        if (this.initialWeights) {
          this.setWeights(this.initialWeights);
        }
        if (this._refCount === null && noneAreSymbolic) {
          this._refCount = 1;
        }
      }
      this.assertInputCompatibility(inputs);
      if (noneAreSymbolic) {
        let output = this.call(inputs, kwargs);
        const outputList = toList(output);
        const outputListCopy = [];
        for (let x of outputList) {
          if (inputsList.indexOf(x) !== -1) {
            x = x.clone();
          }
          outputListCopy.push(x);
        }
        output = singletonOrArray(outputListCopy);
        if (this.activityRegularizer != null) {
          throw new NotImplementedError("Layer invocation in the presence of activity regularizer(s) is not supported yet.");
        }
        return output;
      } else {
        const inputShape = collectInputShape(inputs);
        const outputShape = this.computeOutputShape(inputShape);
        let output;
        const outputDType = guessOutputDType(inputs);
        this.warnOnIncompatibleInputShape(Array.isArray(inputs) ? inputShape[0] : inputShape);
        if (outputShape != null && outputShape.length > 0 && Array.isArray(outputShape[0])) {
          output = outputShape.map((shape, index) => new SymbolicTensor(outputDType, shape, this, toList(inputs), kwargs, this.name, index));
        } else {
          output = new SymbolicTensor(outputDType, outputShape, this, toList(inputs), kwargs, this.name);
        }
        this.addInboundNode(inputs, output, null, null, inputShape, outputShape, kwargs);
        this._refCount++;
        if (this.activityRegularizer != null) {
          throw new NotImplementedError("Layer invocation in the presence of activity regularizer(s) is not supported yet.");
        }
        return output;
      }
    });
  }
  /**
   * Check compatibility between input shape and this layer's batchInputShape.
   *
   * Print warning if any incompatibility is found.
   *
   * @param inputShape Input shape to be checked.
   */
  warnOnIncompatibleInputShape(inputShape) {
    if (this.batchInputShape == null) {
      return;
    } else if (inputShape.length !== this.batchInputShape.length) {
      console.warn(`The rank of the input tensor provided (shape: ${JSON.stringify(inputShape)}) does not match that of the batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer ${this.name}`);
    } else {
      let dimMismatch = false;
      this.batchInputShape.forEach((dimension, i) => {
        if (dimension != null && inputShape[i] != null && inputShape[i] !== dimension) {
          dimMismatch = true;
        }
      });
      if (dimMismatch) {
        console.warn(`The shape of the input tensor (${JSON.stringify(inputShape)}) does not match the expectation of layer ${this.name}: ${JSON.stringify(this.batchInputShape)}`);
      }
    }
  }
  /**
   * Retrieves the output shape(s) of a layer.
   *
   * Only applicable if the layer has only one inbound node, or if all inbound
   * nodes have the same output shape.
   *
   * @returns Output shape or shapes.
   * @throws AttributeError: if the layer is connected to more than one incoming
   *   nodes.
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  get outputShape() {
    if (this.inboundNodes == null || this.inboundNodes.length === 0) {
      throw new AttributeError(`The layer ${this.name} has never been called and thus has no defined output shape.`);
    }
    const allOutputShapes = [];
    for (const node of this.inboundNodes) {
      const shapeString = JSON.stringify(node.outputShapes);
      if (allOutputShapes.indexOf(shapeString) === -1) {
        allOutputShapes.push(shapeString);
      }
    }
    if (allOutputShapes.length === 1) {
      const outputShapes = this.inboundNodes[0].outputShapes;
      if (Array.isArray(outputShapes) && Array.isArray(outputShapes[0]) && outputShapes.length === 1) {
        return outputShapes[0];
      } else {
        return outputShapes;
      }
    } else {
      throw new AttributeError(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`);
    }
  }
  /**
   * Counts the total number of numbers (e.g., float32, int32) in the
   * weights.
   *
   * @returns An integer count.
   * @throws RuntimeError: If the layer is not built yet (in which case its
   *   weights are not defined yet.)
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  countParams() {
    if (!this.built) {
      throw new RuntimeError(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);
    }
    return countParamsInWeights(this.weights);
  }
  /**
   * Creates the layer weights.
   *
   * Must be implemented on all layers that have weights.
   *
   * Called when apply() is called to construct the weights.
   *
   * @param inputShape A `Shape` or array of `Shape` (unused).
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  build(inputShape) {
    this.built = true;
  }
  /**
   * Returns the current values of the weights of the layer.
   *
   * @param trainableOnly Whether to get the values of only trainable weights.
   * @returns Weight values as an `Array` of `tf.Tensor`s.
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  getWeights(trainableOnly = false) {
    return batchGetValue(trainableOnly ? this.trainableWeights : this.weights);
  }
  /**
   * Sets the weights of the layer, from Tensors.
   *
   * @param weights a list of Tensors. The number of arrays and their shape
   *   must match number of the dimensions of the weights of the layer (i.e.
   *   it should match the output of `getWeights`).
   *
   * @exception ValueError If the provided weights list does not match the
   *   layer's specifications.
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  setWeights(weights) {
    tidy(() => {
      const params = this.weights;
      if (params.length !== weights.length) {
        throw new ValueError(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${weights.length}, but the layer was expecting ${params.length} weights. Provided weights: ${weights}...`);
      }
      if (params.length === 0) {
        return;
      }
      const weightValueTuples = [];
      const paramValues = batchGetValue(params);
      for (let i = 0; i < paramValues.length; ++i) {
        const pv = paramValues[i];
        const p = params[i];
        const w = weights[i];
        if (!util_exports.arraysEqual(pv.shape, w.shape)) {
          throw new ValueError(`Layer weight shape ${pv.shape} not compatible with provided weight shape ${w.shape}`);
        }
        weightValueTuples.push([p, w]);
      }
      batchSetValue(weightValueTuples);
    });
  }
  /**
   * Adds a weight variable to the layer.
   *
   * @param name Name of the new weight variable.
   * @param shape The shape of the weight.
   * @param dtype The dtype of the weight.
   * @param initializer An initializer instance.
   * @param regularizer A regularizer instance.
   * @param trainable Whether the weight should be trained via backprop or not
   *   (assuming that the layer itself is also trainable).
   * @param constraint An optional trainable.
   * @return The created weight variable.
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  addWeight(name, shape, dtype, initializer, regularizer, trainable, constraint, getInitializerFunc) {
    if (this._addedWeightNames.indexOf(name) !== -1) {
      throw new ValueError(`Duplicate weight name ${name} for layer ${this.name}`);
    }
    this._addedWeightNames.push(name);
    if (dtype == null) {
      dtype = "float32";
    }
    if (this.fastWeightInitDuringBuild) {
      initializer = getInitializerFunc != null ? getInitializerFunc() : getInitializer("zeros");
    }
    const initValue = initializer.apply(shape, dtype);
    const weight = new LayerVariable(initValue, dtype, name, trainable, constraint);
    initValue.dispose();
    if (regularizer != null) {
      this.addLoss(() => regularizer.apply(weight.read()));
    }
    if (trainable == null) {
      trainable = true;
    }
    if (trainable) {
      this._trainableWeights.push(weight);
    } else {
      this._nonTrainableWeights.push(weight);
    }
    return weight;
  }
  /**
   * Set the fast-weight-initialization flag.
   *
   * In cases where the initialized weight values will be immediately
   * overwritten by loaded weight values during model loading, setting
   * the flag to `true` saves unnecessary calls to potentially expensive
   * initializers and speeds up the loading process.
   *
   * @param value Target value of the flag.
   */
  setFastWeightInitDuringBuild(value) {
    this.fastWeightInitDuringBuild = value;
  }
  /**
   * Add losses to the layer.
   *
   * The loss may potentially be conditional on some inputs tensors,
   * for instance activity losses are conditional on the layer's inputs.
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  addLoss(losses) {
    if (losses == null || Array.isArray(losses) && losses.length === 0) {
      return;
    }
    losses = toList(losses);
    if (this._losses !== void 0 && this._losses !== null) {
      this.losses.push(...losses);
    }
  }
  /**
   * Computes the output shape of the layer.
   *
   * Assumes that the layer will be built to match that input shape provided.
   *
   * @param inputShape A shape (tuple of integers) or a list of shape tuples
   *   (one per output tensor of the layer). Shape tuples can include null for
   *   free dimensions, instead of an integer.
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  computeOutputShape(inputShape) {
    return inputShape;
  }
  /**
   * Computes an output mask tensor.
   *
   * @param inputs Tensor or list of tensors.
   * @param mask Tensor or list of tensors.
   *
   * @return null or a tensor (or list of tensors, one per output tensor of the
   * layer).
   */
  computeMask(inputs, mask) {
    if (!this.supportsMasking) {
      if (mask != null) {
        if (Array.isArray(mask)) {
          mask.forEach((maskElement) => {
            if (maskElement != null) {
              throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);
            }
          });
        } else {
          throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);
        }
      }
      return null;
    }
    return mask;
  }
  /**
   * Internal method to create an inbound node for the layer.
   *
   * @param inputTensors List of input tensors.
   * @param outputTensors List of output tensors.
   * @param inputMasks List of input masks (a mask can be a tensor, or null).
   * @param outputMasks List of output masks (a mask can be a tensor, or null).
   * @param inputShapes List of input shape tuples.
   * @param outputShapes List of output shape tuples.
   * @param kwargs Dictionary of keyword arguments that were passed to the
   *   `call` method of the layer at the call that created the node.
   */
  addInboundNode(inputTensors, outputTensors, inputMasks, outputMasks, inputShapes, outputShapes, kwargs = null) {
    const inputTensorList = toList(inputTensors);
    outputTensors = toList(outputTensors);
    inputMasks = toList(inputMasks);
    outputMasks = toList(outputMasks);
    inputShapes = normalizeShapeList(inputShapes);
    outputShapes = normalizeShapeList(outputShapes);
    const inboundLayers = [];
    const nodeIndices = [];
    const tensorIndices = [];
    for (const x of inputTensorList) {
      inboundLayers.push(x.sourceLayer);
      nodeIndices.push(x.nodeIndex);
      tensorIndices.push(x.tensorIndex);
    }
    new Node({
      outboundLayer: this,
      inboundLayers,
      nodeIndices,
      tensorIndices,
      inputTensors: inputTensorList,
      outputTensors,
      inputMasks,
      outputMasks,
      inputShapes,
      outputShapes
    }, kwargs);
    for (let i = 0; i < outputTensors.length; i++) {
      outputTensors[i].sourceLayer = this;
      outputTensors[i].nodeIndex = this.inboundNodes.length - 1;
      outputTensors[i].tensorIndex = i;
    }
  }
  /**
   * Returns the config of the layer.
   *
   * A layer config is a TS dictionary (serializable)
   * containing the configuration of a layer.
   * The same layer can be reinstantiated later
   * (without its trained weights) from this configuration.
   *
   * The config of a layer does not include connectivity
   * information, nor the layer class name.  These are handled
   * by 'Container' (one layer of abstraction above).
   *
   * Porting Note: The TS dictionary follows TS naming standards for
   * keys, and uses tfjs-layers type-safe Enums.  Serialization methods
   * should use a helper function to convert to the pythonic storage
   * standard. (see serialization_utils.convertTsToPythonic)
   *
   * @returns TS dictionary of configuration.
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  getConfig() {
    const config = { name: this.name, trainable: this.trainable };
    if (this.batchInputShape != null) {
      config["batchInputShape"] = this.batchInputShape;
    }
    if (this.dtype != null) {
      config["dtype"] = this.dtype;
    }
    return config;
  }
  /**
   * Dispose the weight variables that this Layer instance holds.
   *
   * @returns {number} Number of disposed variables.
   */
  disposeWeights() {
    this.weights.forEach((weight) => weight.dispose());
    return this.weights.length;
  }
  assertNotDisposed() {
    if (this._refCount === 0) {
      throw new Error(`Layer '${this.name}' is already disposed.`);
    }
  }
  /**
   * Attempt to dispose layer's weights.
   *
   * This method decreases the reference count of the Layer object by 1.
   *
   * A Layer is reference-counted. Its reference count is incremented by 1
   * the first item its `apply()` method is called and when it becomes a part
   * of a new `Node` (through calling the `apply()` method on a
   * `tf.SymbolicTensor`).
   *
   * If the reference count of a Layer becomes 0, all the weights will be
   * disposed and the underlying memory (e.g., the textures allocated in WebGL)
   * will be freed.
   *
   * Note: If the reference count is greater than 0 after the decrement, the
   * weights of the Layer will *not* be disposed.
   *
   * After a Layer is disposed, it cannot be used in calls such as `apply()`,
   * `getWeights()` or `setWeights()` anymore.
   *
   * @returns A DisposeResult Object with the following fields:
   *   - refCountAfterDispose: The reference count of the Container after this
   *     `dispose()` call.
   *   - numDisposedVariables: Number of `tf.Variable`s (i.e., weights) disposed
   *     during this `dispose()` call.
   * @throws {Error} If the layer is not built yet, or if the layer has already
   *   been disposed.
   *
   * @doc {heading: 'Models', 'subheading': 'Classes'}
   */
  dispose() {
    if (!this.built) {
      throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);
    }
    if (this._refCount === null) {
      throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);
    }
    this.assertNotDisposed();
    let numDisposedVariables = 0;
    if (--this._refCount === 0) {
      numDisposedVariables = this.disposeWeights();
    }
    return { refCountAfterDispose: this._refCount, numDisposedVariables };
  }
};
function collectInputShape(inputTensors) {
  inputTensors = toList(inputTensors);
  const shapes = [];
  for (const x of inputTensors) {
    shapes.push(x.shape);
  }
  return singletonOrArray(shapes);
}
function guessOutputDType(inputTensors) {
  return "float32";
}
function getSourceInputs(tensor, layer, nodeIndex) {
  if (layer == null || nodeIndex != null && nodeIndex > 0) {
    layer = tensor.sourceLayer;
    nodeIndex = tensor.nodeIndex;
  }
  if (layer.inboundNodes.length === 0) {
    return [tensor];
  } else {
    const node = layer.inboundNodes[nodeIndex];
    if (node.inboundLayers.length === 0) {
      return node.inputTensors;
    } else {
      const sourceTensors = [];
      for (let i = 0; i < node.inboundLayers.length; i++) {
        const x = node.inputTensors[i];
        const layer2 = node.inboundLayers[i];
        const nodeIndex2 = node.nodeIndices[i];
        const previousSources = getSourceInputs(x, layer2, nodeIndex2);
        for (const x2 of previousSources) {
          if (sourceTensors.indexOf(x2) === -1) {
            sourceTensors.push(x2);
          }
        }
      }
      return sourceTensors;
    }
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/engine/input_layer.js
var InputLayer = class extends Layer {
  constructor(args) {
    super({
      dtype: args.dtype,
      name: args.name != null ? args.name : getUid("input").toString()
    });
    if (args.batchSize == null) {
      args.batchSize = null;
    }
    if (args.sparse == null) {
      args.sparse = false;
    }
    this.trainable = false;
    this.built = true;
    this.sparse = args.sparse;
    if (args.inputShape != null && args.batchInputShape != null) {
      throw new ValueError("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");
    }
    let batchInputShape = args.batchInputShape;
    if (batchInputShape == null) {
      if (args.inputShape == null) {
        throw new ValueError("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");
      } else {
        batchInputShape = [args.batchSize].concat(args.inputShape);
      }
    } else {
      if (args.batchSize != null) {
        throw new ValueError("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");
      }
    }
    const dtype = args.dtype || "float32";
    this.batchInputShape = batchInputShape;
    this.dtype = dtype;
    this.inputSpec = [{ shape: batchInputShape }];
    const inputTensor = new SymbolicTensor(this.dtype, this.batchInputShape, this, [], {}, this.name);
    inputTensor.nodeIndex = 0;
    inputTensor.tensorIndex = 0;
    new Node({
      outboundLayer: this,
      inboundLayers: [],
      nodeIndices: [],
      tensorIndices: [],
      inputTensors: [inputTensor],
      outputTensors: [inputTensor],
      inputMasks: [null],
      outputMasks: [null],
      inputShapes: [batchInputShape],
      outputShapes: [batchInputShape]
    });
  }
  apply(inputs, kwargs) {
    throw new ValueError(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`);
  }
  dispose() {
    return { refCountAfterDispose: this._refCount, numDisposedVariables: 0 };
  }
  getConfig() {
    return {
      batchInputShape: this.batchInputShape,
      dtype: this.dtype,
      sparse: this.sparse,
      name: this.name
    };
  }
};
InputLayer.className = "InputLayer";
serialization_exports.registerClass(InputLayer);
function Input(config) {
  if (config.batchShape == null && config.shape == null) {
    throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");
  }
  if (config.batchShape != null && config.shape != null) {
    throw new ValueError("Please provide either a `shape` or `batchShape` argument to Input, but not both.");
  }
  let batchShape = config.batchShape;
  if (config.shape != null && batchShape == null) {
    batchShape = [null].concat(config.shape);
  }
  let dtype = config.dtype;
  if (dtype == null) {
    dtype = "float32";
  }
  const inputLayer2 = new InputLayer({
    batchInputShape: batchShape,
    name: config.name,
    dtype,
    sparse: config.sparse
  });
  const outputs = inputLayer2.inboundNodes[0].outputTensors;
  return outputs[0];
}

// node_modules/@tensorflow/tfjs-layers/dist/engine/executor.js
function assertFeedCompatibility(key, val) {
  if (key.dtype == null || key.dtype === val.dtype) {
    return val;
  }
  try {
    return cast(val, key.dtype);
  } catch (err) {
    throw new ValueError(`The dtype of the feed (${val.dtype}) can not be cast to the dtype of the key '${key.name}' (${key.dtype}).`);
  }
}
var FeedDict = class _FeedDict {
  /**
   * Constructor, optionally does copy-construction.
   * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case
   *   copy-construction will be performed.
   */
  constructor(feeds) {
    this.id2Value = {};
    this.id2Mask = {};
    this.name2Id = {};
    if (feeds instanceof _FeedDict) {
      for (const id in feeds.id2Value) {
        this.id2Value[id] = feeds.id2Value[id];
        if (id in feeds.id2Mask) {
          this.id2Mask[id] = feeds.id2Mask[id];
        }
      }
    } else {
      if (feeds == null) {
        return;
      }
      for (const feed of feeds) {
        this.add(feed.key, feed.value);
      }
    }
  }
  /**
   * Add a key-value pair to the FeedDict.
   *
   * @param key The key of the feed.
   * @param value The value of the tensor feed.
   * @param mask The value of the mask feed (optional).
   * @returns This `FeedDict`.
   * @throws ValueError: If the key `SymbolicTensor` already exists in the
   *   `FeedDict`.
   */
  add(key, value, mask) {
    if (this.id2Value[key.id] == null) {
      this.id2Value[key.id] = assertFeedCompatibility(key, value);
      this.name2Id[key.name] = key.id;
      if (mask != null) {
        this.id2Mask[key.id] = mask;
      }
    } else {
      throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);
    }
    return this;
  }
  /**
   * Add a Feed to the FeedDict.
   * @param feed The new `Feed` to add.
   * @returns This `FeedDict`.
   */
  addFeed(feed) {
    this.add(feed.key, feed.value);
  }
  /**
   * Probe whether a key already exists in the FeedDict.
   * @param key
   */
  hasKey(key) {
    return this.id2Value[key.id] != null;
  }
  /**
   * Get all the SymbolicTensor available in this FeedDict.
   */
  names() {
    return Object.keys(this.name2Id);
  }
  /**
   * Get the feed value for given key.
   * @param key The SymbolicTensor, or its name (as a string), of which the
   *     value is sought.
   * @returns If `key` exists, the corresponding feed value.
   * @throws ValueError: If `key` does not exist in this `FeedDict`.
   */
  getValue(key) {
    if (key instanceof SymbolicTensor) {
      if (this.id2Value[key.id] == null) {
        throw new ValueError(`Nonexistent key: ${key.name}`);
      } else {
        return this.id2Value[key.id];
      }
    } else {
      const id = this.name2Id[key];
      if (id == null) {
        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);
      }
      return this.id2Value[id];
    }
  }
  /**
   * Get the feed mask for given key.
   * @param key The SymbolicTensor, or its name (as a string), of which the
   *     value is sought.
   * @returns If `key` exists, the corresponding feed mask.
   * @throws ValueError: If `key` does not exist in this `FeedDict`.
   */
  getMask(key) {
    if (key instanceof SymbolicTensor) {
      if (this.id2Value[key.id] == null) {
        throw new ValueError(`Nonexistent key: ${key.name}`);
      } else {
        return this.id2Mask[key.id];
      }
    } else {
      const id = this.name2Id[key];
      if (id == null) {
        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);
      }
      return this.id2Mask[id];
    }
  }
  /** Dispose all mask Tensors held by this object. */
  disposeMasks() {
    if (this.id2Mask != null) {
      dispose(this.id2Mask);
    }
  }
};
var cachedSorted = new LruCache();
var cachedRecipientCounts = new LruCache();
function updateCacheMaxEntries(maxEntries) {
  if (cachedSorted != null) {
    cachedSorted.setMaxEntries(maxEntries);
  }
  if (cachedRecipientCounts != null) {
    cachedRecipientCounts.setMaxEntries(maxEntries);
  }
}
function execute(fetches, feedDict, kwargs, probe) {
  const training = kwargs == null ? false : kwargs["training"];
  const arrayFetches = Array.isArray(fetches);
  const fetchArray = arrayFetches ? fetches : [fetches];
  const outputNames = fetchArray.map((t) => t.name);
  const finalOutputs = [];
  const feedNames = feedDict.names();
  for (const outputName of outputNames) {
    if (feedNames.indexOf(outputName) !== -1) {
      finalOutputs.push(feedDict.getValue(outputName));
    } else {
      finalOutputs.push(null);
    }
  }
  if (probe != null) {
    probe.maxNumTensors = -Infinity;
    probe.minNumTensors = Infinity;
  }
  const fetchAndFeedKey = outputNames.join(",") + "|" + feedDict.names().sort().join(",");
  let sorted = cachedSorted.get(fetchAndFeedKey);
  let recipientCounts;
  if (sorted == null) {
    const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);
    sorted = out.sorted;
    recipientCounts = out.recipientCounts;
    cachedSorted.put(fetchAndFeedKey, sorted);
    cachedRecipientCounts.put(fetchAndFeedKey, recipientCounts);
  }
  recipientCounts = {};
  if (!training) {
    Object.assign(recipientCounts, cachedRecipientCounts.get(fetchAndFeedKey));
  }
  const internalFeedDict = new FeedDict(feedDict);
  for (let i = 0; i < sorted.length; ++i) {
    if (probe != null) {
      const numTensors = memory().numTensors;
      if (numTensors > probe.maxNumTensors) {
        probe.maxNumTensors = numTensors;
      }
      if (numTensors < probe.minNumTensors) {
        probe.minNumTensors = numTensors;
      }
    }
    const symbolic = sorted[i];
    const srcLayer = symbolic.sourceLayer;
    if (srcLayer instanceof InputLayer) {
      continue;
    }
    const inputValues = [];
    const inputMasks = [];
    const tensorsToDispose = [];
    let maskExists = false;
    for (const input2 of symbolic.inputs) {
      const value = internalFeedDict.getValue(input2);
      const mask = internalFeedDict.getMask(input2);
      inputValues.push(value);
      inputMasks.push(mask);
      if (mask != null) {
        maskExists = true;
      }
      if (!training) {
        recipientCounts[input2.name]--;
        if (recipientCounts[input2.name] === 0 && !feedDict.hasKey(input2) && outputNames.indexOf(input2.name) === -1 && !value.isDisposed && input2.sourceLayer.stateful !== true) {
          tensorsToDispose.push(value);
        }
      }
    }
    if (maskExists) {
      kwargs = kwargs || {};
      kwargs["mask"] = inputMasks[0];
    }
    const outputTensors = toList(srcLayer.apply(inputValues, kwargs));
    let outputMask = null;
    if (srcLayer.supportsMasking) {
      outputMask = srcLayer.computeMask(inputValues, inputMasks);
    }
    const layerOutputs = getNodeOutputs(symbolic);
    const outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];
    for (let i2 = 0; i2 < outputSymbolicTensors.length; ++i2) {
      if (!internalFeedDict.hasKey(outputSymbolicTensors[i2])) {
        internalFeedDict.add(outputSymbolicTensors[i2], outputTensors[i2], Array.isArray(outputMask) ? outputMask[0] : outputMask);
      }
      const index = outputNames.indexOf(outputSymbolicTensors[i2].name);
      if (index !== -1) {
        finalOutputs[index] = outputTensors[i2];
      }
    }
    if (!training) {
      dispose(tensorsToDispose);
    }
  }
  internalFeedDict.disposeMasks();
  return arrayFetches ? finalOutputs : finalOutputs[0];
}
function getTopologicalSortAndRecipientCounts(fetches, feedDict) {
  util_exports.assert(fetches != null && fetches.length > 0, () => `Expected at least one fetch, got none`);
  let finalSorted = [];
  let finalRecipientMap = {};
  if (fetches.length === 1) {
    const out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);
    finalSorted = out.sorted;
    finalRecipientMap = out.recipientMap;
  } else {
    const visited = /* @__PURE__ */ new Set();
    for (const fetch of fetches) {
      const { sorted, recipientMap } = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict);
      for (const symbolicTensor of sorted) {
        if (!visited.has(symbolicTensor.name)) {
          finalSorted.push(symbolicTensor);
          visited.add(symbolicTensor.name);
        }
      }
      for (const name in recipientMap) {
        if (finalRecipientMap[name] == null) {
          finalRecipientMap[name] = /* @__PURE__ */ new Set();
        }
        recipientMap[name].forEach((recipient) => finalRecipientMap[name].add(recipient));
      }
    }
  }
  return {
    sorted: finalSorted,
    recipientCounts: recipientMap2Counts(finalRecipientMap)
  };
}
function recipientMap2Counts(recipientMap) {
  const recipientCounts = {};
  for (const name in recipientMap) {
    recipientCounts[name] = recipientMap[name].size;
  }
  return recipientCounts;
}
function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {
  const visited = /* @__PURE__ */ new Set();
  const sorted = [];
  const recipientMap = {};
  for (const key of feedDict.names()) {
    visited.add(key);
  }
  const stack2 = [];
  const marks = [];
  stack2.push(fetch);
  while (stack2.length > 0) {
    const top = stack2[stack2.length - 1];
    if (visited.has(top.name)) {
      stack2.pop();
      continue;
    }
    const topIsMarked = marks[marks.length - 1] === stack2.length - 1;
    if (top.inputs.length === 0 || topIsMarked) {
      stack2.pop();
      sorted.push(top);
      visited.add(top.name);
      if (topIsMarked) {
        marks.pop();
      }
    } else {
      marks.push(stack2.length - 1);
      for (const input2 of top.inputs) {
        if (recipientMap[input2.name] == null) {
          recipientMap[input2.name] = /* @__PURE__ */ new Set();
        }
        recipientMap[input2.name].add(top.name);
        if (visited.has(input2.name)) {
          continue;
        }
        stack2.push(input2);
      }
    }
  }
  return { sorted, recipientMap };
}
function getNodeOutputs(fetch) {
  let layerOutputs;
  if (fetch.sourceLayer.inboundNodes.length === 1) {
    layerOutputs = fetch.sourceLayer.output;
  } else {
    let nodeIndex = null;
    for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {
      for (const outputTensor of fetch.sourceLayer.inboundNodes[i].outputTensors) {
        if (outputTensor.id === fetch.id) {
          nodeIndex = i;
          break;
        }
      }
    }
    layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);
  }
  return layerOutputs;
}

// node_modules/@tensorflow/tfjs-layers/dist/flags_layers.js
var ENV = env();
ENV.registerFlag("TOPOLOGICAL_SORT_CACHE_MAX_ENTRIES", () => 100, updateCacheMaxEntries);

// node_modules/@tensorflow/tfjs-core/dist/gradients/Abs_grad.js
var absGradConfig = {
  kernelName: Abs,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => mul(dy, step(cast(x, "float32"), -1)) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Acos_grad.js
var acosGradConfig = {
  kernelName: Acos,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return {
      x: () => {
        const a = square(cast(x, "float32"));
        const b = sqrt(sub(scalar(1), a));
        return neg(div(dy, b));
      }
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Acosh_grad.js
var acoshGradConfig = {
  kernelName: Acosh,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return {
      x: () => {
        const a = sqrt(sub(square(cast(x, "float32")), 1));
        return div(dy, a);
      }
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Add_grad.js
var addGradConfig = {
  kernelName: Add,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const outShape = assertAndGetBroadcastShape(a.shape, b.shape);
    const derA = () => {
      let res = dy;
      const reduceAxes = getReductionAxes(a.shape, outShape);
      if (reduceAxes.length > 0) {
        res = sum(res, reduceAxes);
      }
      return reshape(res, a.shape);
    };
    const derB = () => {
      let res = dy;
      const reduceAxes = getReductionAxes(b.shape, outShape);
      if (reduceAxes.length > 0) {
        res = sum(res, reduceAxes);
      }
      return reshape(res, b.shape);
    };
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/AddN_grad.js
var addNGradConfig = {
  kernelName: AddN,
  saveAllInputs: true,
  gradFunc: (dy, saved) => {
    const ders = {};
    saved.forEach((_, i) => {
      ders[i] = () => dy.clone();
    });
    return ders;
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/ArgMax_grad.js
var argMaxGradConfig = {
  kernelName: ArgMax,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => zerosLike(x) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/ArgMin_grad.js
var argMinGradConfig = {
  kernelName: ArgMin,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => zerosLike(x) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Asin_grad.js
var asinGradConfig = {
  kernelName: Asin,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => div(dy, sqrt(sub(scalar(1), square(cast(x, "float32"))))) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Asinh_grad.js
var asinhGradConfig = {
  kernelName: Asinh,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return {
      x: () => {
        const a = sqrt(add(scalar(1), square(cast(x, "float32"))));
        return div(dy, a);
      }
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Atan2_grad.js
var atan2GradConfig = {
  kernelName: Atan2,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const outShape = assertAndGetBroadcastShape(a.shape, b.shape);
    const derA = () => {
      const d = add(square(a), square(b));
      let res = mul(dy, div(b, d));
      const reduceAxes = getReductionAxes(a.shape, outShape);
      if (reduceAxes.length > 0) {
        res = sum(res, reduceAxes);
      }
      return reshape(res, a.shape);
    };
    const derB = () => {
      const d = add(square(a), square(b));
      let res = neg(mul(dy, div(a, d)));
      const reduceAxes = getReductionAxes(b.shape, outShape);
      if (reduceAxes.length > 0) {
        res = sum(res, reduceAxes);
      }
      return reshape(res, b.shape);
    };
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Atan_grad.js
var atanGradConfig = {
  kernelName: Atan,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => div(dy, add(square(cast(x, "float32")), 1)) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Atanh_grad.js
var atanhGradConfig = {
  kernelName: Atanh,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => div(dy, sub(scalar(1), square(cast(x, "float32")))) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/ops/avg_pool_3d_grad.js
function avgPool3dGrad_(dy, input2, filterSize, strides, pad2, dimRoundingMode) {
  const $dy = convertToTensor(dy, "dy", "avgPool3dGrad");
  const $input = convertToTensor(input2, "input", "avgPool3dGrad");
  let dy5D = $dy;
  let input5D = $input;
  let reshapedTo5D = false;
  if ($input.rank === 4) {
    reshapedTo5D = true;
    dy5D = reshape($dy, [1, $dy.shape[0], $dy.shape[1], $dy.shape[2], $dy.shape[3]]);
    input5D = reshape($input, [
      1,
      $input.shape[0],
      $input.shape[1],
      $input.shape[2],
      $input.shape[3]
    ]);
  }
  assert(dy5D.rank === 5, () => `Error in avgPool3dGrad: dy must be rank 5 but got rank ${dy5D.rank}.`);
  assert(input5D.rank === 5, () => `Error in avgPool3dGrad: input must be rank 5 but got rank ${input5D.rank}.`);
  checkPadOnDimRoundingMode("avgPool3dGrad", pad2, dimRoundingMode);
  const inputs = { dy: dy5D, input: input5D };
  const attrs = { filterSize, strides, pad: pad2, dimRoundingMode };
  const res = ENGINE.runKernel(AvgPool3DGrad, inputs, attrs);
  if (reshapedTo5D) {
    return reshape(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);
  }
  return res;
}
var avgPool3dGrad = op({ avgPool3dGrad_ });

// node_modules/@tensorflow/tfjs-core/dist/gradients/AvgPool3D_grad.js
var avgPool3DGradConfig = {
  kernelName: AvgPool3D,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const { filterSize, strides, pad: pad2, dimRoundingMode } = attrs;
    return {
      x: () => avgPool3dGrad(dy, x, filterSize, strides, pad2, dimRoundingMode)
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/ops/avg_pool_grad.js
function avgPoolGrad_(dy, input2, filterSize, strides, pad2) {
  const $dy = convertToTensor(dy, "dy", "avgPoolGrad");
  const $input = convertToTensor(input2, "input", "avgPoolGrad");
  assert($input.rank === $dy.rank, () => `Rank of input (${$input.rank}) does not match rank of dy (${$dy.rank})`);
  let input4D = $input;
  let dy4D = $dy;
  let reshapedTo4D = false;
  if ($input.rank === 3) {
    reshapedTo4D = true;
    input4D = reshape($input, [1, $input.shape[0], $input.shape[1], $input.shape[2]]);
    dy4D = reshape($dy, [1, $dy.shape[0], $dy.shape[1], $dy.shape[2]]);
  }
  assert(dy4D.rank === 4, () => `Error in avgPoolGrad: dy must be rank 4 but got rank ${dy4D.rank}.`);
  assert(input4D.rank === 4, () => `Error in avgPoolGrad: input must be rank 4 but got rank ${input4D.rank}.`);
  const inputs = { dy: dy4D, input: input4D };
  const attrs = { filterSize, strides, pad: pad2 };
  const res = ENGINE.runKernel(AvgPoolGrad, inputs, attrs);
  if (reshapedTo4D) {
    return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);
  }
  return res;
}
var avgPoolGrad = op({ avgPoolGrad_ });

// node_modules/@tensorflow/tfjs-core/dist/gradients/AvgPool_grad.js
var avgPoolGradConfig = {
  kernelName: AvgPool,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const { filterSize, strides, pad: pad2 } = attrs;
    return { x: () => avgPoolGrad(dy, x, filterSize, strides, pad2) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/BatchMatMul_grad.js
var batchMatMulGradConfig = {
  kernelName: BatchMatMul,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved, attrs) => {
    const [a, b] = saved;
    const { transposeA, transposeB } = attrs;
    if (!transposeA && !transposeB) {
      return {
        a: () => matMul(dy, b, false, true),
        b: () => matMul(a, dy, true, false)
      };
    } else if (!transposeA && transposeB) {
      return {
        a: () => matMul(dy, b, false, false),
        b: () => matMul(dy, a, true, false)
      };
    } else if (transposeA && !transposeB) {
      return {
        a: () => matMul(b, dy, false, true),
        b: () => matMul(a, dy, false, false)
      };
    } else {
      return {
        a: () => matMul(b, dy, true, true),
        b: () => matMul(dy, a, true, true)
      };
    }
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/BatchToSpaceND_grad.js
var batchToSpaceNDGradConfig = {
  kernelName: BatchToSpaceND,
  gradFunc: (dy, saved, attrs) => {
    const { blockShape, crops } = attrs;
    return { x: () => spaceToBatchND(dy, blockShape, crops) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/BroadcastTo_grad.js
var broadcastToGradConfig = {
  kernelName: BroadcastTo,
  gradFunc: (dy, saved, attrs) => {
    const broadCastToAttrs = attrs;
    const inputShape = broadCastToAttrs.inputShape;
    const outputShape = broadCastToAttrs.shape;
    const reps = Array.from(outputShape);
    for (let i = inputShape.length - 1; i >= 0; i--) {
      if (inputShape[i] === outputShape[i]) {
        reps[i] = 1;
      } else if (inputShape[i] !== 1) {
        throw new Error(`broadcastTo(): [${inputShape}] cannot be broadcast to [${outputShape}].`);
      }
    }
    const axes = [];
    for (let i = 0; i < reps.length; i++) {
      if (reps[i] > 1) {
        axes.push(i);
      }
    }
    return { x: () => sum(
      dy,
      axes,
      true
      /* keepDims */
    ) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Cast_grad.js
var castGradConfig = {
  kernelName: Cast,
  gradFunc: (dy) => {
    return { x: () => dy.clone() };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Ceil_grad.js
var ceilGradConfig = {
  kernelName: Ceil,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/ClipByValue_grad.js
var clipByValueGradConfig = {
  kernelName: ClipByValue,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const { clipValueMin, clipValueMax } = attrs;
    return {
      x: () => where(logicalAnd(greaterEqual(x, clipValueMin), lessEqual(x, clipValueMax)), dy, zerosLike(dy))
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/ComplexAbs_grad.js
var complexAbsGradConfig = {
  kernelName: ComplexAbs,
  inputsToSave: ["x"],
  gradFunc: absGradConfig.gradFunc
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Concat_grad.js
var concatGradConfig = {
  kernelName: Concat,
  saveAllInputs: true,
  gradFunc: (dy, saved, attrs) => {
    const shapes = saved.map((t) => t.shape);
    const { axis } = attrs;
    const $axis = parseAxisParam(axis, saved[0].shape)[0];
    const sizeSplits = shapes.map((s) => s[$axis]);
    const derTensors = split(dy, sizeSplits, $axis);
    return derTensors.map((t) => () => t);
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Conv2D_grad.js
var conv2DGradConfig = {
  kernelName: Conv2D,
  inputsToSave: ["x", "filter"],
  gradFunc: (dy, saved, attrs) => {
    const [x4D, $filter] = saved;
    const { dilations, strides, pad: pad2, dataFormat } = attrs;
    assert(tupleValuesAreOne(dilations), () => `Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${dilations}'`);
    return {
      x: () => conv2DBackpropInput(x4D.shape, dy, $filter, strides, pad2, dataFormat),
      filter: () => conv2DBackpropFilter(x4D, dy, $filter.shape, strides, pad2, dataFormat)
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Conv2DBackpropInput_grad.js
var conv2DBackpropInputGradConfig = {
  kernelName: Conv2DBackpropInput,
  inputsToSave: ["dy", "filter"],
  gradFunc: (ddx, saved, attrs) => {
    const [dy, filter] = saved;
    const { strides, pad: pad2, dataFormat, dimRoundingMode } = attrs;
    return {
      dy: () => conv2d(ddx, filter, strides, pad2, dataFormat, 1, dimRoundingMode),
      filter: () => conv2DBackpropFilter(ddx, dy, filter.shape, strides, pad2, dataFormat, dimRoundingMode)
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/ops/conv3d_backprop_filter.js
function conv3DBackpropFilter_(x, dy, filterShape, strides, pad2) {
  let x5D = x;
  if (x.rank === 4) {
    x5D = reshape(x, [1, x.shape[0], x.shape[1], x.shape[2], x.shape[3]]);
  }
  let dy5D = dy;
  if (dy5D.rank === 4) {
    dy5D = reshape(dy, [1, dy.shape[0], dy.shape[1], dy.shape[2], dy.shape[3]]);
  }
  assert(x5D.rank === 5, () => `Error in conv3dDerFilter: input must be rank 5, but got shape ${x5D.shape}.`);
  assert(dy5D.rank === 5, () => `Error in conv3dDerFilter: dy must be rank 5, but got shape ${dy5D.shape}.`);
  assert(filterShape.length === 5, () => `Error in conv3dDerFilter: filterShape must be length 5, but got ${filterShape}.`);
  assert(x5D.shape[4] === filterShape[3], () => `Error in conv3dDerFilter: depth of input ${x5D.shape[4]}) must match input depth in filter (${filterShape[3]}.`);
  assert(dy5D.shape[4] === filterShape[4], () => `Error in conv3dDerFilter: depth of dy (${dy5D.shape[4]}) must match output depth for filter (${filterShape[4]}).`);
  const inputs = { x: x5D, dy: dy5D };
  const attrs = { strides, pad: pad2, filterShape };
  return ENGINE.runKernel(Conv3DBackpropFilterV2, inputs, attrs);
}
var conv3DBackpropFilter = op({ conv3DBackpropFilter_ });

// node_modules/@tensorflow/tfjs-core/dist/gradients/Conv3D_grad.js
var conv3DGradConfig = {
  kernelName: Conv3D,
  inputsToSave: ["x", "filter"],
  gradFunc: (dy, saved, attrs) => {
    const { dilations, strides, pad: pad2 } = attrs;
    assert(tupleValuesAreOne(dilations), () => `Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${dilations}'`);
    const [x5D, $filter] = saved;
    return {
      x: () => conv3DBackpropInput(x5D.shape, dy, $filter, strides, pad2),
      filter: () => conv3DBackpropFilter(x5D, dy, $filter.shape, strides, pad2)
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Cos_grad.js
var cosGradConfig = {
  kernelName: Cos,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => mul(neg(sin(cast(x, "float32"))), dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Cosh_grad.js
var coshGradConfig = {
  kernelName: Cosh,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => mul(sinh(cast(x, "float32")), dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Cumsum_grad.js
var cumsumGradConfig = {
  kernelName: Cumsum,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const { axis, exclusive, reverse: reverse2 } = attrs;
    return {
      x: () => {
        const permutation = getAxesPermutation([axis], x.rank);
        let out = cumsum(dy, axis, exclusive, !reverse2);
        if (permutation != null) {
          out = transpose(out, permutation);
        }
        return out;
      }
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/DepthwiseConv2dNative_grad.js
var depthwiseConv2dNativeGradConfig = {
  kernelName: DepthwiseConv2dNative,
  inputsToSave: ["x", "filter"],
  gradFunc: (dy, saved, attrs) => {
    const { dilations, strides, pad: pad2, dimRoundingMode } = attrs;
    const $dilations = dilations == null ? [1, 1] : dilations;
    assert(tupleValuesAreOne($dilations), () => `Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${$dilations}'`);
    const [x, filter] = saved;
    assert(x.rank === 4, () => `Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${x.rank}.`);
    assert(filter.rank === 4, () => `Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${filter.rank}.`);
    assert(x.shape[3] === filter.shape[2], () => `Error in gradient of depthwiseConv2d: number of input channels (${x.shape[3]}) must match the inChannels dimension in filter ${filter.shape[2]}.`);
    assert(eitherStridesOrDilationsAreOne(strides, $dilations), () => `Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${strides} and dilations '${$dilations}'.`);
    checkPadOnDimRoundingMode("depthwiseConv2d", pad2, dimRoundingMode);
    return {
      x: () => depthwiseConv2dNativeBackpropInput(x.shape, dy, filter, strides, pad2, $dilations, dimRoundingMode),
      filter: () => depthwiseConv2dNativeBackpropFilter(x, dy, filter.shape, strides, pad2, $dilations, dimRoundingMode)
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Dilation2D_grad.js
var dilation2dGradConfig = {
  kernelName: Dilation2D,
  inputsToSave: ["x", "filter"],
  gradFunc: (dy, saved, attrs) => {
    const [x, filter] = saved;
    const inputInputs = { x, filter, dy };
    const filterInputs = { x, filter, dy };
    return {
      x: () => ENGINE.runKernel(Dilation2DBackpropInput, inputInputs, attrs),
      filter: () => ENGINE.runKernel(Dilation2DBackpropFilter, filterInputs, attrs)
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Elu_grad.js
var eluGradConfig = {
  kernelName: Elu,
  outputsToSave: [true],
  gradFunc: (dy, saved) => {
    const [y] = saved;
    const inputs = { dy, y };
    return { x: () => ENGINE.runKernel(EluGrad, inputs) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Erf_grad.js
var erfGradConfig = {
  kernelName: Erf,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    const a = mul(exp(neg(square(x))), 2 / Math.sqrt(Math.PI));
    return { x: () => mul(dy, a) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Exp_grad.js
var expGradConfig = {
  kernelName: Exp,
  outputsToSave: [true],
  gradFunc: (dy, saved) => {
    const [y] = saved;
    return { x: () => mul(dy, y) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/ExpandDims_grad.js
var expandDimsGradConfig = {
  kernelName: ExpandDims,
  inputsToSave: ["input"],
  gradFunc: (dy, saved) => {
    const [input2] = saved;
    return { input: () => reshape(dy, input2.shape) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Expm1_grad.js
var expm1GradConfig = {
  kernelName: Expm1,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => mul(dy, exp(x)) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Floor_grad.js
var floorGradConfig = {
  kernelName: Floor,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/FloorDiv_grad.js
var floorDivGradConfig = {
  kernelName: FloorDiv,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const outShape = assertAndGetBroadcastShape(a.shape, b.shape);
    const derA = () => {
      const res = div(dy, cast(b, "float32"));
      const reduceAxes = getReductionAxes(a.shape, outShape);
      if (reduceAxes.length > 0) {
        return reshape(sum(res, reduceAxes), a.shape);
      }
      return res;
    };
    const derB = () => {
      let res = mul(dy, cast(a, "float32"));
      const reduceAxes = getReductionAxes(b.shape, outShape);
      if (reduceAxes.length > 0) {
        res = reshape(sum(res, reduceAxes), b.shape);
      }
      const tmp = square(b);
      return neg(div(res, cast(tmp, "float32")));
    };
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/FusedBatchNorm_grad.js
var fusedBatchNormGradConfig = {
  kernelName: FusedBatchNorm,
  inputsToSave: ["x", "mean", "variance", "scale"],
  gradFunc: (dy, saved, attrs) => {
    const { varianceEpsilon } = attrs;
    const [x, mean2, variance, scale] = saved;
    const scaleValue = scale == null ? scalar(1) : scale;
    const reductionAxes = getReductionAxes(mean2.shape, x.shape);
    const tileShape = [];
    if (mean2.rank === 1) {
      for (let i = 0; i < x.shape.length - 1; ++i) {
        tileShape.push(x.shape[i]);
      }
      tileShape.push(1);
    }
    const xMinusMean = sub(x, mean2);
    const dyTimesScaleValue = mul(dy, scaleValue);
    const oneOverSqrtVariance = rsqrt(add(variance, scalar(varianceEpsilon)));
    const minusHalfRCube = mul(mul(mul(oneOverSqrtVariance, oneOverSqrtVariance), oneOverSqrtVariance), scalar(-0.5));
    const derX = () => {
      if (mean2.rank === 1) {
        return reshape(mul(mul(dy, tile(reshape(oneOverSqrtVariance, [1, 1, 1, mean2.shape[0]]), tileShape)), scaleValue), x.shape);
      } else {
        return reshape(mul(mul(dy, oneOverSqrtVariance), scaleValue), x.shape);
      }
    };
    const derMean = () => {
      let meanDer = mul(mul(oneOverSqrtVariance, scalar(-1)), dyTimesScaleValue);
      if (mean2.rank === 1) {
        meanDer = sum(meanDer, reductionAxes);
      }
      return reshape(meanDer, mean2.shape);
    };
    const derVariance = () => {
      let varianceDer = mul(mul(minusHalfRCube, xMinusMean), dyTimesScaleValue);
      if (mean2.rank === 1) {
        varianceDer = sum(varianceDer, reductionAxes);
      }
      return reshape(varianceDer, mean2.shape);
    };
    const derScale = () => {
      const xMinusMean2TimesRsqrt = mul(xMinusMean, oneOverSqrtVariance);
      let scaleDer = mul(dy, xMinusMean2TimesRsqrt);
      if (mean2.rank === 1) {
        scaleDer = sum(scaleDer, reductionAxes);
      }
      return reshape(scaleDer, mean2.shape);
    };
    const derOffset = () => {
      let offsetDer = dy;
      if (mean2.rank === 1) {
        offsetDer = sum(offsetDer, reductionAxes);
      }
      return reshape(offsetDer, mean2.shape);
    };
    return {
      x: derX,
      mean: derMean,
      variance: derVariance,
      scale: derScale,
      offset: derOffset
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/GatherV2_grad.js
var gatherGradConfig = {
  kernelName: GatherV2,
  inputsToSave: ["x", "indices"],
  gradFunc: (dy, saved, attrs) => {
    const [x, indices] = saved;
    const { axis } = attrs;
    const parsedAxis = parseAxisParam(axis, x.shape)[0];
    const derX = () => {
      const paramsShape = x.shape;
      const indicesSize = indices.size;
      const outerShape = paramsShape.slice(0, parsedAxis);
      const outerDims = outerShape.length;
      const innerShape = paramsShape.slice(axis, paramsShape.length).slice(1);
      const innerDims = innerShape.length;
      const outerAxesIndices = arrayRange(0, outerDims);
      const innerAxesIndices = arrayRange(outerDims + 1, outerDims + 1 + innerDims);
      const valuesShape = arrayConcat([outerShape, [indicesSize], innerShape]);
      const values = reshape(dy, valuesShape);
      const reshapedIndices = reshape(indices, [indicesSize]);
      const transposeDims = arrayConcat([[outerDims], outerAxesIndices, innerAxesIndices]);
      const valuesTranspose = transpose(values, transposeDims);
      let paramsGrad = unsortedSegmentSum(valuesTranspose, reshapedIndices, x.shape[parsedAxis]);
      const invertTransposeDims = getUndoAxesPermutation(transposeDims);
      paramsGrad = transpose(paramsGrad, invertTransposeDims);
      return paramsGrad;
    };
    return { x: derX, indices: () => indices };
  }
};
function arrayRange(start, stop) {
  const result = [];
  for (let i = start; i < stop; ++i) {
    result.push(i);
  }
  return result;
}
function arrayConcat(arrays) {
  const result = [];
  for (let i = 0; i < arrays.length; ++i) {
    for (let j = 0; j < arrays[i].length; ++j) {
      result.push(arrays[i][j]);
    }
  }
  return result;
}

// node_modules/@tensorflow/tfjs-core/dist/gradients/GreaterEqual_grad.js
var greaterEqualGradConfig = {
  kernelName: GreaterEqual,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    return { a: () => zerosLike(a), b: () => zerosLike(b) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Identity_grad.js
var identityGradConfig = {
  kernelName: Identity,
  gradFunc: (dy) => {
    return { x: () => cast(dy, "float32") };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/IsFinite_grad.js
var isFiniteGradConfig = {
  kernelName: IsFinite,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/IsInf_grad.js
var isInfGradConfig = {
  kernelName: IsInf,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/IsNan_grad.js
var isNanGradConfig = {
  kernelName: IsNan,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/LeakyRelu_grad.js
var leakyReluGradConfig = {
  kernelName: LeakyRelu,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const { alpha } = attrs;
    const mask = greater(x, 0);
    return { x: () => where(mask, dy, mul(dy, alpha)) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Log1p_grad.js
var log1pGradConfig = {
  kernelName: Log1p,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => div(dy, add(x, 1)) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Log_grad.js
var logGradConfig = {
  kernelName: Log,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => div(dy, cast(x, "float32")) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/LogSoftmax_grad.js
var logSoftmaxGradConfig = {
  kernelName: LogSoftmax,
  inputsToSave: [],
  outputsToSave: [true],
  gradFunc: (dy, saved, attrs) => {
    const [value] = saved;
    const { axis } = attrs;
    return {
      logits: () => {
        const keepDims = true;
        const softmax3 = exp(value);
        return sub(dy, mul(sum(dy, axis, keepDims), softmax3));
      }
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/ops/local_response_normalization_backprop.js
function localResponseNormalizationBackprop_(x, y, dy, depthRadius = 5, bias = 1, alpha = 1, beta = 0.5) {
  const inputs = { x, y, dy };
  const attrs = { depthRadius, bias, alpha, beta };
  return ENGINE.runKernel(LRNGrad, inputs, attrs);
}
var localResponseNormalizationBackprop = op({ localResponseNormalizationBackprop_ });

// node_modules/@tensorflow/tfjs-core/dist/gradients/LRN_grad.js
var lrnGradConfig = {
  kernelName: LRN,
  inputsToSave: ["x"],
  outputsToSave: [true],
  gradFunc: (dy, saved, attrs) => {
    const [x, y] = saved;
    const { depthRadius, bias, alpha, beta } = attrs;
    return {
      x: () => localResponseNormalizationBackprop(x, y, dy, depthRadius, bias, alpha, beta)
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/min_max_grad_util.js
function gradForMinAndMax(dy, y, xOrig, origAxes) {
  if (y.rank < xOrig.rank) {
    y = reshape(y, expandShapeToKeepDim(y.shape, origAxes));
  }
  if (dy.rank < xOrig.rank) {
    dy = reshape(dy, expandShapeToKeepDim(dy.shape, origAxes));
  }
  return {
    x: () => {
      const dx = mul(dy, cast(equal(xOrig, y), dy.dtype));
      return dx;
    }
  };
}

// node_modules/@tensorflow/tfjs-core/dist/gradients/Max_grad.js
var maxGradConfig = {
  kernelName: Max,
  inputsToSave: ["x"],
  outputsToSave: [true],
  gradFunc: (dy, saved, attrs) => {
    const maxAttrs = attrs;
    const { reductionIndices } = maxAttrs;
    const x = saved[0];
    const y = saved[1];
    const origAxes = parseAxisParam(reductionIndices, x.shape);
    const maxGrad = gradForMinAndMax(dy, y, x, origAxes);
    return {
      x: () => {
        return maxGrad["x"]();
      }
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Maximum_grad.js
var maximumGradConfig = {
  kernelName: Maximum,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const derA = () => mul(dy, cast(greaterEqual(a, b), "float32"));
    const derB = () => mul(dy, cast(less(a, b), "float32"));
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_3d_grad.js
function maxPool3dGrad_(dy, input2, output, filterSize, strides, pad2, dimRoundingMode) {
  const $dy = convertToTensor(dy, "dy", "maxPool3dGrad");
  const $input = convertToTensor(input2, "input", "maxPool3dGrad");
  const $output = convertToTensor(output, "output", "maxPool3dGrad");
  let dy5D = $dy;
  let input5D = $input;
  let output5D = $output;
  let reshapedTo5D = false;
  if ($input.rank === 4) {
    reshapedTo5D = true;
    dy5D = reshape($dy, [1, $dy.shape[0], $dy.shape[1], $dy.shape[2], $dy.shape[3]]);
    input5D = reshape($input, [
      1,
      $input.shape[0],
      $input.shape[1],
      $input.shape[2],
      $input.shape[3]
    ]);
    output5D = reshape($output, [
      1,
      $output.shape[0],
      $output.shape[1],
      $output.shape[2],
      $output.shape[3]
    ]);
  }
  assert(dy5D.rank === 5, () => `Error in maxPool3dGrad: dy must be rank 5 but got rank ${dy5D.rank}.`);
  assert(input5D.rank === 5, () => `Error in maxPool3dGrad: input must be rank 5 but got rank ${input5D.rank}.`);
  assert(output5D.rank === 5, () => `Error in maxPool3dGrad: output must be rank 5 but got rank ${output5D.rank}.`);
  checkPadOnDimRoundingMode("maxPool3dGrad", pad2, dimRoundingMode);
  const inputs = { dy: dy5D, input: input5D, output: output5D };
  const attrs = { filterSize, strides, pad: pad2, dimRoundingMode };
  const res = ENGINE.runKernel(MaxPool3DGrad, inputs, attrs);
  if (reshapedTo5D) {
    return reshape(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);
  }
  return res;
}
var maxPool3dGrad = op({ maxPool3dGrad_ });

// node_modules/@tensorflow/tfjs-core/dist/gradients/MaxPool3D_grad.js
var maxPool3DGradConfig = {
  kernelName: MaxPool3D,
  inputsToSave: ["x"],
  outputsToSave: [true],
  gradFunc: (dy, saved, attrs) => {
    const [x, y] = saved;
    const { filterSize, strides, pad: pad2, dimRoundingMode } = attrs;
    return {
      x: () => maxPool3dGrad(dy, x, y, filterSize, strides, pad2, dimRoundingMode)
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_grad.js
function maxPoolGrad_(dy, input2, output, filterSize, strides, pad2, dimRoundingMode) {
  const $dy = convertToTensor(dy, "dy", "maxPoolGrad");
  const $input = convertToTensor(input2, "input", "maxPoolGrad");
  const $output = convertToTensor(output, "output", "maxPoolGrad");
  assert($input.rank === $dy.rank, () => `Rank of input (${$input.rank}) does not match rank of dy (${$dy.rank})`);
  assert($dy.rank === 4, () => `Error in maxPoolGrad: dy must be rank 4 but got rank ${$dy.rank}.`);
  assert($input.rank === 4, () => `Error in maxPoolGrad: input must be rank 4 but got rank ${$input.rank}.`);
  checkPadOnDimRoundingMode("maxPoolGrad", pad2, dimRoundingMode);
  const inputs = { dy: $dy, input: $input, output: $output };
  const attrs = { filterSize, strides, pad: pad2, dimRoundingMode };
  return ENGINE.runKernel(MaxPoolGrad, inputs, attrs);
}
var maxPoolGrad = op({ maxPoolGrad_ });

// node_modules/@tensorflow/tfjs-core/dist/gradients/MaxPool_grad.js
var maxPoolGradConfig = {
  kernelName: MaxPool,
  inputsToSave: ["x"],
  outputsToSave: [true],
  gradFunc: (dy, saved, attrs) => {
    const [x, y] = saved;
    const { filterSize, strides, pad: pad2 } = attrs;
    return {
      x: () => maxPoolGrad(dy, x, y, filterSize, strides, pad2)
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Mean_grad.js
var meanGradConfig = {
  kernelName: Mean,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const { axis } = attrs;
    const axes = parseAxisParam(axis, x.shape);
    const shapes = computeOutAndReduceShapes(x.shape, axes);
    const reduceShape = shapes[1];
    const reduceSize = sizeFromShape(reduceShape);
    const derX = () => {
      const expandedDyShape = x.shape.slice();
      axes.forEach((axis2) => {
        expandedDyShape[axis2] = 1;
      });
      const expandedDy = reshape(dy, expandedDyShape);
      const res = div(mul(expandedDy, ones(x.shape, "float32")), reduceSize);
      return res;
    };
    return { x: derX };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Min_grad.js
var minGradConfig = {
  kernelName: Min,
  inputsToSave: ["x"],
  outputsToSave: [true],
  gradFunc: (dy, saved, attrs) => {
    const minAttrs = attrs;
    const { axis } = minAttrs;
    const [x, y] = saved;
    const origAxes = parseAxisParam(axis, x.shape);
    const minGrad = gradForMinAndMax(dy, y, x, origAxes);
    return {
      x: () => {
        return minGrad["x"]();
      }
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Minimum_grad.js
var minimumGradConfig = {
  kernelName: Minimum,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const derA = () => mul(dy, cast(lessEqual(a, b), "float32"));
    const derB = () => mul(dy, cast(greater(a, b), "float32"));
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/MirrorPad_grad.js
var mirrorPadGradConfig = {
  kernelName: MirrorPad,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const x = saved[0];
    const { paddings } = attrs;
    const begin = paddings.map((p) => p[0]);
    return { x: () => slice(dy, begin, x.shape) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Mod_grad.js
var modGradConfig = {
  kernelName: Mod,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const outShape = assertAndGetBroadcastShape(a.shape, b.shape);
    const derA = () => {
      const reduceAxes = getReductionAxes(a.shape, outShape);
      if (reduceAxes.length > 0) {
        return reshape(sum(dy, reduceAxes), a.shape);
      }
      return dy;
    };
    const derB = () => {
      const res = mul(dy, neg(floor(div(a, b))));
      const reduceAxes = getReductionAxes(b.shape, outShape);
      if (reduceAxes.length > 0) {
        return reshape(sum(res, reduceAxes), b.shape);
      }
      return res;
    };
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Multiply_grad.js
var multiplyGradConfig = {
  kernelName: Multiply,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const outShape = assertAndGetBroadcastShape(a.shape, b.shape);
    const derA = () => {
      const res = mul(dy, cast(b, "float32"));
      const reduceAxes = getReductionAxes(a.shape, outShape);
      if (reduceAxes.length > 0) {
        return reshape(sum(res, reduceAxes), a.shape);
      }
      return res;
    };
    const derB = () => {
      const res = mul(dy, cast(a, "float32"));
      const reduceAxes = getReductionAxes(b.shape, outShape);
      if (reduceAxes.length > 0) {
        return reshape(sum(res, reduceAxes), b.shape);
      }
      return res;
    };
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Neg_grad.js
var negGradConfig = {
  kernelName: Neg,
  gradFunc: (dy) => {
    return { x: () => neg(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/OneHot_grad.js
var oneHotGradConfig = {
  kernelName: OneHot,
  inputsToSave: ["indices"],
  gradFunc: (dy, saved) => {
    const indices = saved[0];
    return { indices: () => zeros(indices.shape, "float32") };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/OnesLike_grad.js
var onesLikeGradConfig = {
  kernelName: OnesLike,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Pack_grad.js
var packGradConfig = {
  kernelName: Pack,
  saveAllInputs: true,
  gradFunc: (dy, saved, attrs) => {
    const { axis } = attrs;
    const derTensors = unstack(dy, axis);
    return derTensors.map((t) => () => t);
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/PadV2_grad.js
var padV2GradConfig = {
  kernelName: PadV2,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const x = saved[0];
    const { paddings } = attrs;
    const begin = paddings.map((p) => p[0]);
    return { x: () => slice(dy, begin, x.shape) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Pow_grad.js
var powGradConfig = {
  kernelName: Pow,
  inputsToSave: ["a", "b"],
  outputsToSave: [true],
  gradFunc: (dy, saved) => {
    const [a, b, y] = saved;
    const base = a;
    const exp2 = b;
    const outShape = assertAndGetBroadcastShape(base.shape, exp2.shape);
    const derBase = () => {
      const expFloat = cast(exp2, "float32");
      let res = mul(dy, mul(expFloat, pow(base, sub(expFloat, scalar(1)))));
      const reduceAxes = getReductionAxes(base.shape, outShape);
      if (reduceAxes.length > 0) {
        res = sum(res, reduceAxes);
      }
      return reshape(res, base.shape);
    };
    const derExp = () => {
      const condition = greater(base, 0);
      const logBase = where(condition, log(base), zerosLike(base));
      let res = mul(dy, mul(y, logBase));
      const reduceAxes = getReductionAxes(exp2.shape, outShape);
      if (reduceAxes.length > 0) {
        res = sum(res, reduceAxes);
      }
      return reshape(res, exp2.shape);
    };
    return { a: derBase, b: derExp };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Prelu_grad.js
var preluGradConfig = {
  kernelName: Prelu,
  inputsToSave: ["x", "alpha"],
  gradFunc: (dy, saved) => {
    const [x, alpha] = saved;
    const mask = greater(x, 0);
    return {
      x: () => where(mask, dy, mul(dy, alpha)),
      alpha: () => {
        let res = where(mask, zerosLike(dy), mul(dy, x));
        const reduceAxes = getReductionAxes(alpha.shape, dy.shape);
        if (reduceAxes.length > 0) {
          res = sum(res, reduceAxes);
        }
        return reshape(res, alpha.shape);
      }
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Prod_grad.js
function prodGradFn_(x, dy, axis) {
  const expandedYShape = x.shape.slice();
  expandedYShape[axis] = 1;
  const expandedDy = reshape(dy, expandedYShape);
  const xCumProd = cumprod(x, axis, true, false);
  const xCumRevProd = cumprod(x, axis, true, true);
  const dx = mul(xCumProd, xCumRevProd);
  return mul(expandedDy, dx);
}
function prodsGradFn_(x, dy, axis) {
  const xRank = x.shape.length;
  const finalProdAxis = xRank - axis.length;
  const xPermutation = backend_util_exports.getAxesPermutation(axis, xRank);
  let permutedX = x;
  if (xPermutation != null) {
    permutedX = transpose(x, xPermutation);
  }
  const newShape = permutedX.shape.slice();
  const removedShape = newShape.splice(xRank - axis.length, axis.length);
  const endPartShape = removedShape.reduce((p, c) => p * c, 1);
  newShape.push(endPartShape);
  const reshapedPermutedX = permutedX.reshape(newShape);
  let prodGrad = prodGradFn_(reshapedPermutedX, dy, finalProdAxis);
  prodGrad = prodGrad.reshape(permutedX.shape);
  if (xPermutation != null) {
    const undoPermutation = backend_util_exports.getUndoAxesPermutation(xPermutation);
    prodGrad = transpose(prodGrad, undoPermutation);
  }
  return prodGrad;
}
var prodGradConfig = {
  kernelName: Prod,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const { axis } = attrs;
    let axisArr = [];
    if (axis === void 0 || axis === null) {
      axisArr = x.shape.map((_, i) => i);
    } else if (typeof axis === "number") {
      axisArr = [axis];
    } else {
      axisArr = axis;
    }
    return { x: () => prodsGradFn_(x, dy, axisArr) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/RealDiv_grad.js
var divGradConfig = {
  kernelName: RealDiv,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const outShape = assertAndGetBroadcastShape(a.shape, b.shape);
    const derA = () => {
      const res = div(dy, cast(b, "float32"));
      const reduceAxes = getReductionAxes(a.shape, outShape);
      if (reduceAxes.length > 0) {
        return reshape(sum(res, reduceAxes), a.shape);
      }
      return res;
    };
    const derB = () => {
      let res = mul(dy, cast(a, "float32"));
      const reduceAxes = getReductionAxes(b.shape, outShape);
      if (reduceAxes.length > 0) {
        res = reshape(sum(res, reduceAxes), b.shape);
      }
      const tmp = square(b);
      return neg(div(res, cast(tmp, "float32")));
    };
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Reciprocal_grad.js
var reciprocalGradConfig = {
  kernelName: Reciprocal,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => div(dy, neg(square(x))) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Relu6_grad.js
var relu6GradConfig = {
  kernelName: Relu6,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    const mask = mul(lessEqual(x, 6), step(x));
    return { x: () => mul(dy, cast(mask, "float32")) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Relu_grad.js
var reluGradConfig = {
  kernelName: Relu,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => mul(dy, cast(step(x), "float32")) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Reshape_grad.js
var reshapeGradConfig = {
  kernelName: Reshape,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => reshape(dy, x.shape) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/ResizeBilinear_grad.js
var resizeBilinearGradConfig = {
  kernelName: ResizeBilinear,
  inputsToSave: ["images"],
  gradFunc: (dy, saved, attrs) => {
    const [images] = saved;
    const inputs = { dy, images };
    const imagesDer = () => (
      // tslint:disable-next-line: no-unnecessary-type-assertion
      ENGINE.runKernel(ResizeBilinearGrad, inputs, attrs)
    );
    return { images: imagesDer };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/ResizeNearestNeighbor_grad.js
var resizeNearestNeighborGradConfig = {
  kernelName: ResizeNearestNeighbor,
  inputsToSave: ["images"],
  gradFunc: (dy, saved, attrs) => {
    const [images] = saved;
    const inputs = { dy, images };
    const imagesDer = () => (
      // tslint:disable-next-line: no-unnecessary-type-assertion
      ENGINE.runKernel(ResizeNearestNeighborGrad, inputs, attrs)
    );
    return { images: imagesDer };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Reverse_grad.js
var reverseGradConfig = {
  kernelName: Reverse,
  gradFunc: (dy, saved, attrs) => {
    const { dims } = attrs;
    const axes = parseAxisParam(dims, dy.shape);
    return { x: () => reverse(dy, axes) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Round_grad.js
var roundGradConfig = {
  kernelName: Round,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Rsqrt_grad.js
var rsqrtGradConfig = {
  kernelName: Rsqrt,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => neg(div(dy, mul(pow(x, 1.5), 2))) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Select_grad.js
var selectGradConfig = {
  kernelName: Select,
  inputsToSave: ["condition"],
  gradFunc: (dy, saved) => {
    const [condition] = saved;
    return {
      // TODO(julianoks): Return null for condition gradient
      // when backprop supports it.
      condition: () => cast(zerosLike(condition), "float32"),
      t: () => mul(dy, cast(condition, dy.dtype)),
      e: () => mul(dy, cast(logicalNot(condition), dy.dtype))
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Selu_grad.js
var seluGradConfig = {
  kernelName: Selu,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return {
      x: () => {
        const mask = greater(x, scalar(0));
        const scaleAlpha = scalar(SELU_SCALEALPHA);
        const scale = scalar(SELU_SCALE);
        const greaterThanZeroDer = mul(dy, scale);
        const lessEqualZeroDer = mul(mul(dy, scaleAlpha), exp(cast(x, "float32")));
        return where(mask, greaterThanZeroDer, lessEqualZeroDer);
      }
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Sigmoid_grad.js
var sigmoidGradConfig = {
  kernelName: Sigmoid,
  outputsToSave: [true],
  gradFunc: (dy, saved) => {
    const [y] = saved;
    return { x: () => mul(dy, mul(y, sub(scalar(1), y))) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Sign_grad.js
var signGradConfig = {
  kernelName: Sign,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Sin_grad.js
var sinGradConfig = {
  kernelName: Sin,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => mul(cos(cast(x, "float32")), dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Sinh_grad.js
var sinhGradConfig = {
  kernelName: Sinh,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => mul(cosh(cast(x, "float32")), dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Slice_grad.js
var sliceGradConfig = {
  kernelName: Slice,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const { begin, size } = attrs;
    const inputShape = x.shape;
    const [begin_, size_] = parseSliceParams(x, begin, size);
    const paddings = [];
    for (let i = 0; i < dy.rank; i++) {
      paddings.push([begin_[i], inputShape[i] - begin_[i] - size_[i]]);
    }
    return { x: () => pad(dy, paddings) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Softmax_grad.js
var softmaxGradConfig = {
  kernelName: Softmax,
  outputsToSave: [true],
  gradFunc: (dy, saved, attrs) => {
    const [y] = saved;
    const { dim } = attrs;
    const keepDims = true;
    const dyTimesY = mul(dy, y);
    return {
      logits: () => sub(dyTimesY, mul(sum(dyTimesY, [dim], keepDims), y))
    };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Softplus_grad.js
var softplusGradConfig = {
  kernelName: Softplus,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => mul(dy, sigmoid(x)) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/SpaceToBatchND_grad.js
var spaceToBatchNDGradConfig = {
  kernelName: SpaceToBatchND,
  gradFunc: (dy, saved, attrs) => {
    const { blockShape, paddings } = attrs;
    return { x: () => batchToSpaceND(dy, blockShape, paddings) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/SplitV_grad.js
var splitVGradConfig = {
  kernelName: SplitV,
  gradFunc: (dy, saved, attrs) => {
    const { axis } = attrs;
    return { x: () => concat(dy, axis) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Sqrt_grad.js
var sqrtGradConfig = {
  kernelName: Sqrt,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => div(dy, mul(sqrt(cast(x, "float32")), 2)) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Square_grad.js
var squareGradConfig = {
  kernelName: Square,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => mul(dy, mul(cast(x, "float32"), 2)) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/SquaredDifference_grad.js
var squaredDifferenceGradConfig = {
  kernelName: SquaredDifference,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const two = scalar(2);
    const derA = () => mul(dy, mul(two, sub(a, b)));
    const derB = () => mul(dy, mul(two, sub(b, a)));
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Step_grad.js
var stepGradConfig = {
  kernelName: Step,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Sub_grad.js
var subGradConfig = {
  kernelName: Sub,
  inputsToSave: ["a", "b"],
  gradFunc: (dy, saved) => {
    const [a, b] = saved;
    const outShape = assertAndGetBroadcastShape(a.shape, b.shape);
    const derA = () => {
      let res = dy;
      const reduceAxes = getReductionAxes(a.shape, outShape);
      if (reduceAxes.length > 0) {
        res = sum(res, reduceAxes);
      }
      return reshape(res, a.shape);
    };
    const derB = () => {
      let res = dy;
      const reduceAxes = getReductionAxes(b.shape, outShape);
      if (reduceAxes.length > 0) {
        res = sum(res, reduceAxes);
      }
      return reshape(neg(res), b.shape);
    };
    return { a: derA, b: derB };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Sum_grad.js
var sumGradConfig = {
  kernelName: Sum,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const expandedDyShape = x.shape.slice();
    const { axis } = attrs;
    const axes = parseAxisParam(axis, x.shape);
    axes.forEach((axis2) => {
      expandedDyShape[axis2] = 1;
    });
    const expandedDy = reshape(dy, expandedDyShape);
    const derX = mul(expandedDy, ones(x.shape, "float32"));
    return { x: () => derX };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Tan_grad.js
var tanGradConfig = {
  kernelName: Tan,
  inputsToSave: ["x"],
  gradFunc: (dy, saved) => {
    const [x] = saved;
    return { x: () => div(dy, square(cos(x))) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Tanh_grad.js
var tanhGradConfig = {
  kernelName: Tanh,
  outputsToSave: [true],
  gradFunc: (dy, saved) => {
    const [y] = saved;
    return { x: () => mul(sub(scalar(1), square(y)), dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Tile_grad.js
var tileGradConfig = {
  kernelName: Tile,
  inputsToSave: ["x"],
  gradFunc: (dy, saved, attrs) => {
    const [x] = saved;
    const { reps } = attrs;
    const derX = () => {
      let xGrad = zerosLike(x);
      if (x.rank === 1) {
        for (let i = 0; i < reps[0]; ++i) {
          xGrad = add(xGrad, slice(dy, [i * x.shape[0]], [x.shape[0]]));
        }
      } else if (x.rank === 2) {
        for (let i = 0; i < reps[0]; ++i) {
          for (let j = 0; j < reps[1]; ++j) {
            xGrad = add(xGrad, slice(dy, [i * x.shape[0], j * x.shape[1]], [
              x.shape[0],
              x.shape[1]
            ]));
          }
        }
      } else if (x.rank === 3) {
        for (let i = 0; i < reps[0]; ++i) {
          for (let j = 0; j < reps[1]; ++j) {
            for (let k = 0; k < reps[2]; ++k) {
              xGrad = add(xGrad, slice(dy, [i * x.shape[0], j * x.shape[1], k * x.shape[2]], [x.shape[0], x.shape[1], x.shape[2]]));
            }
          }
        }
      } else if (x.rank === 4) {
        for (let i = 0; i < reps[0]; ++i) {
          for (let j = 0; j < reps[1]; ++j) {
            for (let k = 0; k < reps[2]; ++k) {
              for (let l = 0; l < reps[3]; ++l) {
                xGrad = add(xGrad, slice(dy, [
                  i * x.shape[0],
                  j * x.shape[1],
                  k * x.shape[2],
                  l * x.shape[3]
                ], [x.shape[0], x.shape[1], x.shape[2], x.shape[3]]));
              }
            }
          }
        }
      } else {
        throw new Error(`Gradient for tile operation is not implemented for rank-${x.rank} tensors yet.`);
      }
      return xGrad;
    };
    return { x: derX };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Transpose_grad.js
var transposeGradConfig = {
  kernelName: Transpose,
  gradFunc: (dy, saved, attrs) => {
    const transposeAttrs = attrs;
    const { perm } = transposeAttrs;
    const undoPerm = getUndoAxesPermutation(perm);
    return { x: () => transpose(dy, undoPerm) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/Unpack_grad.js
var unpackGradConfig = {
  kernelName: Unpack,
  gradFunc: (dy, saved, attrs) => {
    const unpackAttrs = attrs;
    const { axis } = unpackAttrs;
    return { value: () => stack(dy, axis) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/gradients/UnsortedSegmentSum_grad.js
var unsortedSegmentSumGradConfig = {
  kernelName: UnsortedSegmentSum,
  inputsToSave: ["segmentIds"],
  gradFunc: (dy, saved) => {
    const [segmentIds] = saved;
    const derX = () => {
      return gatherDropNegatives(dy, segmentIds);
    };
    return { x: derX };
  }
};
function gatherDropNegatives(x, indices) {
  const zeroClippedIndices = maximum(indices, zerosLike(indices));
  const gathered = gather(x, zeroClippedIndices);
  let isPositive = greaterEqual(indices, scalar(0, "int32"));
  const numIters = gathered.rank - isPositive.rank;
  for (let i = 0; i < numIters; ++i) {
    isPositive = expandDims(isPositive, i + 1);
  }
  isPositive = logicalAnd(isPositive, ones(gathered.shape, "bool"));
  const zeroSlice = zerosLike(gathered);
  return where(isPositive, gathered, zeroSlice);
}

// node_modules/@tensorflow/tfjs-core/dist/gradients/ZerosLike_grad.js
var zerosLikeGradConfig = {
  kernelName: ZerosLike,
  gradFunc: (dy) => {
    return { x: () => zerosLike(dy) };
  }
};

// node_modules/@tensorflow/tfjs-core/dist/register_all_gradients.js
var gradConfigs = [
  absGradConfig,
  acosGradConfig,
  acoshGradConfig,
  addGradConfig,
  addNGradConfig,
  argMaxGradConfig,
  argMinGradConfig,
  asinGradConfig,
  asinhGradConfig,
  atan2GradConfig,
  atanGradConfig,
  atanhGradConfig,
  avgPool3DGradConfig,
  avgPoolGradConfig,
  batchMatMulGradConfig,
  batchToSpaceNDGradConfig,
  broadcastToGradConfig,
  castGradConfig,
  ceilGradConfig,
  clipByValueGradConfig,
  complexAbsGradConfig,
  concatGradConfig,
  conv2DBackpropInputGradConfig,
  conv2DGradConfig,
  conv3DGradConfig,
  cosGradConfig,
  coshGradConfig,
  cumsumGradConfig,
  depthwiseConv2dNativeGradConfig,
  dilation2dGradConfig,
  divGradConfig,
  eluGradConfig,
  erfGradConfig,
  expGradConfig,
  expandDimsGradConfig,
  expm1GradConfig,
  floorDivGradConfig,
  floorGradConfig,
  fusedBatchNormGradConfig,
  gatherGradConfig,
  greaterEqualGradConfig,
  identityGradConfig,
  isFiniteGradConfig,
  isInfGradConfig,
  isNanGradConfig,
  leakyReluGradConfig,
  log1pGradConfig,
  logGradConfig,
  logSoftmaxGradConfig,
  lrnGradConfig,
  maxGradConfig,
  maxGradConfig,
  maximumGradConfig,
  maxPool3DGradConfig,
  maxPoolGradConfig,
  meanGradConfig,
  minGradConfig,
  minimumGradConfig,
  mirrorPadGradConfig,
  modGradConfig,
  multiplyGradConfig,
  negGradConfig,
  oneHotGradConfig,
  onesLikeGradConfig,
  packGradConfig,
  padV2GradConfig,
  padV2GradConfig,
  powGradConfig,
  preluGradConfig,
  prodGradConfig,
  reciprocalGradConfig,
  relu6GradConfig,
  reluGradConfig,
  reshapeGradConfig,
  resizeBilinearGradConfig,
  resizeNearestNeighborGradConfig,
  reverseGradConfig,
  roundGradConfig,
  rsqrtGradConfig,
  selectGradConfig,
  seluGradConfig,
  sigmoidGradConfig,
  signGradConfig,
  sinGradConfig,
  sinhGradConfig,
  sliceGradConfig,
  softmaxGradConfig,
  softplusGradConfig,
  spaceToBatchNDGradConfig,
  spaceToBatchNDGradConfig,
  splitVGradConfig,
  splitVGradConfig,
  sqrtGradConfig,
  squaredDifferenceGradConfig,
  squareGradConfig,
  stepGradConfig,
  subGradConfig,
  sumGradConfig,
  tanGradConfig,
  tanhGradConfig,
  tileGradConfig,
  transposeGradConfig,
  unpackGradConfig,
  unsortedSegmentSumGradConfig,
  zerosLikeGradConfig
];
for (const gradientConfig of gradConfigs) {
  registerGradient(gradientConfig);
}

// node_modules/@tensorflow/tfjs-layers/dist/exports_constraints.js
var exports_constraints_exports = {};
__export(exports_constraints_exports, {
  maxNorm: () => maxNorm,
  minMaxNorm: () => minMaxNorm,
  nonNeg: () => nonNeg,
  unitNorm: () => unitNorm
});

// node_modules/@tensorflow/tfjs-layers/dist/constraints.js
function calcL2Norms(w, axis) {
  return tidy(() => sqrt(sum(mul(w, w), axis, true)));
}
var Constraint = class extends serialization_exports.Serializable {
  getConfig() {
    return {};
  }
};
var MaxNorm = class extends Constraint {
  constructor(args) {
    super();
    this.defaultMaxValue = 2;
    this.defaultAxis = 0;
    this.maxValue = args.maxValue != null ? args.maxValue : this.defaultMaxValue;
    this.axis = args.axis != null ? args.axis : this.defaultAxis;
  }
  apply(w) {
    return tidy(() => {
      const norms = calcL2Norms(w, this.axis);
      const desired = clipByValue(norms, 0, this.maxValue);
      return mul(w, div(desired, add(epsilon(), norms)));
    });
  }
  getConfig() {
    return { maxValue: this.maxValue, axis: this.axis };
  }
};
MaxNorm.className = "MaxNorm";
serialization_exports.registerClass(MaxNorm);
var UnitNorm = class extends Constraint {
  constructor(args) {
    super();
    this.defaultAxis = 0;
    this.axis = args.axis != null ? args.axis : this.defaultAxis;
  }
  apply(w) {
    return tidy(() => div(w, add(epsilon(), calcL2Norms(w, this.axis))));
  }
  getConfig() {
    return { axis: this.axis };
  }
};
UnitNorm.className = "UnitNorm";
serialization_exports.registerClass(UnitNorm);
var NonNeg = class extends Constraint {
  apply(w) {
    return relu(w);
  }
};
NonNeg.className = "NonNeg";
serialization_exports.registerClass(NonNeg);
var MinMaxNorm = class extends Constraint {
  constructor(args) {
    super();
    this.defaultMinValue = 0;
    this.defaultMaxValue = 1;
    this.defaultRate = 1;
    this.defaultAxis = 0;
    this.minValue = args.minValue != null ? args.minValue : this.defaultMinValue;
    this.maxValue = args.maxValue != null ? args.maxValue : this.defaultMaxValue;
    this.rate = args.rate != null ? args.rate : this.defaultRate;
    this.axis = args.axis != null ? args.axis : this.defaultAxis;
  }
  apply(w) {
    return tidy(() => {
      const norms = calcL2Norms(w, this.axis);
      const desired = add(mul(this.rate, clipByValue(norms, this.minValue, this.maxValue)), mul(1 - this.rate, norms));
      return mul(w, div(desired, add(epsilon(), norms)));
    });
  }
  getConfig() {
    return {
      minValue: this.minValue,
      maxValue: this.maxValue,
      rate: this.rate,
      axis: this.axis
    };
  }
};
MinMaxNorm.className = "MinMaxNorm";
serialization_exports.registerClass(MinMaxNorm);
var CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP = {
  "maxNorm": "MaxNorm",
  "minMaxNorm": "MinMaxNorm",
  "nonNeg": "NonNeg",
  "unitNorm": "UnitNorm"
};
function serializeConstraint(constraint) {
  return serializeKerasObject(constraint);
}
function deserializeConstraint(config, customObjects = {}) {
  return deserializeKerasObject(config, serialization_exports.SerializationMap.getMap().classNameMap, customObjects, "constraint");
}
function getConstraint(identifier) {
  if (identifier == null) {
    return null;
  }
  if (typeof identifier === "string") {
    const className = identifier in CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP ? CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP[identifier] : identifier;
    const config = { className, config: {} };
    return deserializeConstraint(config);
  } else if (identifier instanceof Constraint) {
    return identifier;
  } else {
    return deserializeConstraint(identifier);
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/exports_constraints.js
function maxNorm(args) {
  return new MaxNorm(args);
}
function unitNorm(args) {
  return new UnitNorm(args);
}
function nonNeg() {
  return new NonNeg();
}
function minMaxNorm(config) {
  return new MinMaxNorm(config);
}

// node_modules/@tensorflow/tfjs-layers/dist/exports_initializers.js
var exports_initializers_exports = {};
__export(exports_initializers_exports, {
  constant: () => constant,
  glorotNormal: () => glorotNormal,
  glorotUniform: () => glorotUniform,
  heNormal: () => heNormal,
  heUniform: () => heUniform,
  identity: () => identity,
  leCunNormal: () => leCunNormal,
  leCunUniform: () => leCunUniform,
  ones: () => ones2,
  orthogonal: () => orthogonal,
  randomNormal: () => randomNormal3,
  randomUniform: () => randomUniform2,
  truncatedNormal: () => truncatedNormal2,
  varianceScaling: () => varianceScaling,
  zeros: () => zeros2
});
function zeros2() {
  return new Zeros();
}
function ones2() {
  return new Ones();
}
function constant(args) {
  return new Constant(args);
}
function randomUniform2(args) {
  return new RandomUniform(args);
}
function randomNormal3(args) {
  return new RandomNormal(args);
}
function truncatedNormal2(args) {
  return new TruncatedNormal(args);
}
function identity(args) {
  return new Identity2(args);
}
function varianceScaling(config) {
  return new VarianceScaling(config);
}
function glorotUniform(args) {
  return new GlorotUniform(args);
}
function glorotNormal(args) {
  return new GlorotNormal(args);
}
function heNormal(args) {
  return new HeNormal(args);
}
function heUniform(args) {
  return new HeUniform(args);
}
function leCunNormal(args) {
  return new LeCunNormal(args);
}
function leCunUniform(args) {
  return new LeCunUniform(args);
}
function orthogonal(args) {
  return new Orthogonal(args);
}

// node_modules/@tensorflow/tfjs-layers/dist/exports_layers.js
var exports_layers_exports = {};
__export(exports_layers_exports, {
  Layer: () => Layer,
  RNN: () => RNN,
  RNNCell: () => RNNCell,
  activation: () => activation,
  add: () => add2,
  alphaDropout: () => alphaDropout,
  average: () => average,
  averagePooling1d: () => averagePooling1d,
  averagePooling2d: () => averagePooling2d,
  averagePooling3d: () => averagePooling3d,
  avgPool1d: () => avgPool1d,
  avgPool2d: () => avgPool2d,
  avgPool3d: () => avgPool3d2,
  avgPooling1d: () => avgPooling1d,
  avgPooling2d: () => avgPooling2d,
  avgPooling3d: () => avgPooling3d,
  batchNormalization: () => batchNormalization2,
  bidirectional: () => bidirectional,
  concatenate: () => concatenate2,
  conv1d: () => conv1d2,
  conv2d: () => conv2d2,
  conv2dTranspose: () => conv2dTranspose2,
  conv3d: () => conv3d2,
  conv3dTranspose: () => conv3dTranspose2,
  convLstm2d: () => convLstm2d,
  convLstm2dCell: () => convLstm2dCell,
  cropping2D: () => cropping2D,
  dense: () => dense,
  depthwiseConv2d: () => depthwiseConv2d3,
  dot: () => dot2,
  dropout: () => dropout3,
  elu: () => elu3,
  embedding: () => embedding,
  flatten: () => flatten2,
  gaussianDropout: () => gaussianDropout,
  gaussianNoise: () => gaussianNoise,
  globalAveragePooling1d: () => globalAveragePooling1d,
  globalAveragePooling2d: () => globalAveragePooling2d,
  globalMaxPool1d: () => globalMaxPool1d,
  globalMaxPool2d: () => globalMaxPool2d,
  globalMaxPooling1d: () => globalMaxPooling1d,
  globalMaxPooling2d: () => globalMaxPooling2d,
  gru: () => gru,
  gruCell: () => gruCell,
  input: () => input,
  inputLayer: () => inputLayer,
  layerNormalization: () => layerNormalization,
  leakyReLU: () => leakyReLU,
  lstm: () => lstm,
  lstmCell: () => lstmCell,
  masking: () => masking,
  maxPool1d: () => maxPool1d,
  maxPool2d: () => maxPool2d,
  maxPooling1d: () => maxPooling1d,
  maxPooling2d: () => maxPooling2d,
  maxPooling3d: () => maxPooling3d,
  maximum: () => maximum2,
  minimum: () => minimum2,
  multiply: () => multiply,
  permute: () => permute,
  prelu: () => prelu2,
  reLU: () => reLU,
  repeatVector: () => repeatVector,
  rescaling: () => rescaling,
  reshape: () => reshape2,
  rnn: () => rnn2,
  separableConv2d: () => separableConv2d2,
  simpleRNN: () => simpleRNN,
  simpleRNNCell: () => simpleRNNCell,
  softmax: () => softmax2,
  spatialDropout1d: () => spatialDropout1d,
  stackedRNNCells: () => stackedRNNCells,
  thresholdedReLU: () => thresholdedReLU,
  timeDistributed: () => timeDistributed,
  upSampling2d: () => upSampling2d,
  zeroPadding2d: () => zeroPadding2d
});

// node_modules/@tensorflow/tfjs-layers/dist/logs.js
async function resolveScalarsInLogs(logs) {
  if (logs == null) {
    return;
  }
  const promises = [];
  const keys = [];
  const scalarsToDispose = [];
  for (const key in logs) {
    const value = logs[key];
    if (typeof value !== "number") {
      const valueScalar = value;
      promises.push(valueScalar.data());
      keys.push(key);
      scalarsToDispose.push(valueScalar);
    }
  }
  if (promises.length > 0) {
    const values = await Promise.all(promises);
    for (let i = 0; i < values.length; ++i) {
      logs[keys[i]] = values[i][0];
    }
    dispose(scalarsToDispose);
  }
}
function disposeTensorsInLogs(logs) {
  if (logs == null) {
    return;
  }
  for (const key in logs) {
    const value = logs[key];
    if (typeof value !== "number") {
      value.dispose();
    }
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/base_callbacks.js
var ModelLoggingVerbosity;
(function(ModelLoggingVerbosity2) {
  ModelLoggingVerbosity2[ModelLoggingVerbosity2["SILENT"] = 0] = "SILENT";
  ModelLoggingVerbosity2[ModelLoggingVerbosity2["VERBOSE"] = 1] = "VERBOSE";
})(ModelLoggingVerbosity || (ModelLoggingVerbosity = {}));
var DEFAULT_YIELD_EVERY_MS = 125;
var BaseCallback = class {
  constructor() {
    this.validationData = null;
  }
  setParams(params) {
    this.params = params;
  }
  async onEpochBegin(epoch, logs) {
  }
  async onEpochEnd(epoch, logs) {
  }
  async onBatchBegin(batch, logs) {
  }
  async onBatchEnd(batch, logs) {
  }
  async onTrainBegin(logs) {
  }
  async onTrainEnd(logs) {
  }
  // LayersModel needs to call Callback.setModel(), but cannot actually depend
  // on Callback because that creates a cyclic dependency.  Providing this no-op
  // method on BaseCallback breaks the cycle: this way LayersModel can depend on
  // BaseCallback but not on Callback.  The argument is typed as `Container`
  // (the superclass of LayersModel) to avoid recapitulating the cycle. Callback
  // overrides this method and enforces that the argument is really a
  // LayersModel.
  setModel(model2) {
  }
};
var CallbackList = class {
  // TODO(cais): When the need arises, uncomment the following lines and
  // implement the queue for time values.
  // private deltaTBatch: number;
  // private deltaTsBatchBegin: Array<number>;
  // private deltaTsBatchEnd: Array<number>;
  /**
   * Constructor of CallbackList.
   * @param callbacks Array of `Callback` instances.
   * @param queueLength Queue length for keeping running statistics over
   *   callback execution time.
   */
  constructor(callbacks2, queueLength = 10) {
    if (callbacks2 == null) {
      callbacks2 = [];
    }
    this.callbacks = callbacks2;
    this.queueLength = queueLength;
  }
  append(callback) {
    this.callbacks.push(callback);
  }
  setParams(params) {
    for (const callback of this.callbacks) {
      callback.setParams(params);
    }
  }
  setModel(model2) {
    for (const callback of this.callbacks) {
      callback.setModel(model2);
    }
  }
  /**
   * Called at the start of an epoch.
   * @param epoch Index of epoch.
   * @param logs Dictionary of logs.
   */
  async onEpochBegin(epoch, logs) {
    if (logs == null) {
      logs = {};
    }
    for (const callback of this.callbacks) {
      await callback.onEpochBegin(epoch, logs);
    }
  }
  /**
   * Called at the end of an epoch.
   * @param epoch Index of epoch.
   * @param logs Dictionary of logs.
   */
  async onEpochEnd(epoch, logs) {
    if (logs == null) {
      logs = {};
    }
    for (const callback of this.callbacks) {
      await callback.onEpochEnd(epoch, logs);
    }
  }
  /**
   * Called  right before processing a batch.
   * @param batch Index of batch within the current epoch.
   * @param logs Dictionary of logs.
   */
  async onBatchBegin(batch, logs) {
    if (logs == null) {
      logs = {};
    }
    for (const callback of this.callbacks) {
      await callback.onBatchBegin(batch, logs);
    }
  }
  /**
   * Called at the end of a batch.
   * @param batch Index of batch within the current epoch.
   * @param logs Dictionary of logs.
   */
  async onBatchEnd(batch, logs) {
    if (logs == null) {
      logs = {};
    }
    for (const callback of this.callbacks) {
      await callback.onBatchEnd(batch, logs);
    }
  }
  /**
   * Called at the beginning of training.
   * @param logs Dictionary of logs.
   */
  async onTrainBegin(logs) {
    if (logs == null) {
      logs = {};
    }
    for (const callback of this.callbacks) {
      await callback.onTrainBegin(logs);
    }
  }
  /**
   * Called at the end of training.
   * @param logs Dictionary of logs.
   */
  async onTrainEnd(logs) {
    if (logs == null) {
      logs = {};
    }
    for (const callback of this.callbacks) {
      await callback.onTrainEnd(logs);
    }
  }
};
var BaseLogger = class extends BaseCallback {
  constructor() {
    super();
  }
  async onEpochBegin(epoch) {
    this.seen = 0;
    this.totals = {};
  }
  async onBatchEnd(batch, logs) {
    if (logs == null) {
      logs = {};
    }
    const batchSize = logs["size"] == null ? 0 : logs["size"];
    this.seen += batchSize;
    for (const key in logs) {
      const value = logs[key];
      if (typeof value === "number") {
        if (!this.totals.hasOwnProperty(key)) {
          this.totals[key] = 0;
        }
        this.totals[key] = this.totals[key] + value * batchSize;
      } else {
        let oldTotalsToDispose;
        if (key in this.totals) {
          oldTotalsToDispose = this.totals[key];
        } else {
          this.totals[key] = 0;
        }
        const total = tidy(() => add(this.totals[key], mul(value, batchSize)));
        this.totals[key] = total;
        if (oldTotalsToDispose != null) {
          oldTotalsToDispose.dispose();
        }
      }
    }
  }
  async onEpochEnd(epoch, logs) {
    if (logs != null) {
      for (const key of this.params["metrics"]) {
        if (this.totals[key] == null) {
          continue;
        }
        if (typeof this.totals[key] === "number") {
          logs[key] = this.totals[key] / this.seen;
        } else {
          tidy(() => {
            const log2 = mul(div(1, this.seen), this.totals[key]);
            logs[key] = log2;
            this.totals[key].dispose();
            keep(logs[key]);
          });
        }
      }
    }
  }
};
var History = class extends BaseCallback {
  async onTrainBegin(logs) {
    this.epoch = [];
    this.history = {};
  }
  async onEpochEnd(epoch, logs) {
    if (logs == null) {
      logs = {};
    }
    this.epoch.push(epoch);
    for (const key in logs) {
      if (this.history[key] == null) {
        this.history[key] = [];
      }
      this.history[key].push(logs[key]);
    }
  }
  /**
   * Await the values of all losses and metrics.
   */
  async syncData() {
    const promises = [];
    const keys = [];
    const indices = [];
    for (const key in this.history) {
      const valueArray = this.history[key];
      for (let i = 0; i < valueArray.length; ++i) {
        if (typeof valueArray[i] !== "number") {
          const valueScalar = valueArray[i];
          promises.push(valueScalar.data());
          keys.push(key);
          indices.push(i);
        }
      }
    }
    const values = await Promise.all(promises);
    for (let n = 0; n < values.length; ++n) {
      const tensorToDispose = this.history[keys[n]][indices[n]];
      tensorToDispose.dispose();
      this.history[keys[n]][indices[n]] = values[n][0];
    }
  }
};
var CustomCallback = class extends BaseCallback {
  constructor(args, yieldEvery) {
    super();
    this.currentEpoch = 0;
    this.nowFunc = args.nowFunc;
    this.nextFrameFunc = args.nextFrameFunc || nextFrame;
    this.yieldEvery = yieldEvery || "auto";
    if (this.yieldEvery === "auto") {
      this.yieldEvery = DEFAULT_YIELD_EVERY_MS;
    }
    if (this.yieldEvery === "never" && args.onYield != null) {
      throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");
    }
    if (util_exports.isNumber(this.yieldEvery)) {
      this.maybeWait = debounce(this.maybeWait.bind(this), this.yieldEvery, this.nowFunc);
    }
    this.trainBegin = args.onTrainBegin;
    this.trainEnd = args.onTrainEnd;
    this.epochBegin = args.onEpochBegin;
    this.epochEnd = args.onEpochEnd;
    this.batchBegin = args.onBatchBegin;
    this.batchEnd = args.onBatchEnd;
    this.yield = args.onYield;
  }
  async maybeWait(epoch, batch, logs) {
    const ps = [];
    if (this.yield != null) {
      await resolveScalarsInLogs(logs);
      ps.push(this.yield(epoch, batch, logs));
    }
    ps.push(this.nextFrameFunc());
    await Promise.all(ps);
  }
  async onEpochBegin(epoch, logs) {
    this.currentEpoch = epoch;
    if (this.epochBegin != null) {
      await resolveScalarsInLogs(logs);
      await this.epochBegin(epoch, logs);
    }
  }
  async onEpochEnd(epoch, logs) {
    const ps = [];
    if (this.epochEnd != null) {
      await resolveScalarsInLogs(logs);
      ps.push(this.epochEnd(epoch, logs));
    }
    if (this.yieldEvery === "epoch") {
      ps.push(this.nextFrameFunc());
    }
    await Promise.all(ps);
  }
  async onBatchBegin(batch, logs) {
    if (this.batchBegin != null) {
      await resolveScalarsInLogs(logs);
      await this.batchBegin(batch, logs);
    }
  }
  async onBatchEnd(batch, logs) {
    const ps = [];
    if (this.batchEnd != null) {
      await resolveScalarsInLogs(logs);
      ps.push(this.batchEnd(batch, logs));
    }
    if (this.yieldEvery === "batch") {
      ps.push(this.nextFrameFunc());
    } else if (util_exports.isNumber(this.yieldEvery)) {
      ps.push(this.maybeWait(this.currentEpoch, batch, logs));
    }
    await Promise.all(ps);
  }
  async onTrainBegin(logs) {
    if (this.trainBegin != null) {
      await resolveScalarsInLogs(logs);
      await this.trainBegin(logs);
    }
  }
  async onTrainEnd(logs) {
    if (this.trainEnd != null) {
      await resolveScalarsInLogs(logs);
      await this.trainEnd(logs);
    }
  }
};
function standardizeCallbacks(callbacks2, yieldEvery) {
  if (callbacks2 == null) {
    callbacks2 = {};
  }
  if (callbacks2 instanceof BaseCallback) {
    return [callbacks2];
  }
  if (Array.isArray(callbacks2) && callbacks2[0] instanceof BaseCallback) {
    return callbacks2;
  }
  const callbackConfigs = toList(callbacks2);
  return callbackConfigs.map((callbackConfig) => new CustomCallback(callbackConfig, yieldEvery));
}
var CallbackConstructorRegistry = class _CallbackConstructorRegistry {
  /**
   * Blocks public access to constructor.
   */
  constructor() {
  }
  /**
   * Register a tf.LayersModel.fit() callback constructor.
   *
   * The registered callback constructor will be used to instantiate
   * callbacks for every tf.LayersModel.fit() call afterwards.
   *
   * @param verbosityLevel Level of verbosity at which the `callbackConstructor`
   *   is to be reigstered.
   * @param callbackConstructor A no-arg constructor for `tf.Callback`.
   * @throws Error, if the same callbackConstructor has been registered before,
   *   either at the same or a different `verbosityLevel`.
   */
  static registerCallbackConstructor(verbosityLevel, callbackConstructor) {
    util_exports.assert(verbosityLevel >= 0 && Number.isInteger(verbosityLevel), () => `Verbosity level is expected to be an integer >= 0, but got ${verbosityLevel}`);
    _CallbackConstructorRegistry.checkForDuplicate(callbackConstructor);
    if (_CallbackConstructorRegistry.constructors[verbosityLevel] == null) {
      _CallbackConstructorRegistry.constructors[verbosityLevel] = [];
    }
    _CallbackConstructorRegistry.constructors[verbosityLevel].push(callbackConstructor);
  }
  static checkForDuplicate(callbackConstructor) {
    for (const levelName in _CallbackConstructorRegistry.constructors) {
      const constructors = _CallbackConstructorRegistry.constructors[+levelName];
      constructors.forEach((ctor) => {
        if (ctor === callbackConstructor) {
          throw new ValueError("Duplicate callback constructor.");
        }
      });
    }
  }
  /**
   * Clear all registered callback constructors.
   */
  static clear() {
    _CallbackConstructorRegistry.constructors = {};
  }
  /**
   * Create callbacks using the registered callback constructors.
   *
   * Given `verbosityLevel`, all constructors registered at that level or above
   * will be called and the instantiated callbacks will be used.
   *
   * @param verbosityLevel: Level of verbosity.
   */
  static createCallbacks(verbosityLevel) {
    const constructors = [];
    for (const levelName in _CallbackConstructorRegistry.constructors) {
      const level = +levelName;
      if (verbosityLevel >= level) {
        constructors.push(..._CallbackConstructorRegistry.constructors[level]);
      }
    }
    return constructors.map((ctor) => new ctor());
  }
};
CallbackConstructorRegistry.constructors = {};
function configureCallbacks(callbacks2, verbose, epochs, initialEpoch, numTrainSamples, stepsPerEpoch, batchSize, doValidation, callbackMetrics) {
  const history = new History();
  const actualCallbacks = [
    new BaseLogger(),
    ...CallbackConstructorRegistry.createCallbacks(verbose)
  ];
  if (callbacks2 != null) {
    actualCallbacks.push(...callbacks2);
  }
  actualCallbacks.push(history);
  const callbackList = new CallbackList(actualCallbacks);
  callbackList.setParams({
    epochs,
    initialEpoch,
    samples: numTrainSamples,
    steps: stepsPerEpoch,
    batchSize,
    verbose,
    doValidation,
    metrics: callbackMetrics
  });
  return { callbackList, history };
}

// node_modules/@tensorflow/tfjs-layers/dist/layers/serialization.js
function deserialize(config, customObjects = {}, fastWeightInit = false) {
  return deserializeKerasObject(config, serialization_exports.SerializationMap.getMap().classNameMap, customObjects, "layer", fastWeightInit);
}

// node_modules/@tensorflow/tfjs-layers/dist/losses.js
function l2Normalize(x, axis) {
  return tidy(() => {
    if (x.dtype !== "float32") {
      x = cast(x, "float32");
    }
    const squareSum = sum(square2(x), axis, true);
    const epsilonTensor = fill(squareSum.shape, epsilon());
    const norm = sqrt(maximum(squareSum, epsilonTensor));
    return div(x, norm);
  });
}
function meanSquaredError(yTrue, yPred) {
  return tidy(() => mean(square2(sub(yPred, yTrue)), -1));
}
function meanAbsoluteError(yTrue, yPred) {
  return tidy(() => mean(abs(sub(yPred, yTrue)), -1));
}
function meanAbsolutePercentageError(yTrue, yPred) {
  return tidy(() => {
    const diff = sub(yTrue, yPred);
    const clippedTrue = clipByValue(abs(yTrue), epsilon(), Number.MAX_VALUE);
    const absResult = abs(div(diff, clippedTrue));
    return mul(100, mean(absResult, -1));
  });
}
function meanSquaredLogarithmicError(yTrue, yPred) {
  return tidy(() => {
    const clippedPred = clipByValue(yPred, epsilon(), Number.MAX_VALUE);
    const firstLog = log(add(1, clippedPred));
    const clippedTrue = clipByValue(yTrue, epsilon(), Number.MAX_VALUE);
    const secondLog = log(add(1, clippedTrue));
    return mean(square2(sub(firstLog, secondLog)), -1);
  });
}
function squaredHinge(yTrue, yPred) {
  return tidy(() => {
    const maxResult = maximum(0, sub(1, mul(yTrue, yPred)));
    return mean(square2(maxResult), -1);
  });
}
function hinge(yTrue, yPred) {
  return tidy(() => {
    const maxResult = maximum(0, sub(1, mul(yTrue, yPred)));
    return mean(maxResult, -1);
  });
}
function categoricalHinge(yTrue, yPred) {
  return tidy(() => {
    const pos = sum(mul(yTrue, yPred), -1);
    const neg2 = max(mul(sub(1, yTrue), yPred), -1);
    return maximum(0, add(1, sub(neg2, pos)));
  });
}
function logcosh(yTrue, yPred) {
  return tidy(() => {
    const log2 = Math.log(2);
    const predictionDiff = sub(yPred, yTrue);
    const logcoshResult = sub(add(predictionDiff, softplus(mul(-2, predictionDiff))), log2);
    return mean(logcoshResult, -1);
  });
}
function categoricalCrossentropy(target, output, fromLogits = false) {
  return tidy(() => {
    if (fromLogits) {
      output = softmax(output);
    } else {
      const outputSum = sum(output, output.shape.length - 1, true);
      output = div(output, outputSum);
    }
    output = clipByValue(output, epsilon(), 1 - epsilon());
    return neg(sum(mul(cast(target, "float32"), log(output)), output.shape.length - 1));
  });
}
function sparseCategoricalCrossentropy(target, output, fromLogits = false) {
  return tidy(() => {
    const flatTarget = cast(floor(flatten(target)), "int32");
    output = clipByValue(output, epsilon(), 1 - epsilon());
    const outputShape = output.shape;
    const oneHotTarget = reshape(oneHot(flatTarget, outputShape[outputShape.length - 1]), outputShape);
    return categoricalCrossentropy(oneHotTarget, output, fromLogits);
  });
}
function sigmoidCrossEntropyWithLogits(labels, logits) {
  if (!util_exports.arraysEqual(labels.shape, logits.shape)) {
    throw new ValueError(`logits and labels must have the same shape, but got shapes ${JSON.stringify(labels.shape)} and ${JSON.stringify(logits.shape)}`);
  }
  return tidy(() => {
    const reluLogits = relu(logits);
    const negAbsLogits = neg(abs(logits));
    return add(sub(reluLogits, mul(logits, labels)), log1p(exp(negAbsLogits)));
  });
}
function binaryCrossentropy(yTrue, yPred) {
  return tidy(() => {
    let y;
    y = clipByValue(yPred, epsilon(), 1 - epsilon());
    y = log(div(y, sub(1, y)));
    return mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);
  });
}
function kullbackLeiblerDivergence(yTrue, yPred) {
  return tidy(() => {
    const clippedTrue = clipByValue(yTrue, epsilon(), 1);
    const clippedPred = clipByValue(yPred, epsilon(), 1);
    return sum(mul(yTrue, log(div(clippedTrue, clippedPred))), -1);
  });
}
function poisson(yTrue, yPred) {
  return tidy(() => {
    const logPred = log(add(epsilon(), yPred));
    return mean(sub(yPred, mul(yTrue, logPred)), -1);
  });
}
function cosineProximity(yTrue, yPred) {
  return tidy(() => {
    const trueNormalized = l2Normalize(yTrue, -1);
    const predNormalized = l2Normalize(yPred, -1);
    const trueXPred = mul(trueNormalized, predNormalized);
    return neg(sum(trueXPred, -1));
  });
}
var lossesMap = {
  meanSquaredError,
  meanAbsoluteError,
  meanAbsolutePercentageError,
  meanSquaredLogarithmicError,
  squaredHinge,
  hinge,
  categoricalHinge,
  logcosh,
  categoricalCrossentropy,
  sparseCategoricalCrossentropy,
  binaryCrossentropy,
  kullbackLeiblerDivergence,
  poisson,
  cosineProximity
};
function get(identifierOrFn) {
  if (typeof identifierOrFn === "string") {
    if (identifierOrFn in lossesMap) {
      return lossesMap[identifierOrFn];
    }
    let errMsg = `Unknown loss ${identifierOrFn}`;
    if (identifierOrFn.toLowerCase().includes("softmaxcrossentropy")) {
      errMsg = `Unknown loss ${identifierOrFn}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`;
    }
    throw new ValueError(errMsg);
  } else {
    return identifierOrFn;
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/metrics.js
function binaryAccuracy(yTrue, yPred) {
  return tidy(() => {
    const threshold = mul(0.5, onesLike(yPred));
    const yPredThresholded = cast2(greater(yPred, threshold), yTrue.dtype);
    return mean(equal(yTrue, yPredThresholded), -1);
  });
}
function categoricalAccuracy(yTrue, yPred) {
  return tidy(() => cast2(equal(argMax(yTrue, -1), argMax(yPred, -1)), "float32"));
}
function truePositives(yTrue, yPred) {
  return tidy(() => {
    return cast(sum(logicalAnd(equal(yTrue, 1), equal(yPred, 1))), "float32");
  });
}
function falseNegatives(yTrue, yPred) {
  return tidy(() => {
    return cast(sum(logicalAnd(equal(yTrue, 1), equal(yPred, 0))), "float32");
  });
}
function falsePositives(yTrue, yPred) {
  return tidy(() => {
    return cast(sum(logicalAnd(equal(yTrue, 0), equal(yPred, 1))), "float32");
  });
}
function precision(yTrue, yPred) {
  return tidy(() => {
    const tp = truePositives(yTrue, yPred);
    const fp = falsePositives(yTrue, yPred);
    const denominator = add(tp, fp);
    return cast(where(greater(denominator, 0), div(tp, denominator), 0), "float32");
  });
}
function recall(yTrue, yPred) {
  return tidy(() => {
    const tp = truePositives(yTrue, yPred);
    const fn = falseNegatives(yTrue, yPred);
    const denominator = add(tp, fn);
    return cast(where(greater(denominator, 0), div(tp, denominator), 0), "float32");
  });
}
function binaryCrossentropy2(yTrue, yPred) {
  return binaryCrossentropy(yTrue, yPred);
}
function sparseCategoricalAccuracy(yTrue, yPred) {
  if (yTrue.rank === yPred.rank) {
    yTrue = squeeze(yTrue, [yTrue.rank - 1]);
  }
  yPred = argMax(yPred, -1);
  if (yPred.dtype !== yTrue.dtype) {
    yPred = cast(yPred, yTrue.dtype);
  }
  return cast(equal(yTrue, yPred), "float32");
}
var mse = meanSquaredError;
var MSE = meanSquaredError;
var mae = meanAbsoluteError;
var MAE = meanAbsoluteError;
var mape = meanAbsolutePercentageError;
var MAPE = meanAbsolutePercentageError;
var categoricalCrossentropy2 = categoricalCrossentropy;
var cosine = cosineProximity;
var sparseCategoricalCrossentropy2 = sparseCategoricalCrossentropy;
var metricsMap = {
  binaryAccuracy,
  categoricalAccuracy,
  precision,
  categoricalCrossentropy: categoricalCrossentropy2,
  sparseCategoricalCrossentropy: sparseCategoricalCrossentropy2,
  mse,
  MSE,
  mae,
  MAE,
  mape,
  MAPE,
  cosine
};
function get2(identifier) {
  if (typeof identifier === "string" && identifier in metricsMap) {
    return metricsMap[identifier];
  } else if (typeof identifier !== "string" && identifier != null) {
    return identifier;
  } else {
    throw new ValueError(`Unknown metric ${identifier}`);
  }
}
function getLossOrMetricName(fn) {
  assert2(fn !== null, `Unknown LossOrMetricFn ${fn}`);
  if (typeof fn === "string") {
    return fn;
  } else {
    let fnName;
    for (const key of Object.keys(lossesMap)) {
      if (lossesMap[key] === fn) {
        fnName = key;
        break;
      }
    }
    if (fnName !== void 0) {
      return fnName;
    }
    for (const key of Object.keys(metricsMap)) {
      if (metricsMap[key] === fn) {
        fnName = key;
        break;
      }
    }
    if (fnName !== void 0) {
      return fnName;
    }
    return fn.name;
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/optimizers.js
function getOptimizer(identifier) {
  const optimizerMap = {
    "Adagrad": () => train.adagrad(0.01),
    "Adadelta": () => train.adadelta(1, 0.95, epsilon()),
    "Adam": () => train.adam(1e-3, 0.9, 0.999, epsilon()),
    "Adamax": () => train.adamax(2e-3, 0.9, 0.999, epsilon(), 0),
    "RMSProp": () => train.rmsprop(1e-3, 0.9, 0, epsilon()),
    "SGD": () => train.sgd(0.01)
  };
  optimizerMap["adagrad"] = optimizerMap["Adagrad"];
  optimizerMap["adadelta"] = optimizerMap["Adadelta"];
  optimizerMap["adam"] = optimizerMap["Adam"];
  optimizerMap["adamax"] = optimizerMap["Adamax"];
  optimizerMap["rmsprop"] = optimizerMap["RMSProp"];
  optimizerMap["sgd"] = optimizerMap["SGD"];
  if (identifier in optimizerMap) {
    return optimizerMap[identifier]();
  }
  throw new ValueError(`Unknown Optimizer ${identifier}`);
}

// node_modules/@tensorflow/tfjs-layers/dist/user_defined_metadata.js
var MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH = 1 * 1024 * 1024;
function checkUserDefinedMetadata(userDefinedMetadata, modelName, checkSize = false) {
  if (userDefinedMetadata == null || typeof userDefinedMetadata !== "object" || Object.getPrototypeOf(userDefinedMetadata) !== Object.prototype || !plainObjectCheck(userDefinedMetadata)) {
    throw new Error("User-defined metadata is expected to be a JSON object, but is not.");
  }
  if (checkSize) {
    const out = JSON.stringify(userDefinedMetadata);
    if (out.length > MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH) {
      console.warn(`User-defined metadata of model "${modelName}" is too large in size (length=${out.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= ${MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH}.`);
    }
  }
}
function plainObjectCheck(x) {
  if (x === null) {
    return true;
  } else if (typeof x === "object") {
    if (Object.getPrototypeOf(x) === Object.prototype) {
      const keys = Object.keys(x);
      for (const key of keys) {
        if (typeof key !== "string") {
          return false;
        }
        if (!plainObjectCheck(x[key])) {
          return false;
        }
      }
      return true;
    } else {
      if (Array.isArray(x)) {
        for (const item of x) {
          if (!plainObjectCheck(item)) {
            return false;
          }
        }
        return true;
      } else {
        return false;
      }
    }
  } else {
    const xType = typeof x;
    return xType === "string" || xType === "number" || xType === "boolean";
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/utils/layer_utils.js
function printSummary(model2, lineLength, positions, printFn = console.log) {
  const sequentialLike = isModelSequentialLike(model2);
  const toDisplay = ["Layer (type)", "Input Shape", "Output shape", "Param #"];
  if (sequentialLike) {
    lineLength = lineLength || 90;
    positions = positions || [0.32, 0.61, 0.89, 1];
  } else {
    lineLength = lineLength || 115;
    positions = positions || [0.24, 0.48, 0.7, 0.8, 1];
  }
  if (positions[positions.length - 1] <= 1) {
    positions = positions.map((p) => Math.floor(lineLength * p));
  }
  let relevantNodes;
  if (!sequentialLike) {
    toDisplay.push("Receives inputs");
    relevantNodes = [];
    for (const depth in model2.nodesByDepth) {
      relevantNodes.push(...model2.nodesByDepth[depth]);
    }
  }
  printFn("_".repeat(lineLength));
  printRow(toDisplay, positions, printFn);
  printFn("=".repeat(lineLength));
  const layers = model2.layers;
  for (let i = 0; i < layers.length; ++i) {
    if (sequentialLike) {
      printLayerSummary(layers[i], positions, printFn);
    } else {
      printLayerSummaryWithConnections(layers[i], positions, relevantNodes, printFn);
    }
    printFn((i === layers.length - 1 ? "=" : "_").repeat(lineLength));
  }
  model2.checkTrainableWeightsConsistency();
  const trainableCount = countTrainableParams(model2);
  const nonTrainableCount = countParamsInWeights(model2.nonTrainableWeights);
  printFn(`Total params: ${trainableCount + nonTrainableCount}`);
  printFn(`Trainable params: ${trainableCount}`);
  printFn(`Non-trainable params: ${nonTrainableCount}`);
  printFn("_".repeat(lineLength));
}
function countTrainableParams(model2) {
  let trainableCount;
  if (model2.collectedTrainableWeights != null) {
    trainableCount = countParamsInWeights(model2.collectedTrainableWeights);
  } else {
    trainableCount = countParamsInWeights(model2.trainableWeights);
  }
  return trainableCount;
}
function isModelSequentialLike(model2) {
  let sequentialLike = true;
  const nodesByDepth = [];
  const nodes = [];
  for (const depth in model2.nodesByDepth) {
    nodesByDepth.push(model2.nodesByDepth[depth]);
  }
  for (const depthNodes of nodesByDepth) {
    if (depthNodes.length > 1 || depthNodes.length === 1 && depthNodes[0].inboundLayers.length > 1) {
      sequentialLike = false;
      break;
    }
    nodes.push(...depthNodes);
  }
  if (sequentialLike) {
    for (const layer of model2.layers) {
      let flag = false;
      for (const node of layer.inboundNodes) {
        if (nodes.indexOf(node) !== -1) {
          if (flag) {
            sequentialLike = false;
            break;
          } else {
            flag = true;
          }
        }
      }
      if (!sequentialLike) {
        break;
      }
    }
  }
  return sequentialLike;
}
function printRow(fields, positions, printFn = console.log) {
  let line = "";
  for (let i = 0; i < fields.length; ++i) {
    if (i > 0) {
      line = line.slice(0, line.length - 1) + " ";
    }
    line += fields[i];
    line = line.slice(0, positions[i]);
    line += " ".repeat(positions[i] - line.length);
  }
  printFn(line);
}
function printLayerSummary(layer, positions, printFn) {
  let outputShape;
  let inputShape;
  try {
    inputShape = layer.inboundNodes.map((x) => JSON.stringify(x.inputShapes)).join(",");
  } catch (err) {
    inputShape = "multiple";
  }
  try {
    outputShape = JSON.stringify(layer.outputShape);
  } catch (err) {
    outputShape = "multiple";
  }
  const name = layer.name;
  const className = layer.getClassName();
  const fields = [
    `${name} (${className})`,
    inputShape,
    outputShape,
    layer.countParams().toString()
  ];
  printRow(fields, positions, printFn);
}
function printLayerSummaryWithConnections(layer, positions, relevantNodes, printFn) {
  let outputShape;
  let inputShape;
  try {
    inputShape = layer.inboundNodes.map((x) => JSON.stringify(x.inputShapes)).join(",");
  } catch (err) {
    inputShape = "multiple";
  }
  try {
    outputShape = JSON.stringify(layer.outputShape);
  } catch (err) {
    outputShape = "multiple";
  }
  const connections = [];
  for (const node of layer.inboundNodes) {
    if (relevantNodes != null && relevantNodes.length > 0 && relevantNodes.indexOf(node) === -1) {
      continue;
    }
    for (let i = 0; i < node.inboundLayers.length; ++i) {
      const inboundLayer = node.inboundLayers[i].name;
      const inboundLayerIndex = node.nodeIndices[i];
      const inboundTensorIndex = node.tensorIndices[i];
      connections.push(`${inboundLayer}[${inboundLayerIndex}][${inboundTensorIndex}]`);
    }
  }
  const name = layer.name;
  const className = layer.getClassName();
  const firstConnection = connections.length === 0 ? "" : connections[0];
  const fields = [
    `${name} (${className})`,
    inputShape,
    outputShape,
    layer.countParams().toString(),
    firstConnection
  ];
  printRow(fields, positions, printFn);
  for (let i = 1; i < connections.length; ++i) {
    printRow(["", "", "", "", connections[i]], positions, printFn);
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/utils/serialization_utils.js
function isArrayItemInputOrOutputName(key, index, value) {
  return (key === "inboundNodes" || key === "outputLayers" || key === "inputLayers") && index === 0 && typeof value === "string";
}
function convertPythonicToTs(pythonicConfig, key) {
  if (pythonicConfig === null) {
    return null;
  } else if (typeof pythonicConfig === "string") {
    return toCamelCase(pythonicConfig);
  } else if (typeof pythonicConfig === "number" || typeof pythonicConfig === "boolean") {
    return pythonicConfig;
  } else if (pythonicConfig instanceof Array) {
    const tsArray = [];
    const arrayLength = pythonicConfig.length;
    for (let i = 0; i < arrayLength; ++i) {
      const item = pythonicConfig[i];
      if (isArrayItemInputOrOutputName(key, i, item)) {
        tsArray.push(item);
      } else {
        tsArray.push(convertPythonicToTs(item, key));
      }
    }
    return tsArray;
  } else {
    const tsDict = {};
    for (const pythonicKey of Object.keys(pythonicConfig)) {
      const pythonicValue = pythonicConfig[pythonicKey];
      if (pythonicKey === "name" && typeof pythonicValue === "string") {
        tsDict[pythonicKey] = pythonicValue;
      } else {
        const tsKey = toCamelCase(pythonicKey);
        tsDict[tsKey] = convertPythonicToTs(pythonicValue, tsKey);
      }
    }
    return tsDict;
  }
}
function convertTsToPythonic(tsConfig, key) {
  if (tsConfig === null || tsConfig === void 0) {
    return null;
  } else if (typeof tsConfig === "string") {
    return toSnakeCase(tsConfig);
  } else if (typeof tsConfig === "number" || typeof tsConfig === "boolean") {
    return tsConfig;
  } else if (tsConfig instanceof Array) {
    const pyArray = [];
    const arrayLength = tsConfig.length;
    for (let i = 0; i < arrayLength; ++i) {
      const item = tsConfig[i];
      if (isArrayItemInputOrOutputName(key, i, item)) {
        pyArray.push(item);
      } else {
        pyArray.push(convertTsToPythonic(item, key));
      }
    }
    return pyArray;
  } else {
    const pyDict = {};
    for (const tsKey of Object.keys(tsConfig)) {
      const tsValue = tsConfig[tsKey];
      const pyKey = toSnakeCase(tsKey);
      if ((tsKey === "name" || tsKey === "className") && typeof tsValue === "string") {
        pyDict[pyKey] = tsValue;
      } else {
        pyDict[pyKey] = convertTsToPythonic(tsValue, tsKey);
      }
    }
    return pyDict;
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/version.js
var version = "3.21.0";

// node_modules/@tensorflow/tfjs-layers/dist/engine/container.js
var Container = class _Container extends Layer {
  constructor(args) {
    super({});
    this.containerNodes = /* @__PURE__ */ new Set();
    this.name = args.name;
    if (this.name == null) {
      const prefix = this.getClassName().toLowerCase();
      this.name = getUid(prefix);
    }
    this.supportsMasking = false;
    this.trainable_ = true;
    if (Array.isArray(args.inputs)) {
      this.inputs = args.inputs.slice();
    } else {
      this.inputs = [args.inputs];
    }
    if (Array.isArray(args.outputs)) {
      this.outputs = args.outputs.slice();
    } else {
      this.outputs = [args.outputs];
    }
    if (unique(this.inputs).length !== this.inputs.length) {
      throw new ValueError(`The list of inputs passed to the model is redundant. All inputs should only appear once. Found: ${this.inputs.map((x) => x.name)}`);
    }
    if (unique(this.outputs).length !== this.outputs.length) {
      console.warn(`The list of outputs passed to the model is redundant. All outputs should only appear once. Found: ${this.outputs.map((x) => x.name)}`);
    }
    this.inputLayers = [];
    this.inputLayersNodeIndices = [];
    this.inputLayersTensorIndices = [];
    this.outputLayers = [];
    this.outputLayersNodeIndices = [];
    this.outputLayersTensorIndices = [];
    this.layers = [];
    this.internalContainerRefs = [];
    for (const x of this.outputs) {
      const layer = x.sourceLayer;
      const nodeIndex = x.nodeIndex;
      const tensorIndex = x.tensorIndex;
      this.outputLayers.push(layer);
      this.outputLayersNodeIndices.push(nodeIndex);
      this.outputLayersTensorIndices.push(tensorIndex);
    }
    for (const x of this.inputs) {
      const layer = x.sourceLayer;
      const nodeIndex = x.nodeIndex;
      const tensorIndex = x.tensorIndex;
      assert2(nodeIndex === 0, "input layer has >1 nodes");
      assert2(tensorIndex === 0, "input layer has >1 tensors");
      this.inputLayers.push(layer);
      this.inputLayersNodeIndices.push(nodeIndex);
      this.inputLayersTensorIndices.push(tensorIndex);
    }
    this.inputNames = [];
    this.outputNames = [];
    this.feedInputShapes = [];
    this.feedInputNames = [];
    this.feedOutputNames = [];
    for (let i = 0; i < this.inputLayers.length; i++) {
      const layer = this.inputLayers[i];
      if (!(layer instanceof InputLayer)) {
        throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${args.inputs}. Input ${i} (0-based) originates from layer type ${layer.getClassName()}.`);
      }
      this.inputNames.push(layer.name);
      this.feedInputShapes.push(layer.batchInputShape);
      this.feedInputNames.push(layer.name);
    }
    for (const layer of this.outputLayers) {
      this.outputNames.push(layer.name);
    }
    this.internalInputShapes = this.inputs.map((x) => x.shape);
    this.internalOutputShapes = this.outputs.map((x) => x.shape);
    const nodesDepths = {};
    const nodeIDToNode = {};
    const layersDepths = {};
    const layerIDToLayer = {};
    const layerIndices = {};
    const nodesInDecreasingDepth = [];
    const buildMapOfGraph = (tensor, finishedNodes2, nodesInProgress2, layer, nodeIndex, tensorIndex) => {
      if (layer == null || nodeIndex == null || tensorIndex == null) {
        layer = tensor.sourceLayer;
        nodeIndex = tensor.nodeIndex;
        tensorIndex = tensor.tensorIndex;
      }
      const node = layer.inboundNodes[nodeIndex];
      if (nodesInProgress2.indexOf(node) !== -1) {
        throw new RuntimeError(`The tensor ${tensor.name} at layer "${layer.name}" is part of a cycle.`);
      }
      if (finishedNodes2.indexOf(node) !== -1) {
        return;
      }
      this.containerNodes.add(_Container.nodeKey(layer, nodeIndex));
      if (!(layer.id in layerIndices)) {
        layerIndices[layer.id] = Object.keys(layerIndices).length;
      }
      if (nodesInProgress2.indexOf(node) === -1) {
        nodesInProgress2.push(node);
      }
      const numInboundLayers = node.inboundLayers.length;
      for (let i = 0; i < numInboundLayers; i++) {
        const x = node.inputTensors[i];
        const layer2 = node.inboundLayers[i];
        const nodeIndex2 = node.nodeIndices[i];
        const tensorIndex2 = node.tensorIndices[i];
        buildMapOfGraph(x, finishedNodes2, nodesInProgress2, layer2, nodeIndex2, tensorIndex2);
      }
      finishedNodes2.push(node);
      while (nodesInProgress2.indexOf(node) >= 0) {
        nodesInProgress2.splice(nodesInProgress2.indexOf(node), 1);
      }
      nodesInDecreasingDepth.push(node);
    };
    const finishedNodes = [];
    const nodesInProgress = [];
    for (const x of this.outputs) {
      buildMapOfGraph(x, finishedNodes, nodesInProgress);
    }
    const reversedNodesInDecreasingDepth = nodesInDecreasingDepth.slice().reverse();
    for (const node of reversedNodesInDecreasingDepth) {
      nodeIDToNode[node.id] = node;
      if (!(node.id in nodesDepths)) {
        nodesDepths[node.id] = 0;
      }
      let depth = nodesDepths[node.id];
      const previousDepth = layersDepths[node.outboundLayer.id] == null ? 0 : layersDepths[node.outboundLayer.id];
      depth = Math.max(depth, previousDepth);
      layersDepths[node.outboundLayer.id] = depth;
      layerIDToLayer[node.outboundLayer.id] = node.outboundLayer;
      nodesDepths[node.id] = depth;
      for (let i = 0; i < node.inboundLayers.length; i++) {
        const inboundLayer = node.inboundLayers[i];
        const nodeIndex = node.nodeIndices[i];
        const inboundNode = inboundLayer.inboundNodes[nodeIndex];
        const previousDepth2 = nodesDepths[inboundNode.id] == null ? 0 : nodesDepths[inboundNode.id];
        nodesDepths[inboundNode.id] = Math.max(depth + 1, previousDepth2);
        nodeIDToNode[inboundNode.id] = inboundNode;
      }
    }
    const nodesByDepth = {};
    for (const nodeID in nodesDepths) {
      const depth = nodesDepths[nodeID];
      if (!(depth in nodesByDepth)) {
        nodesByDepth[depth] = [];
      }
      nodesByDepth[depth].push(nodeIDToNode[nodeID]);
    }
    const layersByDepth = {};
    for (const layerID in layersDepths) {
      const depth = layersDepths[layerID];
      if (!(depth in layersByDepth)) {
        layersByDepth[depth] = [];
      }
      layersByDepth[depth].push(layerIDToLayer[layerID]);
    }
    let depthKeys = Object.keys(layersByDepth).map((x) => parseInt(x, 10)).sort(reverseNumberCompare);
    this.layers = [];
    for (const depth of depthKeys) {
      const layersForDepth = layersByDepth[depth];
      layersForDepth.sort((a, b) => {
        const aIndex = layerIndices[a.id];
        const bIndex = layerIndices[b.id];
        if (aIndex < bIndex) {
          return -1;
        }
        if (aIndex > bIndex) {
          return 1;
        }
        return 0;
      });
      for (const layer of layersForDepth) {
        if (layer instanceof _Container) {
          this.internalContainerRefs.push(layer);
        }
        this.layers.push(layer);
      }
    }
    this.layersByDepth = layersByDepth;
    depthKeys = Object.keys(nodesByDepth).map((x) => parseInt(x, 10)).sort(reverseNumberCompare);
    const computableTensors = this.inputs.slice();
    const layersWithCompleteInput = [];
    for (const depth of depthKeys) {
      for (const node of nodesByDepth[depth]) {
        const layer = node.outboundLayer;
        if (layer != null) {
          for (const x of node.inputTensors) {
            if (computableTensors.indexOf(x) === -1) {
              throw new RuntimeError(`Graph disconnected: cannot obtain value for tensor ${x} at layer "${layer.name}". The following previous layers were accessed without issue: ${layersWithCompleteInput}`);
            }
          }
          for (const x of node.outputTensors) {
            computableTensors.push(x);
          }
          layersWithCompleteInput.push(layer.name);
        }
      }
    }
    this.nodesByDepth = nodesByDepth;
    const allNames = this.layers.map((x) => x.name);
    for (const name of allNames) {
      const numOccurrences = allNames.filter((x) => x === name).length;
      if (numOccurrences !== 1) {
        throw new RuntimeError(`The name "${name}" is used ${numOccurrences} times in the model. All layer names should be unique. Layer names: ` + JSON.stringify(allNames));
      }
    }
    this.outboundNodes = [];
    this.inboundNodes = [];
    new Node({
      outboundLayer: this,
      inboundLayers: [],
      nodeIndices: [],
      tensorIndices: [],
      inputTensors: this.inputs,
      outputTensors: this.outputs,
      inputMasks: this.inputs.map((x) => null),
      outputMasks: this.outputs.map((x) => null),
      inputShapes: this.inputs.map((x) => x.shape),
      outputShapes: this.outputs.map((x) => x.shape)
    });
    this.built = true;
    this._refCount = 1;
  }
  assertNotDisposed() {
    if (this._refCount === 0) {
      throw new Error(`Container '${this.name}' is already disposed.`);
    }
  }
  /**
   * Attempt to dispose a LayersModel's weights.
   *
   * This method decrease the reference count of the LayersModel object by 1.
   *
   * A LayersModel is reference-counted. Its reference count is incremented by 1
   * when it is first constructed and when it is used as a Layer of another
   * LayersModel.
   *
   * If the reference count of a LayersModel becomes 0, the `dispose` method of
   * all its constituent `Layer`s will be called.
   *
   * Note: If the reference count is greater than 0 after the decrement, the
   * `dispose` method of its constituent `Layer`s will *not* be called.
   *
   * After a LayersModel is disposed, it cannot be used in calls such as
   * 'predict`, `evaluate` or `fit` anymore.
   *
   * @returns A DisposeResult Object with the following fields:
   *   - refCountAfterDispose: The reference count of the LayersModel after this
   *     `dispose()` call.
   *   - numDisposedVariables: Number of `tf.Variable`s (i.e., weights) disposed
   *     during this `dispose()` call.
   * @throws {Error} If the layer is not built yet, or if the LayersModel has
   *   already been disposed.
   */
  dispose() {
    this.assertNotDisposed();
    const result = { refCountAfterDispose: null, numDisposedVariables: 0 };
    if (--this._refCount === 0) {
      for (const layer of this.layers) {
        result.numDisposedVariables += layer.dispose().numDisposedVariables;
      }
      for (const container of this.internalContainerRefs) {
        result.numDisposedVariables += container.dispose().numDisposedVariables;
      }
    }
    result.refCountAfterDispose = this._refCount;
    return result;
  }
  get trainable() {
    return this.trainable_;
  }
  set trainable(trainable) {
    this.layers.forEach((layer) => {
      layer._trainableWeights.forEach((w) => w.trainable = trainable);
    });
    this.trainable_ = trainable;
  }
  get trainableWeights() {
    if (this._trainableWeights.length > 0) {
      throw new ValueError("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");
    }
    if (!this.trainable) {
      return [];
    }
    let weights = [];
    for (const layer of this.layers) {
      weights = weights.concat(layer.trainableWeights);
    }
    return weights;
  }
  get nonTrainableWeights() {
    const weights = [];
    for (const layer of this.layers) {
      weights.push(...layer.nonTrainableWeights);
    }
    if (!this.trainable) {
      const trainableWeights = [];
      for (const layer of this.layers) {
        trainableWeights.push(...layer.trainableWeights);
      }
      return trainableWeights.concat(weights);
    }
    return weights;
  }
  get weights() {
    return this.trainableWeights.concat(this.nonTrainableWeights);
  }
  /**
   * Loads all layer weights from a JSON object.
   *
   * Porting Note: HDF5 weight files cannot be directly loaded in JavaScript /
   *   TypeScript. The utility script at `scripts/pykeras.py` offers means
   *   to convert them into JSON strings compatible with this method.
   * Porting Note: TensorFlow.js Layers supports only loading by name currently.
   *
   * @param weights A JSON mapping weight names to weight values as nested
   *   arrays of numbers, or a `NamedTensorMap`, i.e., a JSON mapping weight
   *   names to `tf.Tensor` objects.
   * @param strict Require that the provided weights exactly match those
   *   required by the container.  Default: `true`.  Passing `false` means that
   *   extra weights and missing weights will be silently ignored.
   */
  loadWeights(weights, strict = true) {
    const nameToWeight = {};
    let totalWeightsCount = 0;
    for (const layer of this.layers) {
      for (const weight of layer.weights) {
        if (nameToWeight[weight.originalName] != null) {
          throw new ValueError(`Duplicate weight name: ${weight.originalName}`);
        }
        nameToWeight[weight.originalName] = weight;
        totalWeightsCount++;
      }
    }
    const weightValueTuples = [];
    for (const name in weights) {
      let validatedName = name;
      if (nameToWeight[name] == null) {
        const tokens = name.split("/");
        const shortenNameArray = tokens.slice(0, -2).concat([tokens[tokens.length - 1]]);
        validatedName = shortenNameArray.join("/");
      }
      if (nameToWeight[validatedName] != null) {
        weightValueTuples.push([nameToWeight[validatedName], weights[name]]);
      } else if (strict) {
        throw new ValueError(`Provided weight data has no target variable: ${name}`);
      }
      delete nameToWeight[validatedName];
    }
    if (strict) {
      const unsetNames = [];
      for (const name in nameToWeight) {
        unsetNames.push(name);
      }
      if (unsetNames.length > 0) {
        throw new ValueError(`${unsetNames.length} of ${totalWeightsCount} weights are not set: ${unsetNames}`);
      }
    }
    batchSetValue(weightValueTuples);
  }
  /**
   * Util shared between different serialization methods.
   * @returns LayersModel config with Keras version information added.
   */
  updatedConfig() {
    const theConfig = this.getConfig();
    const modelConfig = {};
    modelConfig["className"] = this.getClassName();
    modelConfig["config"] = theConfig;
    modelConfig["kerasVersion"] = `tfjs-layers ${version}`;
    modelConfig["backend"] = "TensorFlow.js";
    return modelConfig;
  }
  /**
   * Returns a JSON string containing the network configuration.
   *
   * To load a network from a JSON save file, use
   * models.modelFromJSON(jsonString);
   * @param extraJsonArgs Unused in tfjs-layers, maintained for PyKeras
   * @param returnString Whether the return value should be stringified
   *    (default: `true`).
   * @returns a JSON string if `returnString` (default), or a JSON object if
   *   `!returnString`.
   */
  // tslint:disable-next-line:no-any
  toJSON(unused, returnString = true) {
    const modelConfig = convertTsToPythonic(this.updatedConfig());
    return returnString ? JSON.stringify(modelConfig) : modelConfig;
  }
  /**
   * Call the model on new inputs.
   *
   * In this case `call` just reapplies all ops in the graph to the new inputs
   * (e.g. build a new computational graph from the provided inputs).
   *
   * @param inputs A tensor or list of tensors.
   * @param mask A mask or list of masks. A mask can be either a tensor or null
   *   (no mask).
   *
   * @return A tensor if there is a single output, or a list of tensors if there
   *   are more than one outputs.
   */
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = toList(inputs);
      const feedDict = new FeedDict();
      for (let i = 0; i < this.inputs.length; ++i) {
        feedDict.add(this.inputs[i], inputs[i]);
      }
      return execute(this.outputs, feedDict, kwargs);
    });
  }
  /**
   * Computes an output mask tensor.
   *
   * @param inputs Tensor or list of tensors.
   * @param mask Tensor or list of tensors.
   *
   * @return null or a tensor (or list of tensors, one per output tensor of the
   * layer).
   */
  computeMask(inputs, mask) {
    return tidy(() => {
      inputs = toList(inputs);
      let masks;
      if (mask == null) {
        masks = pyListRepeat(null, inputs.length);
      } else {
        masks = toList(mask);
      }
      return this.runInternalGraph(inputs, masks)[1];
    });
  }
  /**
   * Computes the output shape of the layer.
   *
   * Assumes that the layer will be built to match that input shape provided.
   *
   * @param inputShape A shape (tuple of integers) or a list of shape tuples
   *   (one per output tensor of the layer). Shape tuples can include null for
   *   free dimensions, instead of an integer.
   */
  computeOutputShape(inputShape) {
    const inputShapes = normalizeShapeList(inputShape);
    if (inputShapes.length !== this.inputLayers.length) {
      throw new ValueError(`Invalid inputShape argument ${inputShape}: model has ${this.inputLayers.length} tensor inputs.`);
    }
    const layersToOutputShapes = {};
    for (let i = 0; i < inputShapes.length; i++) {
      const layer = this.inputLayers[i];
      const inputShape2 = inputShapes[i];
      const shapeKey = layer.name + "_0_0";
      layersToOutputShapes[shapeKey] = inputShape2;
    }
    const depthKeys = Object.keys(this.nodesByDepth).map((x) => parseInt(x, 10)).sort(reverseNumberCompare);
    if (depthKeys.length > 1) {
      for (const depth of depthKeys) {
        const nodes = this.nodesByDepth[depth];
        for (const node of nodes) {
          const layer = node.outboundLayer;
          if (this.inputLayers.map((x) => x.id).indexOf(layer.id) !== -1) {
            continue;
          }
          const inputShapes2 = [];
          for (let j = 0; j < node.inboundLayers.length; j++) {
            const inboundLayer = node.inboundLayers[j];
            const nodeIndex2 = node.nodeIndices[j];
            const tensorIndex = node.tensorIndices[j];
            const shapeKey = `${inboundLayer.name}_${nodeIndex2}_${tensorIndex}`;
            const inputShape2 = layersToOutputShapes[shapeKey];
            inputShapes2.push(inputShape2);
          }
          const outputShape = layer.computeOutputShape(singletonOrArray(inputShapes2));
          const outputShapes2 = normalizeShapeList(outputShape);
          const nodeIndex = layer.inboundNodes.indexOf(node);
          for (let j = 0; j < outputShapes2.length; j++) {
            const shapeKey = `${layer.name}_${nodeIndex}_${j}`;
            layersToOutputShapes[shapeKey] = outputShapes2[j];
          }
        }
      }
    }
    const outputShapes = [];
    const outputShapeKeys = [];
    for (let i = 0; i < this.outputLayers.length; i++) {
      const layer = this.outputLayers[i];
      const nodeIndex = this.outputLayersNodeIndices[i];
      const tensorIndex = this.outputLayersTensorIndices[i];
      const shapeKey = `${layer.name}_${nodeIndex}_${tensorIndex}`;
      outputShapeKeys.push(shapeKey);
    }
    for (let i = 0; i < outputShapeKeys.length; i++) {
      const key = outputShapeKeys[i];
      assert2(key in layersToOutputShapes);
      outputShapes.push(layersToOutputShapes[key]);
    }
    return singletonOrArray(outputShapes);
  }
  /**
   * Computes output tensors for new inputs.
   *
   * Note:
   *   - Expects `inputs` to be a list (potentially with 1 element).
   *
   * @param inputs List of tensors
   * @param masks List of masks (tensors or null).
   * @return Three lists: outputTensors, outputMasks, outputShapes
   */
  runInternalGraph(inputs, masks) {
    if (masks == null) {
      masks = pyListRepeat(null, inputs.length);
    }
    const tensorMap = {};
    for (let i = 0; i < this.inputs.length; ++i) {
      const x = this.inputs[i];
      const y = inputs[i];
      const mask = masks[i];
      tensorMap[x.id] = [y, mask];
    }
    const depthKeys = Object.keys(this.nodesByDepth).map((x) => parseInt(x, 10)).sort(reverseNumberCompare);
    for (const depth of depthKeys) {
      const nodes = this.nodesByDepth[depth];
      for (const node of nodes) {
        const layer = node.outboundLayer;
        const referenceInputTensors = node.inputTensors;
        const referenceOutputTensors = node.outputTensors;
        const computedData = new Array();
        for (const x of referenceInputTensors) {
          if (x.id in tensorMap) {
            computedData.push(tensorMap[x.id]);
          }
        }
        if (computedData.length === referenceInputTensors.length) {
          let kwargs = {};
          let computedTensors;
          let computedMasks;
          let outputTensors2;
          let outputMasks2;
          if (node.callArgs != null) {
            kwargs = node.callArgs;
          }
          if (computedData.length === 1) {
            const [computedTensor, computedMask] = computedData[0];
            if (kwargs["mask"] == null) {
              kwargs["mask"] = computedMask;
            }
            outputTensors2 = toList(layer.call(computedTensor, kwargs));
            outputMasks2 = toList(layer.computeMask(computedTensor, computedMask));
            computedTensors = [computedTensor];
            computedMasks = [computedMask];
          } else {
            computedTensors = computedData.map((x) => x[0]);
            computedMasks = computedData.map((x) => x[1]);
            if (kwargs["mask"] == null) {
              kwargs["mask"] = computedMasks;
            }
            outputTensors2 = toList(layer.call(computedTensors, kwargs));
            outputMasks2 = toList(layer.computeMask(computedTensors, computedMasks));
          }
          if (layer.activityRegularizer) {
            throw new NotImplementedError("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");
          }
          for (let i = 0; i < referenceOutputTensors.length; ++i) {
            const x = referenceOutputTensors[i];
            const y = outputTensors2[i];
            const mask = outputMasks2[i];
            tensorMap[x.id] = [y, mask];
          }
        }
      }
    }
    const outputTensors = [];
    const outputMasks = [];
    const outputShapes = [];
    for (const x of this.outputs) {
      assert2(x.id in tensorMap, `Could not compute output ${x.name} : ${x.id}`);
      const [tensor, mask] = tensorMap[x.id];
      outputShapes.push(tensor.shape);
      outputTensors.push(tensor);
      outputMasks.push(mask);
    }
    return [outputTensors, outputMasks, outputShapes];
  }
  /**
   * Builds a map of internal node keys to node ordering.
   * Used in serializaion a node orderings may change as unused nodes are
   * dropped. Porting Note:  This helper method was pulled out of getConfig to
   * improve readability.
   * @param layers An array of Layers in the model.
   * @returns Map of Node Keys to index order within the layer.
   */
  buildNodeConversionMap(layers) {
    const nodeConversionMap = {};
    let keptNodes;
    for (const layer of this.layers) {
      keptNodes = layer instanceof _Container ? 1 : 0;
      for (let originalNodeIndex = 0; originalNodeIndex < layer.inboundNodes.length; originalNodeIndex++) {
        const nodeKey = _Container.nodeKey(layer, originalNodeIndex);
        if (this.containerNodes.has(nodeKey)) {
          nodeConversionMap[nodeKey] = keptNodes;
          keptNodes += 1;
        }
      }
    }
    return nodeConversionMap;
  }
  /**
   * Retrieves a layer based on either its name (unique) or index.
   *
   * Indices are based on order of horizontal graph traversal (bottom-up).
   *
   * If both `name` and `index` are specified, `index` takes precedence.
   *
   * @param name Name of layer.
   * @param index Index of layer.
   * @returns A Layer instance.
   * @throws ValueError: In case of invalid layer name or index.
   *
   * @doc {
   *    heading: 'Layers',
   *    subheading: 'Classes',
   *    namespace: 'layers',
   *    subclasses: ['LayersModel']
   * }
   */
  getLayer(name, index) {
    if (index != null) {
      if (this.layers.length <= index) {
        throw new ValueError(`Was asked to retrieve layer at index ${index}, but model only has ${this.layers.length} layer(s).`);
      } else {
        return this.layers[index];
      }
    } else {
      if (name == null) {
        throw new ValueError("Provide either a layer name or layer index");
      }
    }
    for (const layer of this.layers) {
      if (layer.name === name) {
        return layer;
      }
    }
    throw new ValueError(`No such layer: ${name}`);
  }
  /**
   * Retrieves the Container's current loss values.
   *
   * Used for regularizers during training.
   */
  calculateLosses() {
    return tidy(() => {
      const losses = [];
      for (const layer of this.layers) {
        for (let nodeIndex = 0; nodeIndex < layer.inboundNodes.length; ++nodeIndex) {
          const nodeKey = _Container.nodeKey(layer, nodeIndex);
          if (this.containerNodes.has(nodeKey)) {
            losses.push(...layer.calculateLosses());
          }
        }
      }
      return losses;
    });
  }
  getConfig() {
    const config = { name: this.name };
    const nodeConversionMap = this.buildNodeConversionMap(this.layers);
    const layerConfigs = [];
    for (const layer of this.layers) {
      const layerClassName = layer.getClassName();
      const layerConfig = layer.getConfig();
      const filteredInboundNodes = [];
      for (let originalNodeIndex = 0; originalNodeIndex < layer.inboundNodes.length; originalNodeIndex++) {
        const node = layer.inboundNodes[originalNodeIndex];
        const nodeKey = _Container.nodeKey(layer, originalNodeIndex);
        let kwargs = {};
        if (this.containerNodes.has(nodeKey)) {
          if (node.callArgs) {
            try {
              JSON.stringify(node.callArgs);
              kwargs = node.callArgs;
            } catch (err) {
              console.warn(`Layer ${layer.name} was passed non-serializable keyword arguments: ${node.callArgs}. They will not be included in the serialized model (and thus will be missing at deserialization time).`);
              kwargs = {};
            }
          }
          if (node.inboundLayers.length > 0) {
            const nodeData = [];
            for (let i = 0; i < node.inboundLayers.length; i++) {
              const inboundLayer = node.inboundLayers[i];
              const nodeIndex = node.nodeIndices[i];
              const tensorIndex = node.tensorIndices[i];
              const nodeKey2 = _Container.nodeKey(inboundLayer, nodeIndex);
              let newNodeIndex = nodeConversionMap[nodeKey2];
              if (newNodeIndex == null) {
                newNodeIndex = 0;
              }
              nodeData.push([inboundLayer.name, newNodeIndex, tensorIndex, kwargs]);
            }
            filteredInboundNodes.push(nodeData);
          }
        }
      }
      const dict = {};
      dict["name"] = layer.name;
      dict["className"] = layerClassName;
      dict["config"] = layerConfig;
      dict["inboundNodes"] = filteredInboundNodes;
      layerConfigs.push(dict);
    }
    config["layers"] = layerConfigs;
    const modelInputs = [];
    for (let i = 0; i < this.inputLayers.length; i++) {
      const layer = this.inputLayers[i];
      const nodeIndex = this.inputLayersNodeIndices[i];
      const nodeKey = _Container.nodeKey(layer, nodeIndex);
      if (!this.containerNodes.has(nodeKey)) {
        continue;
      }
      let newNodeIndex = nodeConversionMap[nodeKey];
      if (newNodeIndex === null || newNodeIndex === void 0) {
        newNodeIndex = 0;
      }
      const tensorIndex = this.inputLayersTensorIndices[i];
      modelInputs.push([layer.name, newNodeIndex, tensorIndex]);
    }
    config["inputLayers"] = modelInputs;
    const modelOutputs = [];
    for (let i = 0; i < this.outputLayers.length; i++) {
      const layer = this.outputLayers[i];
      const nodeIndex = this.outputLayersNodeIndices[i];
      const nodeKey = _Container.nodeKey(layer, nodeIndex);
      if (!this.containerNodes.has(nodeKey)) {
        continue;
      }
      let newNodeIndex = nodeConversionMap[nodeKey];
      if (newNodeIndex === null || newNodeIndex === void 0) {
        newNodeIndex = 0;
      }
      const tensorIndex = this.outputLayersTensorIndices[i];
      modelOutputs.push([layer.name, newNodeIndex, tensorIndex]);
    }
    config["outputLayers"] = modelOutputs;
    return config;
  }
  /**
   * Instantiates a LayersModel from its config (output of `get_config()`).
   * @param cls the class to create
   * @param config LayersModel config dictionary.
   * @param customObjects An optional dictionary of custom objects.
   * @param fastWeightInit Optional flag to use fast weight initialization
   *   during deserialization. This is applicable to cases in which
   *   the initialization will be immediately overwritten by loaded weight
   *   values. Default: `false`.
   * @returns A LayersModel instance.
   * @throws ValueError: In case of improperly formatted config dict.
   */
  /** @nocollapse */
  static fromConfig(cls, config, customObjects = {}, fastWeightInit = false) {
    const createdLayers = {};
    const unprocessedNodes = {};
    function addUnprocessedNode(layer, nodeData) {
      if (!(layer.name in unprocessedNodes)) {
        unprocessedNodes[layer.name] = [nodeData];
      } else {
        unprocessedNodes[layer.name].push(nodeData);
      }
    }
    function processNode(layer, nodeData) {
      const inputTensors2 = [];
      let kwargs;
      for (const inputData of nodeData) {
        const inboundLayerName = inputData[0];
        const inboundNodeIndex = inputData[1];
        const inboundTensorIndex = inputData[2];
        kwargs = inputData[3] == null ? {} : inputData[3];
        if (!(inboundLayerName in createdLayers)) {
          addUnprocessedNode(layer, nodeData);
          return;
        }
        const inboundLayer = createdLayers[inboundLayerName];
        if (inboundLayer.inboundNodes.length <= inboundNodeIndex) {
          addUnprocessedNode(layer, nodeData);
          return;
        }
        const inboundNode = inboundLayer.inboundNodes[inboundNodeIndex];
        inputTensors2.push(inboundNode.outputTensors[inboundTensorIndex]);
      }
      if (inputTensors2.length > 0) {
        layer.apply(singletonOrArray(inputTensors2), kwargs);
      }
    }
    function processLayer(layerData) {
      const layerName = layerData["name"];
      const layer = deserialize(layerData, config["customObjects"] != null ? config["customObjects"] : {});
      layer.setFastWeightInitDuringBuild(fastWeightInit);
      createdLayers[layerName] = layer;
      const inboundNodesData = layerData["inboundNodes"];
      inboundNodesData.forEach((nodeData) => {
        if (!(nodeData instanceof Array)) {
          throw new ValueError(`Corrupted configuration, expected array for nodeData: ${nodeData}`);
        }
        addUnprocessedNode(layer, nodeData);
      });
    }
    const name = config["name"];
    const layersFromConfig = config["layers"];
    for (const layerData of layersFromConfig) {
      processLayer(layerData);
    }
    while (!isObjectEmpty(unprocessedNodes)) {
      for (const layerData of layersFromConfig) {
        const layer = createdLayers[layerData["name"]];
        if (layer.name in unprocessedNodes) {
          const currentUnprocessedNodesForLayer = unprocessedNodes[layer.name];
          delete unprocessedNodes[layer.name];
          for (const nodeData of currentUnprocessedNodesForLayer) {
            processNode(layer, nodeData);
          }
        }
      }
    }
    const inputTensors = [];
    const outputTensors = [];
    const inputLayersFromConfig = config["inputLayers"];
    for (const layerData of inputLayersFromConfig) {
      const layerName = layerData[0];
      const nodeIndex = layerData[1];
      const tensorIndex = layerData[2];
      assert2(layerName in createdLayers);
      const layer = createdLayers[layerName];
      const layerOutputTensors = layer.inboundNodes[nodeIndex].outputTensors;
      inputTensors.push(layerOutputTensors[tensorIndex]);
    }
    const outputLayersFromConfig = config["outputLayers"];
    for (const layerData of outputLayersFromConfig) {
      const layerName = layerData[0];
      const nodeIndex = layerData[1];
      const tensorIndex = layerData[2];
      assert2(layerName in createdLayers);
      const layer = createdLayers[layerName];
      const layerOutputTensors = layer.inboundNodes[nodeIndex].outputTensors;
      outputTensors.push(layerOutputTensors[tensorIndex]);
    }
    return new cls({ inputs: inputTensors, outputs: outputTensors, name });
  }
  /**
   * Determine whether the container is stateful.
   *
   * Porting Note: this is the equivalent of the stateful @property of
   *   the Container class in PyKeras.
   */
  get stateful() {
    if (this._stateful) {
      throw new ValueError("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");
    }
    for (const layer of this.layers) {
      if (layer.stateful) {
        return true;
      }
    }
    return false;
  }
  /**
   * Reset the state of all stateful constituent layers (if any).
   *
   * Examples of stateful layers include RNN layers whose `stateful` property
   * is set as `true`.
   */
  resetStates() {
    tidy(() => {
      this.layers.forEach((layer) => {
        if (layer.stateful) {
          layer.resetStates();
        }
      });
    });
  }
};

// node_modules/@tensorflow/tfjs-layers/dist/engine/training_utils.js
function standardizeSampleOrClassWeights(xWeight, outputNames, weightType) {
  const numOutputs = outputNames.length;
  if (xWeight == null || Array.isArray(xWeight) && xWeight.length === 0) {
    return outputNames.map((name) => null);
  }
  if (numOutputs === 1) {
    if (Array.isArray(xWeight) && xWeight.length === 1) {
      return xWeight;
    } else if (typeof xWeight === "object" && outputNames[0] in xWeight) {
      return [xWeight[outputNames[0]]];
    } else {
      return [xWeight];
    }
  }
  if (Array.isArray(xWeight)) {
    if (xWeight.length !== numOutputs) {
      throw new Error(`Provided ${weightType} is an array of ${xWeight.length} element(s), but the model has ${numOutputs} outputs. Make sure a set of weights is provided for each model output.`);
    }
    return xWeight;
  } else if (typeof xWeight === "object" && Object.keys(xWeight).length > 0 && typeof xWeight[Object.keys(xWeight)[0]] === "object") {
    const output = [];
    outputNames.forEach((outputName) => {
      if (outputName in xWeight) {
        output.push(xWeight[outputName]);
      } else {
        output.push(null);
      }
    });
    return output;
  } else {
    throw new Error(`The model has multiple (${numOutputs}) outputs, so ${weightType} must be either an array with ${numOutputs} elements or an object with ${outputNames} keys. Provided ${weightType} not understood: ${JSON.stringify(xWeight)}`);
  }
}
function standardizeClassWeights(classWeight, outputNames) {
  return standardizeSampleOrClassWeights(classWeight, outputNames, "classWeight");
}
async function standardizeWeights(y, sampleWeight, classWeight, sampleWeightMode) {
  if (sampleWeight != null || sampleWeightMode != null) {
    throw new Error("Support sampleWeight is not implemented yet");
  }
  if (classWeight != null) {
    const yClasses = tidy(() => {
      if (y.shape.length === 1) {
        return clone(y);
      } else if (y.shape.length === 2) {
        if (y.shape[1] > 1) {
          const axis = 1;
          return argMax(y, axis);
        } else if (y.shape[1] === 1) {
          return reshape(y, [y.shape[0]]);
        } else {
          throw new Error(`Encountered unexpected last-dimension size (${y.shape[1]}) during handling of class weights. The size is expected to be >= 1.`);
        }
      } else {
        throw new Error(`Unexpected rank of target (y) tensor (${y.rank}) during handling of class weights. The rank is expected to be 1 or 2.`);
      }
    });
    const yClassIndices = Array.from(await yClasses.data());
    dispose(yClasses);
    const classSampleWeight = [];
    yClassIndices.forEach((classIndex) => {
      if (classWeight[classIndex] == null) {
        throw new Error(`classWeight must contain all classes in the training data. The class ${classIndex} exists in the data but not in classWeight`);
      } else {
        classSampleWeight.push(classWeight[classIndex]);
      }
    });
    return tensor1d(classSampleWeight, "float32");
  } else {
    return null;
  }
}
function computeWeightedLoss(losses, sampleWeights) {
  return mul(losses, sampleWeights);
}

// node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js
var DEFAULT_VALIDATION_BATCH_SIZE = 32;
function standardizeDataIteratorOutput(model2, iteratorOut) {
  let xs;
  let ys;
  const iteratorOutObj = iteratorOut;
  xs = iteratorOutObj["xs"];
  ys = iteratorOutObj["ys"];
  util_exports.assert(xs != null && ys != null, () => `A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${iteratorOut}`);
  const flattenedXs = flattenTensorOrArrayOrMap("input", model2.inputNames, xs);
  const flattenedYs = flattenTensorOrArrayOrMap("output", model2.outputNames, ys);
  const batchSize = flattenedXs[0].shape[0];
  util_exports.assert(flattenedXs.length === model2.inputs.length, () => `LayersModel has ${model2.inputs.length} inputs, but the dataset provides ${flattenedXs.length} inputs.  (Expected input keys: ${JSON.stringify(model2.inputNames)})`);
  util_exports.assert(flattenedYs.length === model2.outputs.length, () => `LayersModel has ${model2.outputs.length} outputs, but the dataset provides ${flattenedYs.length} outputs.  (Expected output keys: ${JSON.stringify(model2.outputNames)})`);
  for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {
    util_exports.assert(flattenedXs[xIndex].shape[0] === batchSize, () => `Batch size mismatch: input ${model2.inputNames[xIndex]} has ${flattenedXs[xIndex].shape[0]}; expected  ${batchSize} based on input ${model2.inputNames[0]}.`);
  }
  for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {
    util_exports.assert(flattenedYs[yIndex].shape[0] === batchSize, () => `Batch size mismatch: output ${model2.outputNames[yIndex]} has ${flattenedYs[yIndex].shape[0]}; expected  ${batchSize} based on input ${model2.inputNames[0]}.`);
  }
  return { xs: flattenedXs, ys: flattenedYs };
}
function flattenTensorOrArrayOrMap(inputOrOutput, names, values) {
  if (values instanceof Tensor) {
    return [values];
  } else if (Array.isArray(values)) {
    util_exports.assert(values.length === names.length, () => `Received an array of ${values.length} Tensors, but expected ${names.length} to match the ${inputOrOutput} keys ${names}.`);
    return values;
  } else {
    const result = [];
    for (const name of names) {
      if (values[name] == null) {
        throw new ValueError(`The feature data generated by the dataset lacks the required ${inputOrOutput} key '${name}'.`);
      }
      result.push(values[name]);
    }
    return result;
  }
}
function standardizeTensorValidationData(data) {
  if (data.length === 3) {
    throw new NotImplementedError("Validation with sample weights is not implemented yet.");
  }
  return { xs: data[0], ys: data[1] };
}
async function fitDataset(model2, dataset, args) {
  const hasBatchesPerEpoch = args.batchesPerEpoch != null;
  util_exports.assert(model2.optimizer != null, () => "You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).");
  util_exports.assert(args != null, () => `For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.`);
  util_exports.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => `For fitDataset(), config.epochs is expected to be a positive integer, but got ${args.epochs}`);
  util_exports.assert(!hasBatchesPerEpoch || args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch), () => `For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${args.batchesPerEpoch}`);
  util_exports.assert(
    // tslint:disable-next-line:no-any
    args["validationSplit"] == null,
    () => "`validationSplit` is not supported by `fitDataset()`. Use validationData instead."
  );
  if (model2.isTraining) {
    throw new Error("Cannot start training because another fit() call is ongoing.");
  }
  model2.isTraining = true;
  try {
    const doValidation = args.validationData != null;
    let valXs;
    let valYs;
    if (doValidation) {
      if (isDatasetObject(args.validationData)) {
        util_exports.assert(args.validationBatches == null || args.validationBatches > 0 && Number.isInteger(args.validationBatches), () => `For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${args.validationBatches}`);
      } else {
        const validationData = standardizeTensorValidationData(args.validationData);
        valXs = validationData.xs;
        valYs = validationData.ys;
      }
    }
    const trainFunction = model2.makeTrainFunction();
    const outLabels = model2.getDedupedMetricsNames();
    let callbackMetrics;
    if (doValidation) {
      callbackMetrics = outLabels.slice().concat(outLabels.map((n) => "val_" + n));
    } else {
      callbackMetrics = outLabels.slice();
    }
    const callbacks2 = standardizeCallbacks(args.callbacks, args.yieldEvery);
    const verbose = args.verbose == null ? 1 : args.verbose;
    const { callbackList, history } = configureCallbacks(
      callbacks2,
      verbose,
      args.epochs,
      null,
      null,
      getStepsPerEpoch(dataset, args),
      null,
      // Batch size determined by the dataset itself.
      doValidation,
      callbackMetrics
    );
    callbackList.setModel(model2);
    model2.history = history;
    await callbackList.onTrainBegin();
    model2.stopTraining_ = false;
    let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;
    let dataIterator = await dataset.iterator();
    while (epoch < args.epochs) {
      const epochLogs = {};
      await callbackList.onEpochBegin(epoch);
      let stepsDone = 0;
      let batchIndex = 0;
      if (!hasBatchesPerEpoch) {
        dataIterator = await dataset.iterator();
      }
      while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {
        const iteratorOut = await dataIterator.next();
        if (hasBatchesPerEpoch && iteratorOut.done) {
          console.warn(`You provided \`batchesPerEpoch\` as ${args.batchesPerEpoch}, but your dataset iterator ran out of data after ${stepsDone} batches; interrupting training. Make sure that your dataset can generate at least \`batchesPerEpoch * epochs\` batches (in this case, ${args.batchesPerEpoch * args.epochs} batches). You may need to use the repeat() function when building your dataset.`);
          break;
        }
        if (iteratorOut.value != null) {
          const { xs, ys } = standardizeDataIteratorOutput(model2, iteratorOut.value);
          const batchLogs = {};
          batchLogs["batch"] = batchIndex;
          batchLogs["size"] = xs[0].shape[0];
          await callbackList.onBatchBegin(batchIndex, batchLogs);
          const sampleWeights = [];
          if (args.classWeight != null) {
            const standardClassWeights = standardizeClassWeights(args.classWeight, model2.outputNames);
            for (let i = 0; i < standardClassWeights.length; ++i) {
              sampleWeights.push(await standardizeWeights(ys[i], null, standardClassWeights[i]));
            }
          }
          const ins = xs.concat(ys).concat(sampleWeights);
          const outs = trainFunction(ins);
          dispose(ins);
          for (let i = 0; i < outLabels.length; ++i) {
            const label = outLabels[i];
            const out = outs[i];
            batchLogs[label] = out;
            keep(out);
          }
          await callbackList.onBatchEnd(batchIndex, batchLogs);
          disposeTensorsInLogs(batchLogs);
          batchIndex++;
          stepsDone++;
        }
        if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch : iteratorOut.done) {
          if (doValidation) {
            let valOuts;
            if (isDatasetObject(args.validationData)) {
              valOuts = toList(await model2.evaluateDataset(args.validationData, { batches: args.validationBatches }));
            } else {
              valOuts = toList(model2.evaluate(valXs, valYs, {
                batchSize: args.validationBatchSize == null ? DEFAULT_VALIDATION_BATCH_SIZE : args.validationBatchSize,
                verbose: 0
              }));
            }
            for (let i = 0; i < model2.metricsNames.length; ++i) {
              epochLogs[`val_${model2.metricsNames[i]}`] = valOuts[i];
            }
          }
          break;
        }
        if (model2.stopTraining_) {
          break;
        }
      }
      await callbackList.onEpochEnd(epoch, epochLogs);
      epoch++;
      if (model2.stopTraining_) {
        break;
      }
    }
    await callbackList.onTrainEnd();
    await model2.history.syncData();
    return model2.history;
  } finally {
    model2.isTraining = false;
  }
}
function getStepsPerEpoch(dataset, args) {
  let stepsPerEpoch = null;
  if (args.batchesPerEpoch != null) {
    stepsPerEpoch = args.batchesPerEpoch;
  } else if (Number.isFinite(dataset.size)) {
    stepsPerEpoch = dataset.size;
  }
  return stepsPerEpoch;
}
function isDatasetObject(dataset) {
  return typeof dataset.iterator === "function";
}
function isLazyIteratorObject(iterator) {
  return typeof iterator.next === "function";
}
async function evaluateDataset(model2, dataset, args) {
  args = args || {};
  const hasBatches = args.batches != null;
  const f = model2.testFunction;
  let outs = [];
  if (args.verbose > 0) {
    throw new NotImplementedError("Verbose mode is not implemented yet.");
  }
  util_exports.assert(!hasBatches || args.batches > 0 && Number.isInteger(args.batches), () => `Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(args.batches)}`);
  const dataIterator = isLazyIteratorObject(dataset) ? dataset : await dataset.iterator();
  let numExamples = 0;
  let batch = 0;
  while (hasBatches ? batch < args.batches : true) {
    const iteratorOut = await dataIterator.next();
    outs = tidy(() => {
      if (iteratorOut.value) {
        const { xs, ys } = standardizeDataIteratorOutput(model2, iteratorOut.value);
        const xsAndYs = xs.concat(ys);
        const batchOuts = tidy(() => f(xsAndYs));
        dispose(xsAndYs);
        if (batch === 0) {
          for (let i = 0; i < batchOuts.length; ++i) {
            outs.push(scalar(0));
          }
        }
        const batchSize = xsAndYs[0].shape[0];
        for (let i = 0; i < batchOuts.length; ++i) {
          const batchOut = batchOuts[i];
          const oldScalar = outs[i];
          outs[i] = tidy(() => add(outs[i], mul(batchSize, batchOut)));
          if (batch > 0) {
            dispose(oldScalar);
          }
        }
        dispose(batchOuts);
        numExamples += batchSize;
        ++batch;
      }
      return outs;
    });
    if (iteratorOut.done) {
      if (hasBatches) {
        console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${args.batches} batches). You may need to use the repeat() function when building your dataset.`);
      }
      break;
    }
  }
  for (let i = 0; i < outs.length; ++i) {
    const oldScalar = outs[i];
    outs[i] = div(outs[i], numExamples);
    dispose(oldScalar);
  }
  return singletonOrArray(outs);
}

// node_modules/@tensorflow/tfjs-layers/dist/engine/training_tensors.js
function checkBatchSize(batchSize) {
  util_exports.assert(batchSize > 0 && Number.isInteger(batchSize), () => `batchSize is required to be a positive integer, but got ${batchSize}`);
}
function sliceArrays(arrays, start, stop) {
  if (arrays == null) {
    return [null];
  } else if (Array.isArray(arrays)) {
    return arrays.map((array) => sliceAlongFirstAxis(array, start, stop - start));
  } else {
    return sliceAlongFirstAxis(arrays, start, stop - start);
  }
}
function sliceArraysByIndices(arrays, indices) {
  return tidy(() => {
    if (arrays == null) {
      return null;
    } else if (Array.isArray(arrays)) {
      return arrays.map((array) => sliceArraysByIndices(array, indices));
    } else {
      return gather2(arrays, indices.dtype === "int32" ? indices : cast(indices, "int32"));
    }
  });
}
function makeBatches(size, batchSize) {
  const output = [];
  let batchStart = 0;
  let batchEnd = null;
  while (batchStart < size) {
    batchEnd = batchStart + batchSize;
    if (batchEnd >= size) {
      batchEnd = size;
    }
    output.push([batchStart, batchEnd]);
    batchStart = batchEnd;
  }
  return output;
}
async function fitLoop(model2, f, ins, outLabels, batchSize, epochs, verbose, callbacks2, valF, valIns, shuffle, callbackMetrics, initialEpoch, stepsPerEpoch, validationSteps) {
  if (batchSize == null) {
    batchSize = 32;
  }
  if (epochs == null) {
    epochs = 1;
  }
  if (shuffle == null) {
    shuffle = true;
  }
  if (initialEpoch == null) {
    initialEpoch = 0;
  }
  let doValidation = false;
  if (valF != null && valIns != null) {
    doValidation = true;
  }
  if (validationSteps != null) {
    doValidation = true;
    if (stepsPerEpoch == null) {
      throw new ValueError("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");
    }
  }
  const numTrainSamples = model2.checkNumSamples(ins, batchSize, stepsPerEpoch, "steps_per_epoch");
  let indexArray;
  if (numTrainSamples != null) {
    indexArray = range(0, numTrainSamples);
  }
  if (verbose == null) {
    verbose = 1;
  }
  const { callbackList, history } = configureCallbacks(callbacks2, verbose, epochs, initialEpoch, numTrainSamples, stepsPerEpoch, batchSize, doValidation, callbackMetrics);
  callbackList.setModel(model2);
  model2.history = history;
  await callbackList.onTrainBegin();
  model2.stopTraining_ = false;
  for (let epoch = initialEpoch; epoch < epochs; ++epoch) {
    await callbackList.onEpochBegin(epoch);
    const epochLogs = {};
    if (stepsPerEpoch != null) {
      throw new NotImplementedError("stepsPerEpoch mode is not implemented yet.");
    } else {
      if (shuffle === "batch") {
        throw new NotImplementedError("batch shuffling is not implemneted yet");
      } else if (shuffle) {
        util_exports.shuffle(indexArray);
      }
      const epochIndexArray1D = tensor1d(indexArray);
      const batches = makeBatches(numTrainSamples, batchSize);
      for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {
        const batchLogs = {};
        await callbackList.onBatchBegin(batchIndex, batchLogs);
        tidy(() => {
          const batchStart = batches[batchIndex][0];
          const batchEnd = batches[batchIndex][1];
          const batchIds = sliceAlongFirstAxis(epochIndexArray1D, batchStart, batchEnd - batchStart);
          batchLogs["batch"] = batchIndex;
          batchLogs["size"] = batchEnd - batchStart;
          const insBatch = sliceArraysByIndices(ins, batchIds);
          const outs = f(insBatch);
          for (let i = 0; i < outLabels.length; ++i) {
            const label = outLabels[i];
            const out = outs[i];
            batchLogs[label] = out;
            keep(out);
          }
          if (batchIndex === batches.length - 1) {
            if (doValidation) {
              const valOuts = model2.testLoop(valF, valIns, batchSize);
              for (let i = 0; i < outLabels.length; ++i) {
                const label = outLabels[i];
                const out = valOuts[i];
                keep(out);
                epochLogs["val_" + label] = out;
              }
            }
          }
        });
        await callbackList.onBatchEnd(batchIndex, batchLogs);
        disposeTensorsInLogs(batchLogs);
        if (model2.stopTraining_) {
          break;
        }
      }
      epochIndexArray1D.dispose();
    }
    await callbackList.onEpochEnd(epoch, epochLogs);
    if (model2.stopTraining_) {
      break;
    }
  }
  await callbackList.onTrainEnd();
  await model2.history.syncData();
  return model2.history;
}
async function fitTensors(model2, x, y, args = {}) {
  if (model2.isTraining) {
    throw new Error("Cannot start training because another fit() call is ongoing.");
  }
  model2.isTraining = true;
  let inputs;
  let targets;
  let originalInputs;
  let originalTargets;
  let inputValX;
  let inputValY;
  let valX;
  let valY;
  let sampleWeights;
  try {
    const batchSize = args.batchSize == null ? 32 : args.batchSize;
    checkBatchSize(batchSize);
    const checkBatchAxis = false;
    const standardizedOuts = await model2.standardizeUserData(x, y, args.sampleWeight, args.classWeight, checkBatchAxis, batchSize);
    inputs = standardizedOuts[0];
    targets = standardizedOuts[1];
    sampleWeights = standardizedOuts[2];
    let doValidation = false;
    let valIns;
    if (args.validationData != null && args.validationData.length > 0) {
      doValidation = true;
      if (args.validationData.length === 2) {
        inputValX = args.validationData[0];
        inputValY = args.validationData[1];
      } else if (args.validationData.length === 3) {
        throw new NotImplementedError("validationData including sample weights is not supported yet.");
      } else {
        throw new ValueError(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${args.validationData} is invalid.`);
      }
      const checkBatchAxis2 = true;
      const valStandardized = await model2.standardizeUserData(
        inputValX,
        inputValY,
        null,
        /** Unused sample weights. */
        null,
        /** Unused class weights. */
        checkBatchAxis2,
        batchSize
      );
      valX = valStandardized[0];
      valY = valStandardized[1];
      valIns = valX.concat(valY);
    } else if (args.validationSplit != null && args.validationSplit > 0 && args.validationSplit < 1) {
      doValidation = true;
      const splitAt = Math.floor(inputs[0].shape[0] * (1 - args.validationSplit));
      const originalBatchSize = inputs[0].shape[0];
      valX = sliceArrays(inputs, splitAt, originalBatchSize);
      originalInputs = inputs;
      inputs = sliceArrays(inputs, 0, splitAt);
      valY = sliceArrays(targets, splitAt, originalBatchSize);
      originalTargets = targets;
      targets = sliceArrays(targets, 0, splitAt);
      valIns = valX.concat(valY);
    } else if (args.validationSteps != null) {
      doValidation = true;
    }
    const ins = inputs.concat(targets).concat(sampleWeights);
    model2.checkTrainableWeightsConsistency();
    const trainFunction = model2.makeTrainFunction();
    const outLabels = model2.getDedupedMetricsNames();
    let valFunction;
    let callbackMetrics;
    if (doValidation) {
      model2.makeTestFunction();
      valFunction = model2.testFunction;
      callbackMetrics = outLabels.slice().concat(outLabels.map((n) => "val_" + n));
    } else {
      valFunction = null;
      valIns = [];
      callbackMetrics = outLabels.slice();
    }
    const callbacks2 = standardizeCallbacks(args.callbacks, args.yieldEvery);
    const out = await fitLoop(model2, trainFunction, ins, outLabels, batchSize, args.epochs, args.verbose, callbacks2, valFunction, valIns, args.shuffle, callbackMetrics, args.initialEpoch, null, null);
    return out;
  } finally {
    model2.isTraining = false;
    disposeNewTensors(inputs, x);
    disposeNewTensors(targets, y);
    disposeNewTensors(originalInputs, x);
    disposeNewTensors(originalTargets, y);
    disposeNewTensors(valX, inputValX);
    disposeNewTensors(valY, inputValY);
    if (sampleWeights != null) {
      dispose(sampleWeights);
    }
  }
}
function ensureTensorsRank2OrHigher(tensors) {
  const outs = [];
  if (tensors instanceof Tensor) {
    tensors = [tensors];
  }
  for (let i = 0; i < tensors.length; ++i) {
    const tensor = tensors[i];
    if (tensor.rank === 1) {
      outs.push(expandDims2(tensor, 1));
    } else if (tensor.rank === 0) {
      throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");
    } else {
      outs.push(tensor);
    }
  }
  return outs;
}
function disposeNewTensors(tensors, refTensors) {
  if (tensors == null) {
    return;
  }
  const oldTensorIds = [];
  if (refTensors instanceof Tensor) {
    oldTensorIds.push(refTensors.id);
  } else if (Array.isArray(refTensors)) {
    refTensors.forEach((t) => oldTensorIds.push(t.id));
  } else if (refTensors != null) {
    for (const name in refTensors) {
      const oldTensor = refTensors[name];
      oldTensorIds.push(oldTensor.id);
    }
  }
  const tensorsToDispose = [];
  if (tensors instanceof Tensor) {
    if (oldTensorIds.indexOf(tensors.id) === -1) {
      tensorsToDispose.push(tensors);
    }
  } else if (Array.isArray(tensors)) {
    tensors.forEach((t) => {
      if (oldTensorIds.indexOf(t.id) === -1) {
        tensorsToDispose.push(t);
      }
    });
  } else if (tensors != null) {
    for (const name in tensors) {
      const tensor = tensors[name];
      if (oldTensorIds.indexOf(tensor.id) === -1) {
        tensorsToDispose.push(tensor);
      }
    }
  }
  tensorsToDispose.forEach((t) => {
    if (!t.isDisposed) {
      t.dispose();
    }
  });
}

// node_modules/@tensorflow/tfjs-layers/dist/engine/training.js
function isDataTensor(x) {
  return x instanceof Tensor;
}
function isDataArray(x) {
  return Array.isArray(x);
}
function isDataDict(x) {
  return !isDataTensor(x) && !isDataArray(x);
}
function standardizeInputData(data, names, shapes, checkBatchAxis = true, exceptionPrefix = "") {
  if (names == null || names.length === 0) {
    if (data != null) {
      let gotUnexpectedData = false;
      if (isDataArray(data) && data.length > 0) {
        gotUnexpectedData = true;
      } else if (isDataDict(data)) {
        for (const key in data) {
          if (data.hasOwnProperty(key)) {
            gotUnexpectedData = true;
            break;
          }
        }
      } else {
        gotUnexpectedData = true;
      }
      if (gotUnexpectedData) {
        throw new ValueError(`Error when checking model ${exceptionPrefix} expected no data, but got ${data}`);
      }
    }
    return [];
  }
  if (data == null) {
    return names.map((name) => null);
  }
  let arrays;
  if (isDataDict(data)) {
    data = data;
    arrays = [];
    for (const name of names) {
      if (data[name] == null) {
        throw new ValueError(`No data provided for "${name}". Need data for each key in: ${names}`);
      }
      arrays.push(data[name]);
    }
  } else if (isDataArray(data)) {
    data = data;
    if (data.length !== names.length) {
      throw new ValueError(`Error when checking model ${exceptionPrefix}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${names.length} Tensor(s), but instead got the following list of Tensor(s): ${data}`);
    }
    arrays = data;
  } else {
    data = data;
    if (names.length > 1) {
      throw new ValueError(`The model ${exceptionPrefix} expects ${names.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${data.shape}`);
    }
    arrays = [data];
  }
  arrays = ensureTensorsRank2OrHigher(arrays);
  if (shapes != null) {
    for (let i = 0; i < names.length; ++i) {
      if (shapes[i] == null) {
        continue;
      }
      const array = arrays[i];
      if (array.shape.length !== shapes[i].length) {
        throw new ValueError(`Error when checking ${exceptionPrefix}: expected ${names[i]} to have ${shapes[i].length} dimension(s). but got array with shape ${array.shape}`);
      }
      for (let j = 0; j < shapes[i].length; ++j) {
        if (j === 0 && !checkBatchAxis) {
          continue;
        }
        const dim = array.shape[j];
        const refDim = shapes[i][j];
        if (refDim != null && refDim >= 0 && dim !== refDim) {
          throw new ValueError(`${exceptionPrefix} expected a batch of elements where each example has shape [${shapes[i].slice(1, shapes[i].length)}] (i.e.,tensor shape [*,${shapes[i].slice(1, shapes[i].length)}]) but the ${exceptionPrefix} received an input with ${array.shape[0]} examples, each with shape [${array.shape.slice(1, array.shape.length)}] (tensor shape [${array.shape}])`);
        }
      }
    }
  }
  return arrays;
}
function checkArrayLengths(inputs, targets, weights) {
  const setX = unique(inputs.map((input2) => input2.shape[0]));
  setX.sort();
  const setY = unique(targets.map((target) => target.shape[0]));
  setY.sort();
  if (setX.length > 1) {
    throw new ValueError(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(inputs.map((input2) => input2.shape))}`);
  }
  if (setY.length > 1) {
    throw new ValueError(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(targets.map((target) => target.shape))}`);
  }
  if (setX.length > 0 && setY.length > 0 && !util_exports.arraysEqual(setX, setY)) {
    throw new ValueError(`Input Tensors should have the same number of samples as target Tensors. Found ${setX[0]} input sample(s) and ${setY[0]} target sample(s).`);
  }
}
function checkLossAndTargetCompatibility(targets, lossFns, outputShapes) {
  const keyLosses = [
    meanSquaredError,
    binaryCrossentropy,
    categoricalCrossentropy
  ];
  for (let i = 0; i < targets.length; ++i) {
    const y = targets[i];
    const loss = lossFns[i];
    const shape = outputShapes[i];
    if (loss == null) {
      continue;
    }
    if (loss === categoricalCrossentropy) {
      if (y.shape[y.shape.length - 1] === 1) {
        throw new ValueError(`You are passing a target array of shape ${y.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);
      }
    }
    if (keyLosses.indexOf(loss) !== -1) {
      const slicedYShape = y.shape.slice(1);
      const slicedShape = shape.slice(1);
      for (let j = 0; j < slicedYShape.length; ++j) {
        const targetDim = slicedYShape[j];
        const outDim = slicedShape[j];
        if (outDim != null && targetDim !== outDim) {
          throw new ValueError(`A target Tensor with shape ${y.shape} was passed for an output of shape ${shape}, while using a loss function that expects targets to have the same shape as the output.`);
        }
      }
    }
  }
}
function checkInputData(data, names, shapes, checkBatchAxis = true, exceptionPrefix = "") {
  let arrays;
  if (Array.isArray(data)) {
    if (data.length !== names.length) {
      throw new ValueError(`Error when checking model ${exceptionPrefix}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${names.length} Tensor(s), but instead got ${data.length} Tensors(s).`);
    }
    arrays = data;
  } else {
    if (names.length > 1) {
      throw new ValueError(`The model expects ${names.length} ${exceptionPrefix} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(data.shape)}.`);
    }
    arrays = [data];
  }
  if (shapes != null) {
    for (let i = 0; i < names.length; ++i) {
      if (shapes[i] == null) {
        continue;
      }
      const array = arrays[i];
      if (array.shape.length !== shapes[i].length) {
        throw new ValueError(`Error when checking ${exceptionPrefix}: expected ${names[i]} to have ${shapes[i].length} dimension(s), but got array with shape ${JSON.stringify(array.shape)}`);
      }
      for (let j = 0; j < shapes[i].length; ++j) {
        if (j === 0 && !checkBatchAxis) {
          continue;
        }
        const dim = array.shape[j];
        const refDim = shapes[i][j];
        if (refDim != null) {
          if (refDim !== dim) {
            throw new ValueError(`Error when checking ${exceptionPrefix}: expected ${names[i]} to have shape ${JSON.stringify(shapes[i])} but got array with shape ${JSON.stringify(array.shape)}.`);
          }
        }
      }
    }
  }
}
function collectMetrics(metrics, outputNames) {
  if (metrics == null || Array.isArray(metrics) && metrics.length === 0) {
    return outputNames.map((name) => []);
  }
  let wrappedMetrics;
  if (typeof metrics === "string" || typeof metrics === "function") {
    wrappedMetrics = [metrics];
  } else if (Array.isArray(metrics) || typeof metrics === "object") {
    wrappedMetrics = metrics;
  } else {
    throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${metrics}`);
  }
  if (Array.isArray(wrappedMetrics)) {
    return outputNames.map((name) => wrappedMetrics);
  } else {
    const nestedMetrics = [];
    for (const name of outputNames) {
      let outputMetrics = wrappedMetrics.hasOwnProperty(name) ? wrappedMetrics[name] : [];
      if (!Array.isArray(outputMetrics)) {
        outputMetrics = [outputMetrics];
      }
      nestedMetrics.push(outputMetrics);
    }
    return nestedMetrics;
  }
}
var LAYERS_MODEL_FORMAT_NAME = "layers-model";
var LayersModel = class extends Container {
  constructor(args) {
    super(args);
    this.isTraining = false;
  }
  /**
   * Print a text summary of the model's layers.
   *
   * The summary includes
   * - Name and type of all layers that comprise the model.
   * - Output shape(s) of the layers
   * - Number of weight parameters of each layer
   * - If the model has non-sequential-like topology, the inputs each layer
   *   receives
   * - The total number of trainable and non-trainable parameters of the model.
   *
   * ```js
   * const input1 = tf.input({shape: [10]});
   * const input2 = tf.input({shape: [20]});
   * const dense1 = tf.layers.dense({units: 4}).apply(input1);
   * const dense2 = tf.layers.dense({units: 8}).apply(input2);
   * const concat = tf.layers.concatenate().apply([dense1, dense2]);
   * const output =
   *     tf.layers.dense({units: 3, activation: 'softmax'}).apply(concat);
   *
   * const model = tf.model({inputs: [input1, input2], outputs: output});
   * model.summary();
   * ```
   *
   * @param lineLength Custom line length, in number of characters.
   * @param positions Custom widths of each of the columns, as either
   *   fractions of `lineLength` (e.g., `[0.5, 0.75, 1]`) or absolute number
   *   of characters (e.g., `[30, 50, 65]`). Each number corresponds to
   *   right-most (i.e., ending) position of a column.
   * @param printFn Custom print function. Can be used to replace the default
   *   `console.log`. For example, you can use `x => {}` to mute the printed
   *   messages in the console.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  summary(lineLength, positions, printFn = console.log) {
    if (!this.built) {
      throw new ValueError(`This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).`);
    }
    printSummary(this, lineLength, positions, printFn);
  }
  /**
   * Configures and prepares the model for training and evaluation.  Compiling
   * outfits the model with an optimizer, loss, and/or metrics.  Calling `fit`
   * or `evaluate` on an un-compiled model will throw an error.
   *
   * @param args a `ModelCompileArgs` specifying the loss, optimizer, and
   * metrics to be used for fitting and evaluating this model.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  compile(args) {
    if (args.loss == null) {
      args.loss = [];
    }
    this.loss = args.loss;
    if (typeof args.optimizer === "string") {
      this.optimizer_ = getOptimizer(args.optimizer);
      this.isOptimizerOwned = true;
    } else {
      if (!(args.optimizer instanceof Optimizer)) {
        throw new ValueError(`User-defined optimizer must be an instance of tf.Optimizer.`);
      }
      this.optimizer_ = args.optimizer;
      this.isOptimizerOwned = false;
    }
    let lossFunctions = [];
    if (!Array.isArray(args.loss) && typeof args.loss !== "string" && typeof args.loss !== "function") {
      args.loss = args.loss;
      for (const name in args.loss) {
        if (this.outputNames.indexOf(name) === -1) {
          throw new ValueError(`Unknown entry in loss dictionary: "${name}". Only expected the following keys: ${this.outputNames}`);
        }
      }
      for (const name of this.outputNames) {
        if (args.loss[name] == null) {
          console.warn(`Output "${name}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${name} during training`);
        }
        lossFunctions.push(get(args.loss[name]));
      }
    } else if (Array.isArray(args.loss)) {
      if (args.loss.length !== this.outputs.length) {
        throw new ValueError(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${args.loss}.`);
      }
      const theLosses = args.loss;
      lossFunctions = theLosses.map((l) => get(l));
    } else {
      const lossFunction = get(args.loss);
      this.outputs.forEach((_) => {
        lossFunctions.push(lossFunction);
      });
    }
    this.lossFunctions = lossFunctions;
    this.feedOutputNames = [];
    this.feedOutputShapes = [];
    this.feedLossFns = [];
    for (let i = 0; i < this.outputs.length; ++i) {
      const shape = this.internalOutputShapes[i];
      const name = this.outputNames[i];
      this.feedOutputNames.push(name);
      this.feedOutputShapes.push(shape);
      this.feedLossFns.push(this.lossFunctions[i]);
    }
    const skipTargetIndices = [];
    this.metrics = args.metrics;
    this.metricsNames = ["loss"];
    this.metricsTensors = [];
    nameScope("loss", () => {
      for (let i = 0; i < this.outputs.length; ++i) {
        if (skipTargetIndices.indexOf(i) !== -1) {
          continue;
        }
        const weightedLoss = this.lossFunctions[i];
        if (this.outputs.length > 1) {
          this.metricsTensors.push([weightedLoss, i]);
          this.metricsNames.push(this.outputNames[i] + "_loss");
        }
      }
    });
    const nestedMetrics = collectMetrics(args.metrics, this.outputNames);
    const appendMetric = (outputIndex, metricName, metricTensor) => {
      if (this.outputNames.length > 1) {
        metricName = this.outputNames[outputIndex] + "_" + metricName;
      }
      this.metricsNames.push(metricName);
      this.metricsTensors.push([metricTensor, outputIndex]);
    };
    nameScope("metric", () => {
      for (let i = 0; i < this.outputs.length; ++i) {
        if (skipTargetIndices.indexOf(i) !== -1) {
          continue;
        }
        const outputMetrics = nestedMetrics[i];
        const handleMetrics = (metrics) => {
          const metricNamePrefix = "";
          let metricName;
          let accFn;
          let weightedMetricFn;
          for (const metric of metrics) {
            if (typeof metric === "string" && ["accuracy", "acc", "crossentropy", "ce"].indexOf(metric) !== -1) {
              const outputShape = this.internalOutputShapes[i];
              if (outputShape[outputShape.length - 1] === 1 || this.lossFunctions[i] === binaryCrossentropy) {
                if (["accuracy", "acc"].indexOf(metric) !== -1) {
                  accFn = binaryAccuracy;
                } else if (["crossentropy", "ce"].indexOf(metric) !== -1) {
                  accFn = binaryCrossentropy2;
                }
              } else if (this.lossFunctions[i] === sparseCategoricalCrossentropy) {
                if (["accuracy", "acc"].indexOf(metric) !== -1) {
                  accFn = sparseCategoricalAccuracy;
                } else if (["crossentropy", "ce"].indexOf(metric) !== -1) {
                  accFn = sparseCategoricalCrossentropy2;
                }
              } else {
                if (["accuracy", "acc"].indexOf(metric) !== -1) {
                  accFn = categoricalAccuracy;
                } else if (["crossentropy", "ce"].indexOf(metric) !== -1) {
                  accFn = categoricalCrossentropy2;
                }
              }
              let suffix;
              if (["accuracy", "acc"].indexOf(metric) !== -1) {
                suffix = "acc";
              } else if (["crossentropy", "ce"].indexOf(metric) !== -1) {
                suffix = "ce";
              }
              weightedMetricFn = accFn;
              metricName = metricNamePrefix + suffix;
            } else {
              const metricFn = get2(metric);
              weightedMetricFn = metricFn;
              metricName = metricNamePrefix + getLossOrMetricName(metric);
            }
            let metricResult;
            nameScope(metricName, () => {
              metricResult = weightedMetricFn;
            });
            appendMetric(i, metricName, metricResult);
          }
        };
        handleMetrics(outputMetrics);
      }
    });
    this.collectedTrainableWeights = this.trainableWeights;
  }
  /**
   * Check trainable weights count consistency.
   *
   * This will raise a warning if `this.trainableWeights` and
   * `this.collectedTrainableWeights` are inconsistent (i.e., have different
   * numbers of parameters).
   * Inconsistency will typically arise when one modifies `model.trainable`
   * without calling `model.compile()` again.
   */
  checkTrainableWeightsConsistency() {
    if (this.collectedTrainableWeights == null) {
      return;
    }
    if (this.trainableWeights.length !== this.collectedTrainableWeights.length) {
      console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?");
    }
  }
  /**
   * Returns the loss value & metrics values for the model in test mode.
   *
   * Loss and metrics are specified during `compile()`, which needs to happen
   * before calls to `evaluate()`.
   *
   * Computation is done in batches.
   *
   * ```js
   * const model = tf.sequential({
   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]
   * });
   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});
   * const result = model.evaluate(
   *     tf.ones([8, 10]), tf.ones([8, 1]), {batchSize: 4});
   * result.print();
   * ```
   *
   * @param x `tf.Tensor` of test data, or an `Array` of `tf.Tensor`s if the
   * model has multiple inputs.
   * @param y `tf.Tensor` of target data, or an `Array` of `tf.Tensor`s if the
   * model has multiple outputs.
   * @param args A `ModelEvaluateArgs`, containing optional fields.
   *
   * @return `Scalar` test loss (if the model has a single output and no
   *   metrics) or `Array` of `Scalar`s (if the model has multiple outputs
   *   and/or metrics). The attribute `model.metricsNames`
   *   will give you the display labels for the scalar outputs.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  evaluate(x, y, args = {}) {
    const batchSize = args.batchSize == null ? 32 : args.batchSize;
    checkBatchSize(batchSize);
    const checkBatchAxis = true;
    const standardizedOuts = this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);
    try {
      const ins = standardizedOuts[0].concat(standardizedOuts[1]);
      this.makeTestFunction();
      const f = this.testFunction;
      const testOuts = this.testLoop(f, ins, batchSize, args.verbose, args.steps);
      return singletonOrArray(testOuts);
    } finally {
      disposeNewTensors(standardizedOuts[0], x);
      disposeNewTensors(standardizedOuts[1], y);
    }
  }
  // TODO(cais): Add code snippet below once real dataset objects are
  //   available.
  /**
   * Evaluate model using a dataset object.
   *
   * Note: Unlike `evaluate()`, this method is asynchronous (`async`).
   *
   * @param dataset A dataset object. Its `iterator()` method is expected
   *   to generate a dataset iterator object, the `next()` method of which
   *   is expected to produce data batches for evaluation. The return value
   *   of the `next()` call ought to contain a boolean `done` field and a
   *   `value` field. The `value` field is expected to be an array of two
   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former
   *   case is for models with exactly one input and one output (e.g.
   *   a sequential model). The latter case is for models with multiple
   *   inputs and/or multiple outputs. Of the two items in the array, the
   *   first is the input feature(s) and the second is the output target(s).
   * @param args A configuration object for the dataset-based evaluation.
   * @returns Loss and metric values as an Array of `Scalar` objects.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  async evaluateDataset(dataset, args) {
    this.makeTestFunction();
    return evaluateDataset(this, dataset, args);
  }
  /**
   * Get number of samples provided for training, evaluation or prediction.
   *
   * @param ins Input `tf.Tensor`.
   * @param batchSize Integer batch size, optional.
   * @param steps Total number of steps (batches of samples) before
   * declaring loop finished. Optional.
   * @param stepsName The public API's parameter name for `steps`.
   * @returns Number of samples provided.
   */
  checkNumSamples(ins, batchSize, steps, stepsName = "steps") {
    let numSamples;
    if (steps != null) {
      numSamples = null;
      if (batchSize != null) {
        throw new ValueError(`If ${stepsName} is set, batchSize must be null or undefined.Got batchSize = ${batchSize}`);
      }
    } else if (ins != null) {
      if (Array.isArray(ins)) {
        numSamples = ins[0].shape[0];
      } else {
        numSamples = ins.shape[0];
      }
    } else {
      throw new ValueError(`Either the input data should have a defined shape, or ${stepsName} shoud be specified.`);
    }
    return numSamples;
  }
  /**
   * Execute internal tensors of the model with input data feed.
   * @param inputs Input data feed. Must match the inputs of the model.
   * @param outputs Names of the output tensors to be fetched. Must match
   *   names of the SymbolicTensors that belong to the graph.
   * @returns Fetched values for `outputs`.
   */
  execute(inputs, outputs) {
    if (Array.isArray(outputs) && outputs.length === 0) {
      throw new ValueError("`outputs` is an empty Array, which is not allowed.");
    }
    const outputsIsArray = Array.isArray(outputs);
    const outputNames = outputsIsArray ? outputs : [outputs];
    const outputSymbolicTensors = this.retrieveSymbolicTensors(outputNames);
    const feedDict = new FeedDict();
    if (inputs instanceof Tensor) {
      inputs = [inputs];
    }
    if (Array.isArray(inputs)) {
      if (inputs.length !== this.inputs.length) {
        throw new ValueError(`The number of inputs provided (${inputs.length}) does not match the number of inputs of this model (${this.inputs.length}).`);
      }
      for (let i = 0; i < this.inputs.length; ++i) {
        feedDict.add(this.inputs[i], inputs[i]);
      }
    } else {
      for (const input2 of this.inputs) {
        const tensorValue = inputs[input2.name];
        if (tensorValue == null) {
          throw new ValueError(`No value is provided for the model's input ${input2.name}`);
        }
        feedDict.add(input2, tensorValue);
      }
    }
    const executeOutputs = execute(outputSymbolicTensors, feedDict);
    return outputsIsArray ? executeOutputs : executeOutputs[0];
  }
  /**
   * Retrieve the model's internal symbolic tensors from symbolic-tensor names.
   */
  retrieveSymbolicTensors(symbolicTensorNames) {
    const outputSymbolicTensors = pyListRepeat(null, symbolicTensorNames.length);
    let outputsRemaining = symbolicTensorNames.length;
    for (const layer of this.layers) {
      const layerOutputs = Array.isArray(layer.output) ? layer.output : [layer.output];
      const layerOutputNames = layerOutputs.map((output) => output.name);
      for (let i = 0; i < symbolicTensorNames.length; ++i) {
        const index = layerOutputNames.indexOf(symbolicTensorNames[i]);
        if (index !== -1) {
          outputSymbolicTensors[i] = layerOutputs[index];
          outputsRemaining--;
        }
        if (outputsRemaining === 0) {
          break;
        }
      }
      if (outputsRemaining === 0) {
        break;
      }
    }
    if (outputsRemaining > 0) {
      const remainingNames = [];
      outputSymbolicTensors.forEach((tensor, i) => {
        if (tensor == null) {
          remainingNames.push(symbolicTensorNames[i]);
        }
      });
      throw new ValueError(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(remainingNames)}`);
    }
    return outputSymbolicTensors;
  }
  /**
   * Helper method to loop over some data in batches.
   *
   * Porting Note: Not using the functional approach in the Python equivalent
   *   due to the imperative backend.
   * Porting Note: Does not support step mode currently.
   *
   * @param ins: input data
   * @param batchSize: integer batch size.
   * @param verbose: verbosity model
   * @returns: Predictions as `tf.Tensor` (if a single output) or an `Array` of
   *   `tf.Tensor` (if multipe outputs).
   */
  predictLoop(ins, batchSize = 32, verbose = false) {
    return tidy(() => {
      const numSamples = this.checkNumSamples(ins);
      if (verbose) {
        throw new NotImplementedError("Verbose predictLoop() is not implemented yet.");
      }
      const batches = makeBatches(numSamples, batchSize);
      const outsBatches = this.outputs.map((output) => []);
      for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {
        const batchOuts = tidy(() => {
          const batchStart = batches[batchIndex][0];
          const batchEnd = batches[batchIndex][1];
          const insBatch = sliceArrays(ins, batchStart, batchEnd);
          const feeds = [];
          if (Array.isArray(insBatch)) {
            for (let i = 0; i < insBatch.length; ++i) {
              feeds.push({ key: this.inputs[i], value: insBatch[i] });
            }
          } else {
            feeds.push({ key: this.inputs[0], value: insBatch });
          }
          const feedDict = new FeedDict(feeds);
          return execute(this.outputs, feedDict);
        });
        batchOuts.forEach((batchOut, i) => outsBatches[i].push(batchOut));
      }
      return singletonOrArray(outsBatches.map((batches2) => concat(batches2, 0)));
    });
  }
  /**
   * Generates output predictions for the input samples.
   *
   * Computation is done in batches.
   *
   * Note: the "step" mode of predict() is currently not supported.
   *   This is because the TensorFlow.js core backend is imperative only.
   *
   * ```js
   * const model = tf.sequential({
   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]
   * });
   * model.predict(tf.ones([8, 10]), {batchSize: 4}).print();
   * ```
   *
   * @param x The input data, as a Tensor, or an `Array` of `tf.Tensor`s if
   *   the model has multiple inputs.
   * @param args A `ModelPredictArgs` object containing optional fields.
   *
   * @return Prediction results as a `tf.Tensor`(s).
   *
   * @exception ValueError In case of mismatch between the provided input data
   *   and the model's expectations, or in case a stateful model receives a
   *   number of samples that is not a multiple of the batch size.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  predict(x, args = {}) {
    const xsRank2OrHigher = ensureTensorsRank2OrHigher(x);
    checkInputData(xsRank2OrHigher, this.inputNames, this.feedInputShapes, false);
    try {
      const batchSize = args.batchSize == null ? 32 : args.batchSize;
      checkBatchSize(batchSize);
      return this.predictLoop(xsRank2OrHigher, batchSize);
    } finally {
      disposeNewTensors(xsRank2OrHigher, x);
    }
  }
  /**
   * Returns predictions for a single batch of samples.
   *
   * ```js
   * const model = tf.sequential({
   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]
   * });
   * model.predictOnBatch(tf.ones([8, 10])).print();
   * ```
   * @param x: Input samples, as a Tensor (for models with exactly one
   *   input) or an array of Tensors (for models with more than one input).
   * @return Tensor(s) of predictions
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  predictOnBatch(x) {
    checkInputData(x, this.inputNames, this.feedInputShapes, true);
    const batchSize = (Array.isArray(x) ? x[0] : x).shape[0];
    return this.predictLoop(x, batchSize);
  }
  standardizeUserDataXY(x, y, checkBatchAxis = true, batchSize) {
    if (this.optimizer_ == null) {
      throw new RuntimeError("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");
    }
    const outputShapes = [];
    for (let i = 0; i < this.feedOutputShapes.length; ++i) {
      const outputShape = this.feedOutputShapes[i];
      const lossFn = this.feedLossFns[i];
      if (lossFn === sparseCategoricalCrossentropy) {
        outputShapes.push(outputShape.slice(0, outputShape.length - 1).concat([1]));
      } else {
        outputShapes.push(outputShape);
      }
    }
    x = standardizeInputData(x, this.feedInputNames, this.feedInputShapes, false, "input");
    y = standardizeInputData(y, this.feedOutputNames, outputShapes, false, "target");
    checkArrayLengths(x, y, null);
    checkLossAndTargetCompatibility(y, this.feedLossFns, this.feedOutputShapes);
    if (this.stateful && batchSize != null && batchSize > 0) {
      if (x[0].shape[0] % batchSize !== 0) {
        throw new ValueError(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${batchSize}. Found: ${x[0].shape[0]} sample(s).`);
      }
    }
    return [x, y];
  }
  async standardizeUserData(x, y, sampleWeight, classWeight, checkBatchAxis = true, batchSize) {
    const [standardXs, standardYs] = this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);
    if (sampleWeight != null) {
      throw new Error("sample weight is not supported yet.");
    }
    let standardSampleWeights = null;
    if (classWeight != null) {
      const classWeights = standardizeClassWeights(classWeight, this.outputNames);
      standardSampleWeights = [];
      for (let i = 0; i < classWeights.length; ++i) {
        standardSampleWeights.push(await standardizeWeights(standardYs[i], null, classWeights[i]));
      }
    }
    return [standardXs, standardYs, standardSampleWeights];
  }
  /**
   * Loop over some test data in batches.
   * @param f A Function returning a list of tensors.
   * @param ins Array of tensors to be fed to `f`.
   * @param batchSize Integer batch size or `null` / `undefined`.
   * @param verbose verbosity mode.
   * @param steps Total number of steps (batches of samples) before
   * declaring test finished. Ignored with the default value of `null` /
   * `undefined`.
   * @returns Array of Scalars.
   */
  testLoop(f, ins, batchSize, verbose = 0, steps) {
    return tidy(() => {
      const numSamples = this.checkNumSamples(ins, batchSize, steps, "steps");
      const outs = [];
      if (verbose > 0) {
        throw new NotImplementedError("Verbose mode is not implemented yet.");
      }
      if (steps != null) {
        throw new NotImplementedError("steps mode in testLoop() is not implemented yet");
      } else {
        const batches = makeBatches(numSamples, batchSize);
        const indexArray = tensor1d(range(0, numSamples));
        for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {
          const batchStart = batches[batchIndex][0];
          const batchEnd = batches[batchIndex][1];
          const batchIds = sliceAlongFirstAxis(indexArray, batchStart, batchEnd - batchStart);
          const insBatch = sliceArraysByIndices(ins, batchIds);
          const batchOuts = f(insBatch);
          if (batchIndex === 0) {
            for (let i = 0; i < batchOuts.length; ++i) {
              outs.push(scalar(0));
            }
          }
          for (let i = 0; i < batchOuts.length; ++i) {
            const batchOut = batchOuts[i];
            outs[i] = add(outs[i], mul(batchEnd - batchStart, batchOut));
          }
        }
        for (let i = 0; i < outs.length; ++i) {
          outs[i] = div(outs[i], numSamples);
        }
      }
      return outs;
    });
  }
  getDedupedMetricsNames() {
    const outLabels = this.metricsNames;
    const dedupedOutLabels = [];
    for (let i = 0; i < outLabels.length; ++i) {
      const label = outLabels[i];
      let newLabel = label;
      if (count(outLabels, label) > 1) {
        const dupIndex = count(outLabels.slice(0, i), label);
        newLabel += `_${dupIndex}`;
      }
      dedupedOutLabels.push(newLabel);
    }
    return dedupedOutLabels;
  }
  /**
   * Creates a function that performs the following actions:
   *
   * 1. computes the losses
   * 2. sums them to get the total loss
   * 3. call the optimizer computes the gradients of the LayersModel's
   *    trainable weights w.r.t. the total loss and update the variables
   * 4. calculates the metrics
   * 5. returns the values of the losses and metrics.
   */
  makeTrainFunction() {
    return (data) => {
      const lossValues = [];
      const inputs = data.slice(0, this.inputs.length);
      const targets = data.slice(this.inputs.length, this.inputs.length + this.outputs.length);
      const sampleWeights = data.slice(this.inputs.length + this.outputs.length, this.inputs.length + this.outputs.length * 2);
      const metricsValues = [];
      const totalLossFunction = () => {
        const feeds = [];
        for (let i = 0; i < this.inputs.length; ++i) {
          feeds.push({ key: this.inputs[i], value: inputs[i] });
        }
        const feedDict = new FeedDict(feeds);
        const outputs = execute(this.outputs, feedDict, { "training": true });
        let totalLoss;
        for (let i = 0; i < this.lossFunctions.length; ++i) {
          const lossFunction = this.lossFunctions[i];
          let loss = lossFunction(targets[i], outputs[i]);
          if (sampleWeights[i] != null) {
            loss = computeWeightedLoss(loss, sampleWeights[i]);
          }
          const meanLoss = mean(loss);
          lossValues.push(meanLoss);
          if (i === 0) {
            totalLoss = loss;
          } else {
            totalLoss = add(totalLoss, loss);
          }
        }
        for (let i = 0; i < this.metricsTensors.length; ++i) {
          let weightedMetric;
          if (this.outputs.length > 1 && i < this.outputs.length) {
            weightedMetric = lossValues[i];
          } else {
            const metric = this.metricsTensors[i][0];
            const outputIndex = this.metricsTensors[i][1];
            weightedMetric = mean(metric(targets[outputIndex], outputs[outputIndex]));
          }
          keep(weightedMetric);
          metricsValues.push(weightedMetric);
        }
        totalLoss = mean(totalLoss);
        this.calculateLosses().forEach((regularizerLoss) => {
          totalLoss = add(totalLoss, regularizerLoss);
        });
        return totalLoss;
      };
      const variables = this.collectedTrainableWeights.map((param) => param.read());
      const returnCost = true;
      const totalLossValue = this.optimizer_.minimize(totalLossFunction, returnCost, variables);
      return [totalLossValue].concat(metricsValues);
    };
  }
  /**
   * Create a function which, when invoked with an array of `tf.Tensor`s as a
   * batch of inputs, returns the prespecified loss and metrics of the model
   * under the batch of input data.
   */
  makeTestFunction() {
    this.testFunction = (data) => {
      return tidy(() => {
        const valOutputs = [];
        let totalLoss;
        const inputs = data.slice(0, this.inputs.length);
        const targets = data.slice(this.inputs.length, this.inputs.length + this.outputs.length);
        const feeds = [];
        for (let i = 0; i < this.inputs.length; ++i) {
          feeds.push({ key: this.inputs[i], value: inputs[i] });
        }
        const feedDict = new FeedDict(feeds);
        const outputs = execute(this.outputs, feedDict);
        for (let i = 0; i < this.lossFunctions.length; ++i) {
          const lossFunction = this.lossFunctions[i];
          const loss = mean(lossFunction(targets[i], outputs[i]));
          if (i === 0) {
            totalLoss = loss;
          } else {
            totalLoss = add(totalLoss, loss);
          }
          valOutputs.push(totalLoss);
        }
        for (let i = 0; i < this.metricsTensors.length; ++i) {
          const metric = this.metricsTensors[i][0];
          const outputIndex = this.metricsTensors[i][1];
          const meanMetric = mean(metric(targets[outputIndex], outputs[outputIndex]));
          valOutputs.push(meanMetric);
        }
        return valOutputs;
      });
    };
  }
  /**
   * Trains the model for a fixed number of epochs (iterations on a
   * dataset).
   *
   * ```js
   * const model = tf.sequential({
   *     layers: [tf.layers.dense({units: 1, inputShape: [10]})]
   * });
   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});
   * for (let i = 1; i < 5 ; ++i) {
   *   const h = await model.fit(tf.ones([8, 10]), tf.ones([8, 1]), {
   *       batchSize: 4,
   *       epochs: 3
   *   });
   *   console.log("Loss after Epoch " + i + " : " + h.history.loss[0]);
   * }
   * ```
   *
   * @param x `tf.Tensor` of training data, or an array of `tf.Tensor`s if the
   * model has multiple inputs. If all inputs in the model are named, you
   * can also pass a dictionary mapping input names to `tf.Tensor`s.
   * @param y `tf.Tensor` of target (label) data, or an array of `tf.Tensor`s if
   * the model has multiple outputs. If all outputs in the model are named,
   * you can also pass a dictionary mapping output names to `tf.Tensor`s.
   * @param args A `ModelFitArgs`, containing optional fields.
   *
   * @return A `History` instance. Its `history` attribute contains all
   *   information collected during training.
   *
   * @exception ValueError In case of mismatch between the provided input
   * data and what the model expects.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  async fit(x, y, args = {}) {
    return fitTensors(this, x, y, args);
  }
  // TODO(cais): Add code snippet below when it's possible to instantiate
  //   actual dataset objects.
  /**
   * Trains the model using a dataset object.
   *
   * @param dataset A dataset object. Its `iterator()` method is expected
   *   to generate a dataset iterator object, the `next()` method of which
   *   is expected to produce data batches for training. The return value
   *   of the `next()` call ought to contain a boolean `done` field and a
   *   `value` field. The `value` field is expected to be an array of two
   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former
   *   case is for models with exactly one input and one output (e.g.
   *   a sequential model). The latter case is for models with multiple
   *   inputs and/or multiple outputs.
   *   Of the two items in the array, the first is the input feature(s) and
   *   the second is the output target(s).
   * @param args A `ModelFitDatasetArgs`, containing optional fields.
   *
   * @return A `History` instance. Its `history` attribute contains all
   *   information collected during training.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  async fitDataset(dataset, args) {
    return fitDataset(this, dataset, args);
  }
  /**
   * Runs a single gradient update on a single batch of data.
   *
   * This method differs from `fit()` and `fitDataset()` in the following
   * regards:
   *   - It operates on exactly one batch of data.
   *   - It returns only the loss and metric values, instead of
   *     returning the batch-by-batch loss and metric values.
   *   - It doesn't support fine-grained options such as verbosity and
   *     callbacks.
   *
   * @param x Input data. It could be one of the following:
   *   - A `tf.Tensor`, or an Array of `tf.Tensor`s (in case the model has
   *     multiple inputs).
   *   - An Object mapping input names to corresponding `tf.Tensor` (if the
   *     model has named inputs).
   * @param y Target data. It could be either a `tf.Tensor` or multiple
   *   `tf.Tensor`s. It should be consistent with `x`.
   * @returns Training loss or losses (in case the model has
   *   multiple outputs), along with metrics (if any), as numbers.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  async trainOnBatch(x, y) {
    const standardizeOut = await this.standardizeUserData(x, y);
    const inputs = standardizeOut[0];
    const targets = standardizeOut[1];
    const trainFunction = this.makeTrainFunction();
    const losses = trainFunction(inputs.concat(targets));
    const lossValues = [];
    for (const loss of losses) {
      const v = await loss.data();
      lossValues.push(v[0]);
    }
    dispose(losses);
    disposeNewTensors(standardizeOut[0], x);
    disposeNewTensors(standardizeOut[1], y);
    return singletonOrArray(lossValues);
  }
  /**
   * Extract weight values of the model.
   *
   * @param config: An instance of `io.SaveConfig`, which specifies
   * model-saving options such as whether only trainable weights are to be
   * saved.
   * @returns A `NamedTensorMap` mapping original weight names (i.e.,
   *   non-uniqueified weight names) to their values.
   */
  getNamedWeights(config) {
    const namedWeights = [];
    const trainableOnly = config != null && config.trainableOnly;
    const weights = trainableOnly ? this.trainableWeights : this.weights;
    const weightValues = this.getWeights(trainableOnly);
    for (let i = 0; i < weights.length; ++i) {
      if (trainableOnly && !weights[i].trainable) {
        continue;
      }
      namedWeights.push({ name: weights[i].originalName, tensor: weightValues[i] });
    }
    return namedWeights;
  }
  /**
   * Setter used for force stopping of LayersModel.fit() (i.e., training).
   *
   * Example:
   *
   * ```js
   * const input = tf.input({shape: [10]});
   * const output = tf.layers.dense({units: 1}).apply(input);
   * const model = tf.model({inputs: [input], outputs: [output]});
   * model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});
   * const xs = tf.ones([8, 10]);
   * const ys = tf.zeros([8, 1]);
   *
   * const history = await model.fit(xs, ys, {
   *   epochs: 10,
   *   callbacks: {
   *     onEpochEnd: async (epoch, logs) => {
   *       if (epoch === 2) {
   *         model.stopTraining = true;
   *       }
   *     }
   *   }
   * });
   *
   * // There should be only 3 values in the loss array, instead of 10
   * values,
   * // due to the stopping after 3 epochs.
   * console.log(history.history.loss);
   * ```
   */
  set stopTraining(stop) {
    this.stopTraining_ = stop;
  }
  get stopTraining() {
    return this.stopTraining_;
  }
  get optimizer() {
    return this.optimizer_;
  }
  set optimizer(optimizer) {
    if (this.optimizer_ !== optimizer) {
      this.optimizer_ = optimizer;
      this.isOptimizerOwned = false;
    }
  }
  dispose() {
    const result = super.dispose();
    if (result.refCountAfterDispose === 0 && this.optimizer != null && this.isOptimizerOwned) {
      const numTensorsBeforeOptmizerDisposal = memory().numTensors;
      this.optimizer_.dispose();
      result.numDisposedVariables += numTensorsBeforeOptmizerDisposal - memory().numTensors;
    }
    return result;
  }
  getLossIdentifiers() {
    let lossNames;
    if (typeof this.loss === "string") {
      lossNames = toSnakeCase(this.loss);
    } else if (Array.isArray(this.loss)) {
      for (const loss of this.loss) {
        if (typeof loss !== "string") {
          throw new Error("Serialization of non-string loss is not supported.");
        }
      }
      lossNames = this.loss.map((name) => toSnakeCase(name));
    } else {
      const outputNames = Object.keys(this.loss);
      lossNames = {};
      const losses = this.loss;
      for (const outputName of outputNames) {
        if (typeof losses[outputName] === "string") {
          lossNames[outputName] = toSnakeCase(losses[outputName]);
        } else {
          throw new Error("Serialization of non-string loss is not supported.");
        }
      }
    }
    return lossNames;
  }
  getMetricIdentifiers() {
    if (typeof this.metrics === "string" || typeof this.metrics === "function") {
      return [toSnakeCase(getLossOrMetricName(this.metrics))];
    } else if (Array.isArray(this.metrics)) {
      return this.metrics.map((metric) => toSnakeCase(getLossOrMetricName(metric)));
    } else {
      const metricsIdentifiers = {};
      for (const key in this.metrics) {
        metricsIdentifiers[key] = toSnakeCase(getLossOrMetricName(this.metrics[key]));
      }
      return metricsIdentifiers;
    }
  }
  getTrainingConfig() {
    return {
      loss: this.getLossIdentifiers(),
      metrics: this.getMetricIdentifiers(),
      optimizer_config: {
        class_name: this.optimizer.getClassName(),
        config: this.optimizer.getConfig()
      }
    };
  }
  loadTrainingConfig(trainingConfig) {
    if (trainingConfig.weighted_metrics != null) {
      throw new Error("Loading weight_metrics is not supported yet.");
    }
    if (trainingConfig.loss_weights != null) {
      throw new Error("Loading loss_weights is not supported yet.");
    }
    if (trainingConfig.sample_weight_mode != null) {
      throw new Error("Loading sample_weight_mode is not supported yet.");
    }
    const tsConfig = convertPythonicToTs(trainingConfig.optimizer_config);
    const optimizer = deserialize(tsConfig);
    let loss;
    if (typeof trainingConfig.loss === "string") {
      loss = toCamelCase(trainingConfig.loss);
    } else if (Array.isArray(trainingConfig.loss)) {
      loss = trainingConfig.loss.map((lossEntry) => toCamelCase(lossEntry));
    } else if (trainingConfig.loss != null) {
      loss = {};
      for (const key in trainingConfig.loss) {
        loss[key] = toCamelCase(trainingConfig.loss[key]);
      }
    }
    let metrics;
    if (Array.isArray(trainingConfig.metrics)) {
      metrics = trainingConfig.metrics.map((metric) => toCamelCase(metric));
    } else if (trainingConfig.metrics != null) {
      metrics = {};
      for (const key in trainingConfig.metrics) {
        metrics[key] = toCamelCase(trainingConfig.metrics[key]);
      }
    }
    this.compile({ loss, metrics, optimizer });
  }
  /**
   * Save the configuration and/or weights of the LayersModel.
   *
   * An `IOHandler` is an object that has a `save` method of the proper
   * signature defined. The `save` method manages the storing or
   * transmission of serialized data ("artifacts") that represent the
   * model's topology and weights onto or via a specific medium, such as
   * file downloads, local storage, IndexedDB in the web browser and HTTP
   * requests to a server. TensorFlow.js provides `IOHandler`
   * implementations for a number of frequently used saving mediums, such as
   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`
   * for more details.
   *
   * This method also allows you to refer to certain types of `IOHandler`s
   * as URL-like string shortcuts, such as 'localstorage://' and
   * 'indexeddb://'.
   *
   * Example 1: Save `model`'s topology and weights to browser [local
   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);
   * then load it back.
   *
   * ```js
   * const model = tf.sequential(
   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});
   * console.log('Prediction from original model:');
   * model.predict(tf.ones([1, 3])).print();
   *
   * const saveResults = await model.save('localstorage://my-model-1');
   *
   * const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');
   * console.log('Prediction from loaded model:');
   * loadedModel.predict(tf.ones([1, 3])).print();
   * ```
   *
   * Example 2. Saving `model`'s topology and weights to browser
   * [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API);
   * then load it back.
   *
   * ```js
   * const model = tf.sequential(
   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});
   * console.log('Prediction from original model:');
   * model.predict(tf.ones([1, 3])).print();
   *
   * const saveResults = await model.save('indexeddb://my-model-1');
   *
   * const loadedModel = await tf.loadLayersModel('indexeddb://my-model-1');
   * console.log('Prediction from loaded model:');
   * loadedModel.predict(tf.ones([1, 3])).print();
   * ```
   *
   * Example 3. Saving `model`'s topology and weights as two files
   * (`my-model-1.json` and `my-model-1.weights.bin`) downloaded from
   * browser.
   *
   * ```js
   * const model = tf.sequential(
   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});
   * const saveResults = await model.save('downloads://my-model-1');
   * ```
   *
   * Example 4. Send  `model`'s topology and weights to an HTTP server.
   * See the documentation of `tf.io.http` for more details
   * including specifying request parameters and implementation of the
   * server.
   *
   * ```js
   * const model = tf.sequential(
   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});
   * const saveResults = await model.save('http://my-server/model/upload');
   * ```
   *
   * @param handlerOrURL An instance of `IOHandler` or a URL-like,
   * scheme-based string shortcut for `IOHandler`.
   * @param config Options for saving the model.
   * @returns A `Promise` of `SaveResult`, which summarizes the result of
   * the saving, such as byte sizes of the saved artifacts for the model's
   *   topology and weight values.
   *
   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}
   */
  async save(handlerOrURL, config) {
    if (typeof handlerOrURL === "string") {
      const handlers = io_exports.getSaveHandlers(handlerOrURL);
      if (handlers.length === 0) {
        throw new ValueError(`Cannot find any save handlers for URL '${handlerOrURL}'`);
      } else if (handlers.length > 1) {
        throw new ValueError(`Found more than one (${handlers.length}) save handlers for URL '${handlerOrURL}'`);
      }
      handlerOrURL = handlers[0];
    }
    if (handlerOrURL.save == null) {
      throw new ValueError("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");
    }
    const weightDataAndSpecs = await io_exports.encodeWeights(this.getNamedWeights(config));
    const returnString = false;
    const unusedArg = null;
    const modelConfig = this.toJSON(unusedArg, returnString);
    const modelArtifacts = {
      modelTopology: modelConfig,
      format: LAYERS_MODEL_FORMAT_NAME,
      generatedBy: `TensorFlow.js tfjs-layers v${version}`,
      convertedBy: null
    };
    const includeOptimizer = config == null ? false : config.includeOptimizer;
    if (includeOptimizer && this.optimizer != null) {
      modelArtifacts.trainingConfig = this.getTrainingConfig();
      const weightType = "optimizer";
      const { data: optimizerWeightData, specs: optimizerWeightSpecs } = await io_exports.encodeWeights(await this.optimizer.getWeights(), weightType);
      weightDataAndSpecs.specs.push(...optimizerWeightSpecs);
      weightDataAndSpecs.data = io_exports.concatenateArrayBuffers([weightDataAndSpecs.data, optimizerWeightData]);
    }
    if (this.userDefinedMetadata != null) {
      const checkSize = true;
      checkUserDefinedMetadata(this.userDefinedMetadata, this.name, checkSize);
      modelArtifacts.userDefinedMetadata = this.userDefinedMetadata;
    }
    modelArtifacts.weightData = weightDataAndSpecs.data;
    modelArtifacts.weightSpecs = weightDataAndSpecs.specs;
    return handlerOrURL.save(modelArtifacts);
  }
  /**
   * Set user-defined metadata.
   *
   * The set metadata will be serialized together with the topology
   * and weights of the model during `save()` calls.
   *
   * @param setUserDefinedMetadata
   */
  setUserDefinedMetadata(userDefinedMetadata) {
    checkUserDefinedMetadata(userDefinedMetadata, this.name);
    this.userDefinedMetadata = userDefinedMetadata;
  }
  /**
   * Get user-defined metadata.
   *
   * The metadata is supplied via one of the two routes:
   *   1. By calling `setUserDefinedMetadata()`.
   *   2. Loaded during model loading (if the model is constructed
   *      via `tf.loadLayersModel()`.)
   *
   * If no user-defined metadata is available from either of the
   * two routes, this function will return `undefined`.
   */
  getUserDefinedMetadata() {
    return this.userDefinedMetadata;
  }
};
LayersModel.className = "Model";
serialization_exports.registerClass(LayersModel);
var Functional = class extends LayersModel {
};
Functional.className = "Functional";
serialization_exports.registerClass(Functional);

// node_modules/@tensorflow/tfjs-layers/dist/models.js
async function modelFromJSON(modelAndWeightsConfig, customObjects) {
  if (!("modelTopology" in modelAndWeightsConfig)) {
    modelAndWeightsConfig = { modelTopology: modelAndWeightsConfig };
  }
  modelAndWeightsConfig = modelAndWeightsConfig;
  let modelTopology = modelAndWeightsConfig.modelTopology;
  if (modelTopology["model_config"] != null) {
    modelTopology = modelTopology["model_config"];
  }
  const tsConfig = convertPythonicToTs(modelTopology);
  const model2 = deserialize(tsConfig, customObjects);
  if (modelAndWeightsConfig.weightsManifest != null) {
    const weightValues = await io_exports.loadWeights(modelAndWeightsConfig.weightsManifest, modelAndWeightsConfig.pathPrefix, model2.weights.map((weight) => weight.originalName));
    const uniqueWeightValues = {};
    for (const weight of model2.weights) {
      uniqueWeightValues[weight.originalName] = weightValues[weight.originalName];
    }
    model2.loadWeights(uniqueWeightValues);
    dispose(weightValues);
  }
  return model2;
}
async function loadLayersModelInternal(pathOrIOHandler, options) {
  if (options == null) {
    options = {};
  }
  if (typeof pathOrIOHandler === "string") {
    const handlers = io_exports.getLoadHandlers(pathOrIOHandler, options);
    if (handlers.length === 0) {
      handlers.push(io_exports.browserHTTPRequest(pathOrIOHandler, options));
    } else if (handlers.length > 1) {
      throw new ValueError(`Found more than one (${handlers.length}) load handlers for URL '${pathOrIOHandler}'`);
    }
    pathOrIOHandler = handlers[0];
  }
  return loadLayersModelFromIOHandler(pathOrIOHandler, void 0, options);
}
async function loadLayersModelFromIOHandler(handler, customObjects, options) {
  if (options == null) {
    options = {};
  }
  if (handler.load == null) {
    throw new ValueError("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");
  }
  const artifacts = await handler.load();
  let modelTopology = artifacts.modelTopology;
  if (modelTopology["model_config"] != null) {
    modelTopology = modelTopology["model_config"];
  }
  const strict = options.strict == null ? true : options.strict;
  const fastWeightInit = artifacts.weightData != null && artifacts.weightSpecs != null && strict;
  const model2 = deserialize(convertPythonicToTs(modelTopology), customObjects, fastWeightInit);
  const trainingConfig = artifacts.trainingConfig;
  if (trainingConfig != null) {
    model2.loadTrainingConfig(trainingConfig);
  }
  if (artifacts.userDefinedMetadata != null) {
    model2.setUserDefinedMetadata(artifacts.userDefinedMetadata);
  }
  if (artifacts.weightData != null) {
    if (artifacts.weightSpecs == null) {
      throw new ValueError("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");
    }
    const { modelWeights, optimizerWeights } = decodeModelAndOptimizerWeights(artifacts.weightData, artifacts.weightSpecs);
    model2.loadWeights(modelWeights, strict);
    if (model2.optimizer != null && optimizerWeights.length > 0) {
      await model2.optimizer.setWeights(optimizerWeights);
    }
    dispose(modelWeights);
    dispose(optimizerWeights.map((w) => w.tensor));
  }
  return model2;
}
function decodeModelAndOptimizerWeights(buffer, specs) {
  const name2Tensor = io_exports.decodeWeights(buffer, specs);
  const modelWeights = {};
  const optimizerWeights = [];
  specs.forEach((spec) => {
    if (spec.group === "optimizer") {
      optimizerWeights.push({ name: spec.name, tensor: name2Tensor[spec.name] });
    } else {
      modelWeights[spec.name] = name2Tensor[spec.name];
    }
  });
  return { modelWeights, optimizerWeights };
}
var Sequential = class _Sequential extends LayersModel {
  constructor(args) {
    super({ inputs: [], outputs: [] });
    args = args || {};
    this.trainable = true;
    this.built = false;
    this.name = args.name != null ? args.name : getUid("sequential_");
    if (args.layers != null) {
      for (const layer of args.layers) {
        this.add(layer);
      }
    }
  }
  // Helper function to Sequential.add  Throws if the new output shape will be
  // invalid.
  checkShape(layer) {
    const shape = layer.inboundNodes[0].outputTensors[0].shape;
    if (shape.some((x) => x < 0)) {
      throw new ValueError(`Negative dimension size caused by adding layer ${layer.name} with input shape [${layer.inboundNodes[0].inputTensors[0].shape}]`);
    }
  }
  /**
   * Adds a layer instance on top of the layer stack.
   *
   * ```js
   *  const model = tf.sequential();
   *  model.add(tf.layers.dense({units: 8, inputShape: [1]}));
   *  model.add(tf.layers.dense({units: 4, activation: 'relu6'}));
   *  model.add(tf.layers.dense({units: 1, activation: 'relu6'}));
   *  // Note that the untrained model is random at this point.
   *  model.predict(tf.randomNormal([10, 1])).print();
   * ```
   * @param layer Layer instance.
   *
   * @exception ValueError In case the `layer` argument does not know its
   * input shape.
   * @exception ValueError In case the `layer` argument has multiple output
   *   tensors, or is already connected somewhere else (forbidden in
   *   `Sequential` models).
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  add(layer) {
    const isLayerModelInstance = layer instanceof _Sequential || layer instanceof LayersModel;
    let modelLayer;
    if (isLayerModelInstance) {
      modelLayer = layer;
      if (modelLayer.outputs.length !== 1) {
        throw new ValueError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");
      }
      if (modelLayer.inputs.length !== 1) {
        throw new ValueError("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.");
      }
    }
    if (this.outputs.length === 0) {
      if (layer.inboundNodes.length === 0) {
        if (layer.batchInputShape == null) {
          throw new ValueError("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");
        }
        const x = Input({
          batchShape: layer.batchInputShape,
          dtype: layer.dtype,
          name: layer.name + "_input"
        });
        layer.apply(x);
      }
      if (isLayerModelInstance) {
        this.outputs = modelLayer.outputs;
        this.inputs = modelLayer.inputs;
      } else {
        if (layer.inboundNodes.length !== 1) {
          throw new ValueError(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${layer.name} which has ${layer.inboundNodes.length} pre-existing inbound connections.`);
        }
        if (layer.inboundNodes[0].outputTensors.length !== 1) {
          throw new ValueError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");
        }
        this.checkShape(layer);
        this.outputs = [layer.inboundNodes[0].outputTensors[0]];
        this.inputs = getSourceInputs(this.outputs[0]);
      }
      this.inboundNodes = [];
      new Node({
        outboundLayer: this,
        inboundLayers: [],
        nodeIndices: [],
        tensorIndices: [],
        inputTensors: this.inputs,
        outputTensors: this.outputs,
        // no model-level masking for now
        inputMasks: pyListRepeat(null, this.inputs.length),
        outputMasks: [null],
        inputShapes: this.inputs.map((x) => x.shape),
        outputShapes: this.outputs[0].shape
      });
    } else {
      const outputTensor = layer.apply(this.outputs[0]);
      if (Array.isArray(outputTensor)) {
        throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");
      }
      this.checkShape(layer);
      this.outputs = [outputTensor];
      this.inboundNodes[0].outputTensors = this.outputs;
      this.inboundNodes[0].outputShapes = [this.outputs[0].shape];
    }
    this.layers.push(layer);
    this.built = false;
  }
  /**
   * Removes the last layer in the model.
   *
   * @exception TypeError if there are no layers in the model.
   */
  pop() {
    if (this.layers.length === 0) {
      throw new TypeError("There are no layers in the model.");
    }
    this.layers.pop();
    if (this.layers.length === 0) {
      this.outputs = [];
      this.inboundNodes = [];
      this.outboundNodes = [];
    } else {
      const lastLayerIndex = this.layers.length - 1;
      this.layers[lastLayerIndex].outboundNodes = [];
      this.outputs = [this.layers[lastLayerIndex].output];
      this.inboundNodes[0].outputTensors = this.outputs;
      this.inboundNodes[0].outputShapes = [this.outputs[0].shape];
    }
  }
  call(inputs, kwargs) {
    if (this.model == null) {
      this.build();
    }
    return this.model.call(inputs, kwargs);
  }
  build(inputShape) {
    getExactlyOneShape(inputShape);
    if (this.inputs.length === 0 || this.outputs.length === 0) {
      throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");
    }
    this.model = new LayersModel({
      inputs: this.inputs,
      outputs: this.outputs[0],
      name: this.name + "_model"
    });
    this.model.trainable = this.trainable;
    this.supportsMasking = this.model.supportsMasking;
    this.inputLayers = this.model.inputLayers;
    this.inputLayersNodeIndices = this.model.inputLayersNodeIndices;
    this.inputLayersTensorIndices = this.model.inputLayersTensorIndices;
    this.outputLayers = this.model.outputLayers;
    this.outputLayersNodeIndices = this.model.outputLayersNodeIndices;
    this.outputLayersTensorIndices = this.model.outputLayersTensorIndices;
    this.nodesByDepth = this.model.nodesByDepth;
    this.containerNodes = this.model.containerNodes;
    this.outputNames = this.model.outputNames;
    this.inputNames = this.model.inputNames;
    this.built = true;
  }
  countParams() {
    if (!this.built) {
      this.build();
    }
    return super.countParams();
  }
  /**
   * Print a text summary of the Sequential model's layers.
   *
   * The summary includes
   * - Name and type of all layers that comprise the model.
   * - Output shape(s) of the layers
   * - Number of weight parameters of each layer
   * - The total number of trainable and non-trainable parameters of the
   * model.
   *
   * ```js
   * const model = tf.sequential();
   * model.add(
   *     tf.layers.dense({units: 100, inputShape: [10], activation: 'relu'}));
   * model.add(tf.layers.dense({units: 1, activation: 'sigmoid'}));
   *
   * model.summary();
   * ```
   *
   * @param lineLength Custom line length, in number of characters.
   * @param positions Custom widths of each of the columns, as either
   *   fractions of `lineLength` (e.g., `[0.5, 0.75, 1]`) or absolute number
   *   of characters (e.g., `[30, 50, 65]`). Each number corresponds to
   *   right-most (i.e., ending) position of a column.
   * @param printFn Custom print function. Can be used to replace the default
   *   `console.log`. For example, you can use `x => {}` to mute the printed
   *   messages in the console.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  summary(lineLength, positions, printFn = console.log) {
    if (!this.built) {
      this.build();
    }
    super.summary(lineLength, positions, printFn);
  }
  /**
   * Sets the weights of the model.
   *
   * @param weights Should be a list of Tensors with shapes and types matching
   *   the output of `model.getWeights()`.
   */
  setWeights(weights) {
    if (this.model == null) {
      this.build();
    }
    this.model.setWeights(weights);
  }
  /**
   * Returns the loss value & metrics values for the model in test mode.
   *
   * Loss and metrics are specified during `compile()`, which needs to happen
   * before calls to `evaluate()`.
   *
   * Computation is done in batches.
   *
   * ```js
   * const model = tf.sequential({
   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]
   * });
   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});
   * const result = model.evaluate(tf.ones([8, 10]), tf.ones([8, 1]), {
   *   batchSize: 4,
   * });
   * result.print();
   * ```
   *
   * @param x `tf.Tensor` of test data, or an `Array` of `tf.Tensor`s if the
   * model has multiple inputs.
   * @param y `tf.Tensor` of target data, or an `Array` of `tf.Tensor`s if the
   * model has multiple outputs.
   * @param args A `ModelEvaluateConfig`, containing optional fields.
   *
   * @return `Scalar` test loss (if the model has a single output and no
   *   metrics) or `Array` of `Scalar`s (if the model has multiple outputs
   *   and/or metrics). The attribute `model.metricsNames`
   *   will give you the display labels for the scalar outputs.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  evaluate(x, y, args = {}) {
    if (!this.built) {
      throw new RuntimeError("The model needs to be compiled before being used.");
    }
    return this.model.evaluate(x, y, args);
  }
  // TODO(cais): Add code snippet below once real dataset objects are
  //   available.
  /**
   * Evaluate model using a dataset object.
   *
   * Note: Unlike `evaluate()`, this method is asynchronous (`async`).
   *
   * @param dataset A dataset object. Its `iterator()` method is expected
   *   to generate a dataset iterator object, the `next()` method of which
   *   is expected to produce data batches for evaluation. The return value
   *   of the `next()` call ought to contain a boolean `done` field and a
   *   `value` field. The `value` field is expected to be an array of two
   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former
   *   case is for models with exactly one input and one output (e.g.
   *   a sequential model). The latter case is for models with multiple
   *   inputs and/or multiple outputs. Of the two items in the array, the
   *   first is the input feature(s) and the second is the output target(s).
   * @param args A configuration object for the dataset-based evaluation.
   * @returns Loss and metric values as an Array of `Scalar` objects.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  async evaluateDataset(dataset, args) {
    if (!this.built) {
      throw new RuntimeError("The model needs to be compiled before being used.");
    }
    return this.model.evaluateDataset(dataset, args);
  }
  /**
   * Generates output predictions for the input samples.
   *
   * Computation is done in batches.
   *
   * Note: the "step" mode of predict() is currently not supported.
   *   This is because the TensorFlow.js core backend is imperative only.
   *
   * ```js
   * const model = tf.sequential({
   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]
   * });
   * model.predict(tf.ones([2, 10])).print();
   * ```
   *
   * @param x The input data, as a Tensor, or an `Array` of `tf.Tensor`s if
   *   the model has multiple inputs.
   * @param conifg A `ModelPredictConfig` object containing optional fields.
   *
   * @return `tf.Tensor`(s) of predictions.
   *
   * @exception ValueError In case of mismatch between the provided input data
   *   and the model's expectations, or in case a stateful model receives a
   *   number of samples that is not a multiple of the batch size.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  predict(x, args = {}) {
    if (this.model == null) {
      this.build();
    }
    return this.model.predict(x, args);
  }
  /**
   * Returns predictions for a single batch of samples.
   *
   * @param x: Input samples, as a Tensor, or list of Tensors (if the model
   *   has multiple inputs).
   * @return Tensor(s) of predictions
   */
  predictOnBatch(x) {
    if (this.model == null) {
      this.build();
    }
    return this.model.predictOnBatch(x);
  }
  /**
   * See `LayersModel.compile`.
   *
   * @param args
   */
  compile(args) {
    this.build();
    this.model.compile(args);
    this.optimizer_ = this.model.optimizer;
    this.isOptimizerOwned = this.model.isOptimizerOwned;
    this.loss = this.model.loss;
    this.metrics = this.model.metrics;
    this.metricsTensors = this.model.metricsTensors;
    this.metricsNames = this.model.metricsNames;
  }
  get optimizer() {
    return this.model == null ? void 0 : this.model.optimizer;
  }
  set optimizer(optimizer) {
    this.model.optimizer = optimizer;
  }
  /**
   * Trains the model for a fixed number of epochs (iterations on a dataset).
   *
   * ```js
   * const model = tf.sequential({
   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]
   * });
   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});
   * const history = await model.fit(tf.ones([8, 10]), tf.ones([8, 1]), {
   *   batchSize: 4,
   *   epochs: 3
   * });
   * console.log(history.history.loss[0]);
   * ```
   *
   * @param x `tf.Tensor` of training data, or an array of `tf.Tensor`s if the
   * model has multiple inputs. If all inputs in the model are named, you can
   * also pass a dictionary mapping input names to `tf.Tensor`s.
   * @param y `tf.Tensor` of target (label) data, or an array of `tf.Tensor`s if
   * the model has multiple outputs. If all outputs in the model are named, you
   *  can also pass a dictionary mapping output names to `tf.Tensor`s.
   * @param args  A `ModelFitConfig`, containing optional fields.
   *
   * @return A `History` instance. Its `history` attribute contains all
   *   information collected during training.
   *
   * @exception ValueError In case of mismatch between the provided input data
   *   and what the model expects.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  async fit(x, y, args = {}) {
    if (!this.built) {
      throw new RuntimeError("The model needs to be compiled before being used.");
    }
    return this.model.fit(x, y, args);
  }
  /**
   * Trains the model using a dataset object.
   *
   * ```js
   * const xArray = [
   *   [1, 1, 1, 1, 1, 1, 1, 1, 1],
   *   [1, 1, 1, 1, 1, 1, 1, 1, 1],
   *   [1, 1, 1, 1, 1, 1, 1, 1, 1],
   *   [1, 1, 1, 1, 1, 1, 1, 1, 1],
   * ];
   * const yArray = [1, 1, 1, 1];
   * // Create a dataset from the JavaScript array.
   * const xDataset = tf.data.array(xArray);
   * const yDataset = tf.data.array(yArray);
   * // Zip combines the `x` and `y` Datasets into a single Dataset, the
   * // iterator of which will return an object containing of two tensors,
   * // corresponding to `x` and `y`.  The call to `batch(4)` will bundle
   * // four such samples into a single object, with the same keys now pointing
   * // to tensors that hold 4 examples, organized along the batch dimension.
   * // The call to `shuffle(4)` causes each iteration through the dataset to
   * // happen in a different order.  The size of the shuffle window is 4.
   * const xyDataset = tf.data.zip({xs: xDataset, ys: yDataset})
   *     .batch(4)
   *     .shuffle(4);
   * const model = tf.sequential({
   *   layers: [tf.layers.dense({units: 1, inputShape: [9]})]
   * });
   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});
   * const history = await model.fitDataset(xyDataset, {
   *   epochs: 4,
   *   callbacks: {onEpochEnd: (epoch, logs) => console.log(logs.loss)}
   * });
   * ```
   *
   * @param dataset A dataset object. Its `iterator()` method is expected to
   *   generate a dataset iterator object, the `next()` method of which is
   *   expected to produce data batches for evaluation. The return value of the
   *   `next()` call ought to contain a boolean `done` field and a `value`
   *   field.
   *
   *   The `value` field is expected to be an object of with fields
   *   `xs` and `ys`, which point to the feature tensor and the target tensor,
   *   respectively. This case is for models with exactly one input and one
   *   output (e.g. a sequential model). For example:
   *   ```js
   *   {value: {xs: xsTensor, ys: ysTensor}, done: false}
   *   ```
   *
   *   If the model has multiple inputs, the `xs` field of `value` should
   *   be an object mapping input names to their respective feature tensors.
   *   For example:
   *   ```js
   *   {
   *     value: {
   *       xs: {
   *         input_1: xsTensor1,
   *         input_2: xsTensor2
   *       },
   *       ys: ysTensor
   *     },
   *     done: false
   *   }
   *   ```
   *   If the model has multiple outputs, the `ys` field of `value` should
   *   be an object mapping output names to their respective target tensors.
   *   For example:
   *   ```js
   *   {
   *     value: {
   *       xs: xsTensor,
   *       ys: {
   *         output_1: ysTensor1,
   *         output_2: ysTensor2
   *       },
   *     },
   *     done: false
   *   }
   *   ```
   * @param args A `ModelFitDatasetArgs`, containing optional fields.
   *
   * @return A `History` instance. Its `history` attribute contains all
   *   information collected during training.
   *
   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}
   */
  async fitDataset(dataset, args) {
    if (!this.built) {
      throw new RuntimeError("The model needs to be compiled before being used.");
    }
    return this.model.fitDataset(dataset, args);
  }
  /**
   * Runs a single gradient update on a single batch of data.
   *
   * This method differs from `fit()` and `fitDataset()` in the following
   * regards:
   *   - It operates on exactly one batch of data.
   *   - It returns only the loss and metric values, instead of
   *     returning the batch-by-batch loss and metric values.
   *   - It doesn't support fine-grained options such as verbosity and
   *     callbacks.
   *
   * @param x Input data. It could be one of the following:
   *   - A `tf.Tensor`, or an Array of `tf.Tensor`s (in case the model has
   *     multiple inputs).
   *   - An Object mapping input names to corresponding `tf.Tensor` (if the
   *     model has named inputs).
   * @param y Target data. It could be either a `tf.Tensor` or multiple
   *   `tf.Tensor`s. It should be consistent with `x`.
   * @returns Training loss or losses (in case the model has
   *   multiple outputs), along with metrics (if any), as numbers.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  async trainOnBatch(x, y) {
    return this.model.trainOnBatch(x, y);
  }
  /* See parent class for JsDoc */
  /** @nocollapse */
  static fromConfig(cls, config, customObjects = {}, fastWeightInit = false) {
    let configArray;
    let extraModelConfig = {};
    if (config instanceof Array) {
      if (!(config[0].className != null) || config[0]["className"] === "Merge") {
        throw new ValueError("Legacy serialization format not supported yet.");
      }
      configArray = config;
    } else {
      util_exports.assert(config["layers"] != null, () => `When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.`);
      configArray = config["layers"];
      delete config["layers"];
      extraModelConfig = config;
    }
    const model2 = new cls(extraModelConfig);
    if (!(model2 instanceof _Sequential)) {
      throw new NotImplementedError(`Sequential.fromConfig called on non-Sequential input: ${model2}`);
    }
    for (const conf of configArray) {
      const customObjects2 = void 0;
      const layer = deserialize(conf, customObjects2, fastWeightInit);
      if (fastWeightInit) {
        layer.setFastWeightInitDuringBuild(true);
      }
      model2.add(layer);
    }
    return model2;
  }
  /**
   * Setter used for force stopping of LayersModel.fit() (i.e., training).
   *
   * Example:
   *
   * ```js
   * const model = tf.sequential();
   * model.add(tf.layers.dense({units: 1, inputShape: [10]}));
   * model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});
   * const xs = tf.ones([8, 10]);
   * const ys = tf.zeros([8, 1]);
   *
   * const history = await model.fit(xs, ys, {
   *   epochs: 10,
   *   callbacks: {
   *     onEpochEnd: async (epoch, logs) => {
   *       if (epoch === 2) {
   *         model.stopTraining = true;
   *       }
   *     }
   *   }
   * });
   *
   * // There should be only 3 values in the loss array, instead of 10 values,
   * // due to the stopping after 3 epochs.
   * console.log(history.history.loss);
   * ```
   */
  set stopTraining(stop) {
    if (this.model == null) {
      throw new ValueError("Cannot set the stopTraining property of a sequential model before it is compiled.");
    }
    this.model.stopTraining = stop;
  }
  get stopTraining() {
    if (this.model == null) {
      throw new ValueError("Cannot get the stopTraining property of a sequential model before it is compiled.");
    }
    return this.model.stopTraining;
  }
  // TODO(cais): Override get trainableWeights() here
  // tslint:disable-next-line:no-any
  getConfig() {
    const layers = [];
    for (const layer of this.layers) {
      const dict = {};
      dict["className"] = layer.getClassName();
      dict["config"] = layer.getConfig();
      layers.push(dict);
    }
    return { name: this.name, layers };
  }
};
Sequential.className = "Sequential";
serialization_exports.registerClass(Sequential);

// node_modules/@tensorflow/tfjs-layers/dist/exports.js
function model(args) {
  return new LayersModel(args);
}
function sequential(config) {
  return new Sequential(config);
}
function loadLayersModel(pathOrIOHandler, options) {
  if (options == null) {
    options = {};
  }
  return loadLayersModelInternal(pathOrIOHandler, options);
}
function input(config) {
  return Input(config);
}
function registerCallbackConstructor(verbosityLevel, callbackConstructor) {
  CallbackConstructorRegistry.registerCallbackConstructor(verbosityLevel, callbackConstructor);
}

// node_modules/@tensorflow/tfjs-layers/dist/activations.js
var Activation = class extends serialization_exports.Serializable {
  getConfig() {
    return {};
  }
};
var Elu2 = class extends Activation {
  /**
   * Calculate the activation function.
   *
   * @param x: Input.
   * @param alpha: Scaling factor the negative section.
   * @return Output of the ELU activation.
   */
  apply(x, alpha = 1) {
    return elu2(x, alpha);
  }
};
Elu2.className = "elu";
serialization_exports.registerClass(Elu2);
var Selu2 = class extends Activation {
  apply(x) {
    return selu(x);
  }
};
Selu2.className = "selu";
serialization_exports.registerClass(Selu2);
var Relu2 = class extends Activation {
  apply(x) {
    return relu(x);
  }
};
Relu2.className = "relu";
serialization_exports.registerClass(Relu2);
var Relu62 = class extends Activation {
  apply(x) {
    return tidy(() => minimum(6, relu(x)));
  }
};
Relu62.className = "relu6";
serialization_exports.registerClass(Relu62);
var Linear = class extends Activation {
  apply(x) {
    return x;
  }
};
Linear.className = "linear";
serialization_exports.registerClass(Linear);
var Sigmoid2 = class extends Activation {
  apply(x) {
    return sigmoid(x);
  }
};
Sigmoid2.className = "sigmoid";
serialization_exports.registerClass(Sigmoid2);
var HardSigmoid = class extends Activation {
  apply(x) {
    return hardSigmoid(x);
  }
};
HardSigmoid.className = "hardSigmoid";
serialization_exports.registerClass(HardSigmoid);
var Softplus2 = class extends Activation {
  apply(x) {
    return softplus(x);
  }
};
Softplus2.className = "softplus";
serialization_exports.registerClass(Softplus2);
var Softsign = class extends Activation {
  apply(x) {
    return softsign(x);
  }
};
Softsign.className = "softsign";
serialization_exports.registerClass(Softsign);
var Tanh2 = class extends Activation {
  apply(x) {
    return tanh(x);
  }
};
Tanh2.className = "tanh";
serialization_exports.registerClass(Tanh2);
var Softmax2 = class extends Activation {
  /**
   * Calculate the activation function.
   *
   * @param x Tensor.
   * @param axis Integer, axis along which the softmax normalization is applied.
   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be
   * an error.
   *
   * @returns a Tensor of the same shape as x
   *
   * @throws ValueError: In case `dim(x) < 2`.
   */
  apply(x, axis = -1) {
    return softmax(x, axis);
  }
};
Softmax2.className = "softmax";
serialization_exports.registerClass(Softmax2);
var LogSoftmax2 = class extends Activation {
  /**
   * Calculate the activation function of log softmax:
   * log( exp(x_i) / sum(exp(x)) )
   *
   * @param x Tensor.
   * @param axis Integer, axis along which the softmax normalization is applied.
   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be
   * an error.
   *
   * @returns a Tensor of the same shape as x
   *
   * @throws ValueError: In case `dim(x) < 2`.
   */
  apply(x, axis = -1) {
    return logSoftmax(x, axis);
  }
};
LogSoftmax2.className = "logSoftmax";
serialization_exports.registerClass(LogSoftmax2);
var Swish = class extends Activation {
  /**
   * Calculate the activation function.
   *
   * @param x Tensor.
   * @param alpha Scaling factor for the sigmoid function.
   * @returns a Tensor of the same shape as x
   */
  apply(x, alpha = 1) {
    return tidy(() => mul(sigmoid(mul(x, alpha)), x));
  }
};
Swish.className = "swish";
serialization_exports.registerClass(Swish);
var Mish = class extends Activation {
  /**
   * Calculate the activation function.
   *
   * @param x Tensor.
   * @returns a Tensor of the same shape as x
   */
  apply(x) {
    return tidy(() => mul(x, tanh(softplus(x))));
  }
};
Mish.className = "mish";
serialization_exports.registerClass(Mish);
function serializeActivation(activation2) {
  return activation2.getClassName();
}
function deserializeActivation(config, customObjects = {}) {
  return deserializeKerasObject(config, serialization_exports.SerializationMap.getMap().classNameMap, customObjects, "activation");
}
function getActivation(identifier) {
  if (identifier == null) {
    const config = {};
    config["className"] = "linear";
    config["config"] = {};
    return deserializeActivation(config);
  }
  if (typeof identifier === "string") {
    const config = {};
    config["className"] = identifier;
    config["config"] = {};
    return deserializeActivation(config);
  } else if (identifier instanceof Activation) {
    return identifier;
  } else {
    return deserializeActivation(identifier);
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/regularizers.js
function assertObjectArgs(args) {
  if (args != null && typeof args !== "object") {
    throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${args}`);
  }
}
var Regularizer = class extends serialization_exports.Serializable {
};
var L1L2 = class extends Regularizer {
  constructor(args) {
    super();
    assertObjectArgs(args);
    this.l1 = args == null || args.l1 == null ? 0.01 : args.l1;
    this.l2 = args == null || args.l2 == null ? 0.01 : args.l2;
    this.hasL1 = this.l1 !== 0;
    this.hasL2 = this.l2 !== 0;
  }
  /**
   * Porting note: Renamed from __call__.
   * @param x Variable of which to calculate the regularization score.
   */
  apply(x) {
    return tidy(() => {
      let regularization = zeros([1]);
      if (this.hasL1) {
        regularization = add(regularization, sum(mul(this.l1, abs(x))));
      }
      if (this.hasL2) {
        regularization = add(regularization, sum(mul(this.l2, square2(x))));
      }
      return reshape(regularization, []);
    });
  }
  getConfig() {
    return { "l1": this.l1, "l2": this.l2 };
  }
  /** @nocollapse */
  static fromConfig(cls, config) {
    return new cls({ l1: config["l1"], l2: config["l2"] });
  }
};
L1L2.className = "L1L2";
serialization_exports.registerClass(L1L2);
function l1(args) {
  assertObjectArgs(args);
  return new L1L2({ l1: args != null ? args.l1 : null, l2: 0 });
}
function l2(args) {
  assertObjectArgs(args);
  return new L1L2({ l2: args != null ? args.l2 : null, l1: 0 });
}
var REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP = {
  "l1l2": "L1L2"
};
function serializeRegularizer(constraint) {
  return serializeKerasObject(constraint);
}
function deserializeRegularizer(config, customObjects = {}) {
  return deserializeKerasObject(config, serialization_exports.SerializationMap.getMap().classNameMap, customObjects, "regularizer");
}
function getRegularizer(identifier) {
  if (identifier == null) {
    return null;
  }
  if (typeof identifier === "string") {
    const className = identifier in REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ? REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[identifier] : identifier;
    const config = { className, config: {} };
    return deserializeRegularizer(config);
  } else if (identifier instanceof Regularizer) {
    return identifier;
  } else {
    return deserializeRegularizer(identifier);
  }
}

// node_modules/@tensorflow/tfjs-layers/dist/layers/advanced_activations.js
var ReLU = class extends Layer {
  constructor(args) {
    super(args == null ? {} : args);
    this.supportsMasking = true;
    if (args != null) {
      this.maxValue = args.maxValue;
    }
  }
  call(inputs, kwargs) {
    inputs = getExactlyOneTensor(inputs);
    let output = relu(inputs);
    if (this.maxValue != null) {
      output = clipByValue(output, 0, this.maxValue);
    }
    return output;
  }
  computeOutputShape(inputShape) {
    return inputShape;
  }
  getConfig() {
    const config = { maxValue: this.maxValue };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
ReLU.className = "ReLU";
serialization_exports.registerClass(ReLU);
var LeakyReLU = class extends Layer {
  constructor(args) {
    super(args == null ? {} : args);
    this.DEFAULT_ALPHA = 0.3;
    if (args == null) {
      args = {};
    }
    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;
  }
  call(inputs, kwargs) {
    const x = getExactlyOneTensor(inputs);
    return leakyRelu(x, this.alpha);
  }
  computeOutputShape(inputShape) {
    return inputShape;
  }
  getConfig() {
    const config = { alpha: this.alpha };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
LeakyReLU.className = "LeakyReLU";
serialization_exports.registerClass(LeakyReLU);
var PReLU = class extends Layer {
  constructor(args) {
    super(args == null ? {} : args);
    this.DEFAULT_ALPHA_INITIALIZER = "zeros";
    if (args == null) {
      args = {};
    }
    this.supportsMasking = true;
    this.alphaInitializer = getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);
    this.alphaRegularizer = getRegularizer(args.alphaRegularizer);
    this.alphaConstraint = getConstraint(args.alphaConstraint);
    if (args.sharedAxes == null) {
      this.sharedAxes = null;
    } else if (Array.isArray(args.sharedAxes)) {
      this.sharedAxes = args.sharedAxes;
    } else if (typeof args.sharedAxes === "number") {
      this.sharedAxes = [args.sharedAxes];
    } else {
      throw new ValueError(`Expected sharedAxes to be a number or an array of numbers, but got ${args.sharedAxes}`);
    }
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const paramShape = inputShape.slice(1);
    if (this.sharedAxes != null) {
      for (const i of this.sharedAxes) {
        paramShape[i - 1] = 1;
      }
    }
    this.alpha = this.addWeight("alpha", paramShape, "float32", this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint);
    const axes = {};
    if (this.sharedAxes != null) {
      for (let i = 1; i < inputShape.length; ++i) {
        axes[i] = inputShape[i];
      }
    }
    this.inputSpec = [new InputSpec({
      ndim: inputShape.length,
      axes
    })];
    this.built = true;
  }
  call(inputs, kwargs) {
    inputs = getExactlyOneTensor(inputs);
    return prelu(inputs, this.alpha.read());
  }
  getConfig() {
    const config = {
      alphaInitializer: serializeInitializer(this.alphaInitializer),
      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),
      alphaConstraint: serializeConstraint(this.alphaConstraint),
      sharedAxes: this.sharedAxes
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
PReLU.className = "PReLU";
serialization_exports.registerClass(PReLU);
var ELU = class extends Layer {
  constructor(args) {
    super(args == null ? {} : args);
    this.DEFAULT_ALPHA = 1;
    if (args == null) {
      args = {};
    }
    if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {
      throw new NotImplementedError(`Non-default alpha value (${args.alpha}) is not supported by the ELU layer yet.`);
    }
    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;
  }
  call(inputs, kwargs) {
    const x = getExactlyOneTensor(inputs);
    return elu(x);
  }
  computeOutputShape(inputShape) {
    return inputShape;
  }
  getConfig() {
    const config = { alpha: this.alpha };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
ELU.className = "ELU";
serialization_exports.registerClass(ELU);
var ThresholdedReLU = class extends Layer {
  constructor(args) {
    super(args == null ? {} : args);
    this.DEFAULT_THETA = 1;
    if (args == null) {
      args = {};
    }
    this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;
  }
  call(inputs, kwargs) {
    const x = getExactlyOneTensor(inputs);
    return mul(x, cast(greater(x, this.theta), "float32"));
  }
  computeOutputShape(inputShape) {
    return inputShape;
  }
  getConfig() {
    const config = { theta: this.theta };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
ThresholdedReLU.className = "ThresholdedReLU";
serialization_exports.registerClass(ThresholdedReLU);
var Softmax3 = class extends Layer {
  constructor(args) {
    super(args == null ? {} : args);
    this.DEFAULT_AXIS = 1;
    if (args == null) {
      args = {};
    }
    this.softmax = new Softmax2().apply;
    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;
  }
  call(inputs, kwargs) {
    const x = getExactlyOneTensor(inputs);
    return this.softmax(x, this.axis);
  }
  computeOutputShape(inputShape) {
    return inputShape;
  }
  getConfig() {
    const config = { axis: this.axis };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Softmax3.className = "Softmax";
serialization_exports.registerClass(Softmax3);

// node_modules/@tensorflow/tfjs-layers/dist/utils/conv_utils.js
function normalizeArray(value, n, name) {
  if (typeof value === "number") {
    return pyListRepeat(value, n);
  } else {
    if (value.length !== n) {
      throw new ValueError(`The ${name} argument must be an integer or tuple of ${n} integers. Received: ${value.length} elements.`);
    }
    for (let i = 0; i < n; ++i) {
      const singleValue = value[i];
      if (!isInteger(singleValue)) {
        throw new ValueError(`The ${name} argument must be an integer or tuple of ${n} integers. Received: ${JSON.stringify(value)} including a non-integer number ${singleValue}`);
      }
    }
    return value;
  }
}
function convOutputLength(inputLength, filterSize, padding, stride, dilation = 1) {
  if (inputLength == null) {
    return inputLength;
  }
  const dilatedFilterSize = filterSize + (filterSize - 1) * (dilation - 1);
  let outputLength;
  if (padding === "same") {
    outputLength = inputLength;
  } else {
    outputLength = inputLength - dilatedFilterSize + 1;
  }
  return Math.floor((outputLength + stride - 1) / stride);
}
function deconvLength(dimSize, strideSize, kernelSize, padding) {
  if (dimSize == null) {
    return null;
  }
  if (padding === "valid") {
    dimSize = dimSize * strideSize + max2([kernelSize - strideSize, 0]);
  } else if (padding === "same") {
    dimSize = dimSize * strideSize;
  } else {
    throw new ValueError(`Unsupport padding mode: ${padding}.`);
  }
  return dimSize;
}

// node_modules/@tensorflow/tfjs-layers/dist/layers/convolutional.js
function preprocessConv2DInput(x, dataFormat) {
  return tidy(() => {
    checkDataFormat(dataFormat);
    if (dataFormat === "channelsFirst") {
      return transpose(x, [0, 2, 3, 1]);
    } else {
      return x;
    }
  });
}
function preprocessConv3DInput(x, dataFormat) {
  return tidy(() => {
    checkDataFormat(dataFormat);
    if (dataFormat === "channelsFirst") {
      return transpose(x, [0, 2, 3, 4, 1]);
    } else {
      return x;
    }
  });
}
function conv1dWithBias(x, kernel, bias, strides = 1, padding = "valid", dataFormat, dilationRate = 1) {
  return tidy(() => {
    if (dataFormat == null) {
      dataFormat = imageDataFormat();
    }
    checkDataFormat(dataFormat);
    if (x.shape.length !== 3) {
      throw new ValueError(`The input of a conv1dWithBias operation should be 3, but is ${x.shape.length} instead.`);
    }
    if (kernel.shape.length !== 3) {
      throw new ValueError(`The kernel for a conv1dWithBias operation should be 3, but is ${kernel.shape.length} instead`);
    }
    if (bias != null && bias.shape.length !== 1) {
      throw new ValueError(`The bias for a conv1dWithBias operation should be 1, but is ${kernel.shape.length} instead`);
    }
    if (dataFormat === "channelsFirst") {
      x = transpose(x, [0, 2, 1]);
    }
    if (padding === "causal") {
      throw new NotImplementedError("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");
    }
    let y = conv1d(x, kernel, strides, padding === "same" ? "same" : "valid", "NWC", dilationRate);
    if (bias != null) {
      y = biasAdd(y, bias);
    }
    return y;
  });
}
function conv2dWithBiasActivation(x, kernel, bias, strides = [1, 1], padding = "valid", dataFormat, dilationRate, activation2 = null) {
  return tidy(() => {
    if (dataFormat == null) {
      dataFormat = imageDataFormat();
    }
    checkDataFormat(dataFormat);
    if (x.rank !== 3 && x.rank !== 4) {
      throw new ValueError(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${x.rank}.`);
    }
    if (kernel.rank !== 3 && kernel.rank !== 4) {
      throw new ValueError(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${x.rank}.`);
    }
    let y = preprocessConv2DInput(x, dataFormat);
    if (padding === "causal") {
      throw new NotImplementedError("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");
    }
    y = fused_ops_exports.conv2d({
      x: y,
      filter: kernel,
      strides,
      pad: padding === "same" ? "same" : "valid",
      dilations: dilationRate,
      dataFormat: "NHWC",
      bias,
      activation: activation2
    });
    if (dataFormat === "channelsFirst") {
      y = transpose(y, [0, 3, 1, 2]);
    }
    return y;
  });
}
function conv3dWithBias(x, kernel, bias, strides = [1, 1, 1], padding = "valid", dataFormat, dilationRate) {
  return tidy(() => {
    if (dataFormat == null) {
      dataFormat = imageDataFormat();
    }
    checkDataFormat(dataFormat);
    if (x.rank !== 4 && x.rank !== 5) {
      throw new ValueError(`conv3dWithBias expects input to be of rank 4 or 5, but received ${x.rank}.`);
    }
    if (kernel.rank !== 4 && kernel.rank !== 5) {
      throw new ValueError(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${x.rank}.`);
    }
    let y = preprocessConv3DInput(x, dataFormat);
    if (padding === "causal") {
      throw new NotImplementedError("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");
    }
    y = conv3d(y, kernel, strides, padding === "same" ? "same" : "valid", "NDHWC", dilationRate);
    if (bias != null) {
      y = biasAdd(y, bias);
    }
    if (dataFormat === "channelsFirst") {
      y = transpose(y, [0, 4, 1, 2, 3]);
    }
    return y;
  });
}
var BaseConv = class _BaseConv extends Layer {
  constructor(rank, args) {
    super(args);
    this.bias = null;
    this.DEFAULT_KERNEL_INITIALIZER = "glorotNormal";
    this.DEFAULT_BIAS_INITIALIZER = "zeros";
    _BaseConv.verifyArgs(args);
    this.rank = rank;
    assertPositiveInteger(this.rank, "rank");
    if (this.rank !== 1 && this.rank !== 2 && this.rank !== 3) {
      throw new NotImplementedError(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);
    }
    this.kernelSize = normalizeArray(args.kernelSize, rank, "kernelSize");
    this.strides = normalizeArray(args.strides == null ? 1 : args.strides, rank, "strides");
    this.padding = args.padding == null ? "valid" : args.padding;
    checkPaddingMode(this.padding);
    this.dataFormat = args.dataFormat == null ? "channelsLast" : args.dataFormat;
    checkDataFormat(this.dataFormat);
    this.activation = getActivation(args.activation);
    this.useBias = args.useBias == null ? true : args.useBias;
    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);
    this.biasConstraint = getConstraint(args.biasConstraint);
    this.biasRegularizer = getRegularizer(args.biasRegularizer);
    this.activityRegularizer = getRegularizer(args.activityRegularizer);
    this.dilationRate = normalizeArray(args.dilationRate == null ? 1 : args.dilationRate, rank, "dilationRate");
    if (this.rank === 1 && (Array.isArray(this.dilationRate) && this.dilationRate.length !== 1)) {
      throw new ValueError(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);
    } else if (this.rank === 2) {
      if (typeof this.dilationRate === "number") {
        this.dilationRate = [this.dilationRate, this.dilationRate];
      } else if (this.dilationRate.length !== 2) {
        throw new ValueError(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`);
      }
    } else if (this.rank === 3) {
      if (typeof this.dilationRate === "number") {
        this.dilationRate = [this.dilationRate, this.dilationRate, this.dilationRate];
      } else if (this.dilationRate.length !== 3) {
        throw new ValueError(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`);
      }
    }
  }
  static verifyArgs(args) {
    assert2("kernelSize" in args, `required key 'kernelSize' not in config`);
    if (typeof args.kernelSize !== "number" && !checkArrayTypeAndLength(args.kernelSize, "number", 1, 3)) {
      throw new ValueError(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(args.kernelSize)}.`);
    }
  }
  getConfig() {
    const config = {
      kernelSize: this.kernelSize,
      strides: this.strides,
      padding: this.padding,
      dataFormat: this.dataFormat,
      dilationRate: this.dilationRate,
      activation: serializeActivation(this.activation),
      useBias: this.useBias,
      biasInitializer: serializeInitializer(this.biasInitializer),
      biasRegularizer: serializeRegularizer(this.biasRegularizer),
      activityRegularizer: serializeRegularizer(this.activityRegularizer),
      biasConstraint: serializeConstraint(this.biasConstraint)
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
var Conv = class _Conv extends BaseConv {
  constructor(rank, args) {
    super(rank, args);
    this.kernel = null;
    _Conv.verifyArgs(args);
    this.filters = args.filters;
    assertPositiveInteger(this.filters, "filters");
    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);
    this.kernelConstraint = getConstraint(args.kernelConstraint);
    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const channelAxis = this.dataFormat === "channelsFirst" ? 1 : inputShape.length - 1;
    if (inputShape[channelAxis] == null) {
      throw new ValueError(`The channel dimension of the input should be defined. Found ${inputShape[channelAxis]}`);
    }
    const inputDim = inputShape[channelAxis];
    const kernelShape = this.kernelSize.concat([inputDim, this.filters]);
    this.kernel = this.addWeight("kernel", kernelShape, null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);
    if (this.useBias) {
      this.bias = this.addWeight("bias", [this.filters], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);
    }
    this.inputSpec = [{ ndim: this.rank + 2, axes: { [channelAxis]: inputDim } }];
    this.built = true;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = getExactlyOneTensor(inputs);
      let outputs;
      const biasValue = this.bias == null ? null : this.bias.read();
      const fusedActivationName = mapActivationToFusedKernel(this.activation.getClassName());
      if (fusedActivationName != null && this.rank === 2) {
        outputs = conv2dWithBiasActivation(inputs, this.kernel.read(), biasValue, this.strides, this.padding, this.dataFormat, this.dilationRate, fusedActivationName);
      } else {
        if (this.rank === 1) {
          outputs = conv1dWithBias(inputs, this.kernel.read(), biasValue, this.strides[0], this.padding, this.dataFormat, this.dilationRate[0]);
        } else if (this.rank === 2) {
          outputs = conv2dWithBiasActivation(inputs, this.kernel.read(), biasValue, this.strides, this.padding, this.dataFormat, this.dilationRate);
        } else if (this.rank === 3) {
          outputs = conv3dWithBias(inputs, this.kernel.read(), biasValue, this.strides, this.padding, this.dataFormat, this.dilationRate);
        } else {
          throw new NotImplementedError("convolutions greater than 3D are not implemented yet.");
        }
        if (this.activation != null) {
          outputs = this.activation.apply(outputs);
        }
      }
      return outputs;
    });
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const newSpace = [];
    const space = this.dataFormat === "channelsLast" ? inputShape.slice(1, inputShape.length - 1) : inputShape.slice(2);
    for (let i = 0; i < space.length; ++i) {
      const newDim = convOutputLength(space[i], this.kernelSize[i], this.padding, this.strides[i], typeof this.dilationRate === "number" ? this.dilationRate : this.dilationRate[i]);
      newSpace.push(newDim);
    }
    let outputShape = [inputShape[0]];
    if (this.dataFormat === "channelsLast") {
      outputShape = outputShape.concat(newSpace);
      outputShape.push(this.filters);
    } else {
      outputShape.push(this.filters);
      outputShape = outputShape.concat(newSpace);
    }
    return outputShape;
  }
  getConfig() {
    const config = {
      filters: this.filters,
      kernelInitializer: serializeInitializer(this.kernelInitializer),
      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),
      kernelConstraint: serializeConstraint(this.kernelConstraint)
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
  static verifyArgs(args) {
    if (!("filters" in args) || typeof args.filters !== "number" || args.filters < 1) {
      throw new ValueError(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(args.filters)}`);
    }
  }
};
var Conv2D2 = class _Conv2D extends Conv {
  constructor(args) {
    super(2, args);
    _Conv2D.verifyArgs(args);
  }
  getConfig() {
    const config = super.getConfig();
    delete config["rank"];
    return config;
  }
  static verifyArgs(args) {
    if (typeof args.kernelSize !== "number" && !checkArrayTypeAndLength(args.kernelSize, "number", 1, 2)) {
      throw new ValueError(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(args.kernelSize)}.`);
    }
  }
};
Conv2D2.className = "Conv2D";
serialization_exports.registerClass(Conv2D2);
var Conv3D2 = class _Conv3D extends Conv {
  constructor(args) {
    super(3, args);
    _Conv3D.verifyArgs(args);
  }
  getConfig() {
    const config = super.getConfig();
    delete config["rank"];
    return config;
  }
  static verifyArgs(args) {
    if (typeof args.kernelSize !== "number") {
      if (!(Array.isArray(args.kernelSize) && (args.kernelSize.length === 1 || args.kernelSize.length === 3))) {
        throw new ValueError(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(args.kernelSize)}.`);
      }
    }
  }
};
Conv3D2.className = "Conv3D";
serialization_exports.registerClass(Conv3D2);
var Conv2DTranspose = class extends Conv2D2 {
  constructor(args) {
    super(args);
    this.inputSpec = [new InputSpec({ ndim: 4 })];
    if (this.padding !== "same" && this.padding !== "valid") {
      throw new ValueError(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`);
    }
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    if (inputShape.length !== 4) {
      throw new ValueError("Input should have rank 4; Received input shape: " + JSON.stringify(inputShape));
    }
    const channelAxis = this.dataFormat === "channelsFirst" ? 1 : inputShape.length - 1;
    if (inputShape[channelAxis] == null) {
      throw new ValueError("The channel dimension of the inputs should be defined. Found `None`.");
    }
    const inputDim = inputShape[channelAxis];
    const kernelShape = this.kernelSize.concat([this.filters, inputDim]);
    this.kernel = this.addWeight("kernel", kernelShape, "float32", this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);
    if (this.useBias) {
      this.bias = this.addWeight("bias", [this.filters], "float32", this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);
    }
    this.inputSpec = [new InputSpec({ ndim: 4, axes: { [channelAxis]: inputDim } })];
    this.built = true;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      let input2 = getExactlyOneTensor(inputs);
      if (input2.shape.length !== 4) {
        throw new ValueError(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${input2.shape.length}`);
      }
      const inputShape = input2.shape;
      const batchSize = inputShape[0];
      let hAxis;
      let wAxis;
      if (this.dataFormat === "channelsFirst") {
        hAxis = 2;
        wAxis = 3;
      } else {
        hAxis = 1;
        wAxis = 2;
      }
      const height = inputShape[hAxis];
      const width = inputShape[wAxis];
      const kernelH = this.kernelSize[0];
      const kernelW = this.kernelSize[1];
      const strideH = this.strides[0];
      const strideW = this.strides[1];
      const outHeight = deconvLength(height, strideH, kernelH, this.padding);
      const outWidth = deconvLength(width, strideW, kernelW, this.padding);
      const outputShape = [batchSize, outHeight, outWidth, this.filters];
      if (this.dataFormat !== "channelsLast") {
        input2 = transpose(input2, [0, 2, 3, 1]);
      }
      let outputs = conv2dTranspose(input2, this.kernel.read(), outputShape, this.strides, this.padding);
      if (this.dataFormat !== "channelsLast") {
        outputs = transpose(outputs, [0, 3, 1, 2]);
      }
      if (this.bias != null) {
        outputs = biasAdd(outputs, this.bias.read(), this.dataFormat);
      }
      if (this.activation != null) {
        outputs = this.activation.apply(outputs);
      }
      return outputs;
    });
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const outputShape = inputShape.slice();
    let channelAxis;
    let heightAxis;
    let widthAxis;
    if (this.dataFormat === "channelsFirst") {
      channelAxis = 1;
      heightAxis = 2;
      widthAxis = 3;
    } else {
      channelAxis = 3;
      heightAxis = 1;
      widthAxis = 2;
    }
    const kernelH = this.kernelSize[0];
    const kernelW = this.kernelSize[1];
    const strideH = this.strides[0];
    const strideW = this.strides[1];
    outputShape[channelAxis] = this.filters;
    outputShape[heightAxis] = deconvLength(outputShape[heightAxis], strideH, kernelH, this.padding);
    outputShape[widthAxis] = deconvLength(outputShape[widthAxis], strideW, kernelW, this.padding);
    return outputShape;
  }
  getConfig() {
    const config = super.getConfig();
    delete config["dilationRate"];
    return config;
  }
};
Conv2DTranspose.className = "Conv2DTranspose";
serialization_exports.registerClass(Conv2DTranspose);
var Conv3DTranspose = class extends Conv3D2 {
  constructor(args) {
    super(args);
    this.inputSpec = [new InputSpec({ ndim: 5 })];
    if (this.padding !== "same" && this.padding !== "valid") {
      throw new ValueError(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`);
    }
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    if (inputShape.length !== 5) {
      throw new ValueError("Input should have rank 5; Received input shape: " + JSON.stringify(inputShape));
    }
    const channelAxis = this.dataFormat === "channelsFirst" ? 1 : inputShape.length - 1;
    if (inputShape[channelAxis] == null) {
      throw new ValueError("The channel dimension of the inputs should be defined. Found `None`.");
    }
    const inputDim = inputShape[channelAxis];
    const kernelShape = this.kernelSize.concat([this.filters, inputDim]);
    this.kernel = this.addWeight("kernel", kernelShape, "float32", this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);
    if (this.useBias) {
      this.bias = this.addWeight("bias", [this.filters], "float32", this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);
    }
    this.inputSpec = [new InputSpec({ ndim: 5, axes: { [channelAxis]: inputDim } })];
    this.built = true;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      let input2 = getExactlyOneTensor(inputs);
      if (input2.shape.length !== 5) {
        throw new ValueError(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${input2.shape.length}`);
      }
      const inputShape = input2.shape;
      const batchSize = inputShape[0];
      let hAxis;
      let wAxis;
      let dAxis;
      if (this.dataFormat === "channelsFirst") {
        dAxis = 2;
        hAxis = 3;
        wAxis = 4;
      } else {
        dAxis = 1;
        hAxis = 2;
        wAxis = 3;
      }
      const depth = inputShape[dAxis];
      const height = inputShape[hAxis];
      const width = inputShape[wAxis];
      const kernelD = this.kernelSize[0];
      const kernelH = this.kernelSize[1];
      const kernelW = this.kernelSize[2];
      const strideD = this.strides[0];
      const strideH = this.strides[1];
      const strideW = this.strides[2];
      const outDepth = deconvLength(depth, strideD, kernelD, this.padding);
      const outHeight = deconvLength(height, strideH, kernelH, this.padding);
      const outWidth = deconvLength(width, strideW, kernelW, this.padding);
      const outputShape = [batchSize, outDepth, outHeight, outWidth, this.filters];
      if (this.dataFormat !== "channelsLast") {
        input2 = transpose(input2, [0, 2, 3, 4, 1]);
      }
      let outputs = conv3dTranspose(input2, this.kernel.read(), outputShape, this.strides, this.padding);
      if (this.dataFormat !== "channelsLast") {
        outputs = transpose(outputs, [0, 4, 1, 2, 3]);
      }
      if (this.bias !== null) {
        outputs = biasAdd(outputs, this.bias.read(), this.dataFormat);
      }
      if (this.activation !== null) {
        outputs = this.activation.apply(outputs);
      }
      return outputs;
    });
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const outputShape = inputShape.slice();
    let channelAxis;
    let depthAxis;
    let heightAxis;
    let widthAxis;
    if (this.dataFormat === "channelsFirst") {
      channelAxis = 1;
      depthAxis = 2;
      heightAxis = 3;
      widthAxis = 4;
    } else {
      channelAxis = 4;
      depthAxis = 1;
      heightAxis = 2;
      widthAxis = 3;
    }
    const kernelD = this.kernelSize[0];
    const kernelH = this.kernelSize[1];
    const kernelW = this.kernelSize[2];
    const strideD = this.strides[0];
    const strideH = this.strides[1];
    const strideW = this.strides[2];
    outputShape[channelAxis] = this.filters;
    outputShape[depthAxis] = deconvLength(outputShape[depthAxis], strideD, kernelD, this.padding);
    outputShape[heightAxis] = deconvLength(outputShape[heightAxis], strideH, kernelH, this.padding);
    outputShape[widthAxis] = deconvLength(outputShape[widthAxis], strideW, kernelW, this.padding);
    return outputShape;
  }
  getConfig() {
    const config = super.getConfig();
    delete config["dilationRate"];
    return config;
  }
};
Conv3DTranspose.className = "Conv3DTranspose";
serialization_exports.registerClass(Conv3DTranspose);
var SeparableConv = class extends Conv {
  constructor(rank, config) {
    super(rank, config);
    this.DEFAULT_DEPTHWISE_INITIALIZER = "glorotUniform";
    this.DEFAULT_POINTWISE_INITIALIZER = "glorotUniform";
    this.depthwiseKernel = null;
    this.pointwiseKernel = null;
    if (config.filters == null) {
      throw new ValueError("The `filters` configuration field is required by SeparableConv, but is unspecified.");
    }
    if (config.kernelInitializer != null || config.kernelRegularizer != null || config.kernelConstraint != null) {
      throw new ValueError("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");
    }
    if (config.padding != null && config.padding !== "same" && config.padding !== "valid") {
      throw new ValueError(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(config.padding)}`);
    }
    this.depthMultiplier = config.depthMultiplier == null ? 1 : config.depthMultiplier;
    this.depthwiseInitializer = getInitializer(config.depthwiseInitializer || this.DEFAULT_DEPTHWISE_INITIALIZER);
    this.depthwiseRegularizer = getRegularizer(config.depthwiseRegularizer);
    this.depthwiseConstraint = getConstraint(config.depthwiseConstraint);
    this.pointwiseInitializer = getInitializer(config.depthwiseInitializer || this.DEFAULT_POINTWISE_INITIALIZER);
    this.pointwiseRegularizer = getRegularizer(config.pointwiseRegularizer);
    this.pointwiseConstraint = getConstraint(config.pointwiseConstraint);
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    if (inputShape.length < this.rank + 2) {
      throw new ValueError(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank + 2}, but received input shape: ${JSON.stringify(inputShape)}`);
    }
    const channelAxis = this.dataFormat === "channelsFirst" ? 1 : inputShape.length - 1;
    if (inputShape[channelAxis] == null || inputShape[channelAxis] < 0) {
      throw new ValueError(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(inputShape[channelAxis])}`);
    }
    const inputDim = inputShape[channelAxis];
    const depthwiseKernelShape = this.kernelSize.concat([inputDim, this.depthMultiplier]);
    const pointwiseKernelShape = [];
    for (let i = 0; i < this.rank; ++i) {
      pointwiseKernelShape.push(1);
    }
    pointwiseKernelShape.push(inputDim * this.depthMultiplier, this.filters);
    const trainable = true;
    this.depthwiseKernel = this.addWeight("depthwise_kernel", depthwiseKernelShape, "float32", this.depthwiseInitializer, this.depthwiseRegularizer, trainable, this.depthwiseConstraint);
    this.pointwiseKernel = this.addWeight("pointwise_kernel", pointwiseKernelShape, "float32", this.pointwiseInitializer, this.pointwiseRegularizer, trainable, this.pointwiseConstraint);
    if (this.useBias) {
      this.bias = this.addWeight("bias", [this.filters], "float32", this.biasInitializer, this.biasRegularizer, trainable, this.biasConstraint);
    } else {
      this.bias = null;
    }
    this.inputSpec = [new InputSpec({ ndim: this.rank + 2, axes: { [channelAxis]: inputDim } })];
    this.built = true;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = getExactlyOneTensor(inputs);
      let output;
      if (this.rank === 1) {
        throw new NotImplementedError("1D separable convolution is not implemented yet.");
      } else if (this.rank === 2) {
        if (this.dataFormat === "channelsFirst") {
          inputs = transpose(inputs, [0, 2, 3, 1]);
        }
        output = separableConv2d(inputs, this.depthwiseKernel.read(), this.pointwiseKernel.read(), this.strides, this.padding, this.dilationRate, "NHWC");
      }
      if (this.useBias) {
        output = biasAdd(output, this.bias.read(), this.dataFormat);
      }
      if (this.activation != null) {
        output = this.activation.apply(output);
      }
      if (this.dataFormat === "channelsFirst") {
        output = transpose(output, [0, 3, 1, 2]);
      }
      return output;
    });
  }
  getConfig() {
    const config = super.getConfig();
    delete config["rank"];
    delete config["kernelInitializer"];
    delete config["kernelRegularizer"];
    delete config["kernelConstraint"];
    config["depthwiseInitializer"] = serializeInitializer(this.depthwiseInitializer);
    config["pointwiseInitializer"] = serializeInitializer(this.pointwiseInitializer);
    config["depthwiseRegularizer"] = serializeRegularizer(this.depthwiseRegularizer);
    config["pointwiseRegularizer"] = serializeRegularizer(this.pointwiseRegularizer);
    config["depthwiseConstraint"] = serializeConstraint(this.depthwiseConstraint);
    config["pointwiseConstraint"] = serializeConstraint(this.pointwiseConstraint);
    return config;
  }
};
SeparableConv.className = "SeparableConv";
var SeparableConv2D = class extends SeparableConv {
  constructor(args) {
    super(2, args);
  }
};
SeparableConv2D.className = "SeparableConv2D";
serialization_exports.registerClass(SeparableConv2D);
var Conv1D = class _Conv1D extends Conv {
  constructor(args) {
    super(1, args);
    _Conv1D.verifyArgs(args);
    this.inputSpec = [{ ndim: 3 }];
  }
  getConfig() {
    const config = super.getConfig();
    delete config["rank"];
    delete config["dataFormat"];
    return config;
  }
  static verifyArgs(args) {
    if (typeof args.kernelSize !== "number" && !checkArrayTypeAndLength(args.kernelSize, "number", 1, 1)) {
      throw new ValueError(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(args.kernelSize)}.`);
    }
  }
};
Conv1D.className = "Conv1D";
serialization_exports.registerClass(Conv1D);
var Cropping2D = class extends Layer {
  constructor(args) {
    super(args);
    if (typeof args.cropping === "number") {
      this.cropping = [[args.cropping, args.cropping], [args.cropping, args.cropping]];
    } else if (typeof args.cropping[0] === "number") {
      this.cropping = [
        [args.cropping[0], args.cropping[0]],
        [args.cropping[1], args.cropping[1]]
      ];
    } else {
      this.cropping = args.cropping;
    }
    this.dataFormat = args.dataFormat === void 0 ? "channelsLast" : args.dataFormat;
    this.inputSpec = [{ ndim: 4 }];
  }
  computeOutputShape(inputShape) {
    if (this.dataFormat === "channelsFirst") {
      return [
        inputShape[0],
        inputShape[1],
        inputShape[2] - this.cropping[0][0] - this.cropping[0][1],
        inputShape[3] - this.cropping[1][0] - this.cropping[1][1]
      ];
    } else {
      return [
        inputShape[0],
        inputShape[1] - this.cropping[0][0] - this.cropping[0][1],
        inputShape[2] - this.cropping[1][0] - this.cropping[1][1],
        inputShape[3]
      ];
    }
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = getExactlyOneTensor(inputs);
      if (this.dataFormat === "channelsLast") {
        const hSliced = sliceAlongAxis(inputs, this.cropping[0][0], inputs.shape[1] - this.cropping[0][0] - this.cropping[0][1], 2);
        return sliceAlongAxis(hSliced, this.cropping[1][0], inputs.shape[2] - this.cropping[1][1] - this.cropping[1][0], 3);
      } else {
        const hSliced = sliceAlongAxis(inputs, this.cropping[0][0], inputs.shape[2] - this.cropping[0][0] - this.cropping[0][1], 3);
        return sliceAlongAxis(hSliced, this.cropping[1][0], inputs.shape[3] - this.cropping[1][1] - this.cropping[1][0], 4);
      }
    });
  }
  getConfig() {
    const config = { cropping: this.cropping, dataFormat: this.dataFormat };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Cropping2D.className = "Cropping2D";
serialization_exports.registerClass(Cropping2D);
var UpSampling2D = class extends Layer {
  constructor(args) {
    super(args);
    this.DEFAULT_SIZE = [2, 2];
    this.inputSpec = [{ ndim: 4 }];
    this.size = args.size == null ? this.DEFAULT_SIZE : args.size;
    this.dataFormat = args.dataFormat == null ? "channelsLast" : args.dataFormat;
    checkDataFormat(this.dataFormat);
    this.interpolation = args.interpolation == null ? "nearest" : args.interpolation;
    checkInterpolationFormat(this.interpolation);
  }
  computeOutputShape(inputShape) {
    if (this.dataFormat === "channelsFirst") {
      const height = inputShape[2] == null ? null : this.size[0] * inputShape[2];
      const width = inputShape[3] == null ? null : this.size[1] * inputShape[3];
      return [inputShape[0], inputShape[1], height, width];
    } else {
      const height = inputShape[1] == null ? null : this.size[0] * inputShape[1];
      const width = inputShape[2] == null ? null : this.size[1] * inputShape[2];
      return [inputShape[0], height, width, inputShape[3]];
    }
  }
  call(inputs, kwargs) {
    return tidy(() => {
      let input2 = getExactlyOneTensor(inputs);
      const inputShape = input2.shape;
      if (this.dataFormat === "channelsFirst") {
        input2 = transpose(input2, [0, 2, 3, 1]);
        const height = this.size[0] * inputShape[2];
        const width = this.size[1] * inputShape[3];
        const resized = this.interpolation === "nearest" ? image.resizeNearestNeighbor(input2, [height, width]) : image.resizeBilinear(input2, [height, width]);
        return transpose(resized, [0, 3, 1, 2]);
      } else {
        const height = this.size[0] * inputShape[1];
        const width = this.size[1] * inputShape[2];
        return this.interpolation === "nearest" ? image.resizeNearestNeighbor(input2, [height, width]) : image.resizeBilinear(input2, [height, width]);
      }
    });
  }
  getConfig() {
    const config = {
      size: this.size,
      dataFormat: this.dataFormat,
      interpolation: this.interpolation
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
UpSampling2D.className = "UpSampling2D";
serialization_exports.registerClass(UpSampling2D);

// node_modules/@tensorflow/tfjs-layers/dist/layers/convolutional_depthwise.js
function depthwiseConv2d2(x, depthwiseKernel, strides = [1, 1], padding = "valid", dataFormat, dilationRate) {
  return tidy(() => {
    if (dataFormat == null) {
      dataFormat = imageDataFormat();
    }
    checkDataFormat(dataFormat);
    let y = preprocessConv2DInput(x, dataFormat);
    if (x.rank !== 4) {
      throw new ValueError(`Input for depthwiseConv2d is required to be 4-D, but is instead ${x.rank}-D`);
    }
    if (depthwiseKernel.rank !== 4) {
      throw new ValueError(`depthwiseKernel is required to be 4-D, but is instead ${depthwiseKernel.rank}-D`);
    }
    y = depthwiseConv2d(y, depthwiseKernel, strides, padding === "same" ? "same" : "valid", "NHWC", dilationRate);
    if (dataFormat === "channelsFirst") {
      y = transpose(y, [0, 3, 1, 2]);
    }
    return y;
  });
}
var DepthwiseConv2D = class extends BaseConv {
  constructor(args) {
    super(2, args);
    this.depthwiseKernel = null;
    this.depthMultiplier = args.depthMultiplier == null ? 1 : args.depthMultiplier;
    this.depthwiseInitializer = getInitializer(args.depthwiseInitializer || this.DEFAULT_KERNEL_INITIALIZER);
    this.depthwiseConstraint = getConstraint(args.depthwiseConstraint);
    this.depthwiseRegularizer = getRegularizer(args.depthwiseRegularizer);
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    if (inputShape.length < 4) {
      throw new ValueError(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(inputShape)}.`);
    }
    const channelAxis = this.dataFormat === "channelsFirst" ? 1 : 3;
    if (inputShape[channelAxis] == null || inputShape[channelAxis] < 0) {
      throw new ValueError(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${inputShape[channelAxis]}).`);
    }
    const inputDim = inputShape[channelAxis];
    const depthwiseKernelShape = [
      this.kernelSize[0],
      this.kernelSize[1],
      inputDim,
      this.depthMultiplier
    ];
    this.depthwiseKernel = this.addWeight("depthwise_kernel", depthwiseKernelShape, null, this.depthwiseInitializer, this.depthwiseRegularizer, true, this.depthwiseConstraint);
    if (this.useBias) {
      this.bias = this.addWeight("bias", [inputDim * this.depthMultiplier], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);
    } else {
      this.bias = null;
    }
    this.built = true;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = getExactlyOneTensor(inputs);
      let outputs = depthwiseConv2d2(inputs, this.depthwiseKernel.read(), this.strides, this.padding, this.dataFormat, null);
      if (this.useBias) {
        outputs = biasAdd(outputs, this.bias.read(), this.dataFormat);
      }
      if (this.activation != null) {
        outputs = this.activation.apply(outputs);
      }
      return outputs;
    });
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const rows = this.dataFormat === "channelsFirst" ? inputShape[2] : inputShape[1];
    const cols = this.dataFormat === "channelsFirst" ? inputShape[3] : inputShape[2];
    const outFilters = this.dataFormat === "channelsFirst" ? inputShape[1] * this.depthMultiplier : inputShape[3] * this.depthMultiplier;
    const outRows = convOutputLength(rows, this.kernelSize[0], this.padding, this.strides[0]);
    const outCols = convOutputLength(cols, this.kernelSize[1], this.padding, this.strides[1]);
    if (this.dataFormat === "channelsFirst") {
      return [inputShape[0], outFilters, outRows, outCols];
    } else {
      return [inputShape[0], outRows, outCols, outFilters];
    }
  }
  getConfig() {
    const config = super.getConfig();
    config["depthMultiplier"] = this.depthMultiplier;
    config["depthwiseInitializer"] = serializeInitializer(this.depthwiseInitializer);
    config["depthwiseRegularizer"] = serializeRegularizer(this.depthwiseRegularizer);
    config["depthwiseConstraint"] = serializeConstraint(this.depthwiseRegularizer);
    return config;
  }
};
DepthwiseConv2D.className = "DepthwiseConv2D";
serialization_exports.registerClass(DepthwiseConv2D);

// node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js
function standardizeArgs(inputs, initialState, constants, numConstants) {
  if (Array.isArray(inputs)) {
    if (initialState != null || constants != null) {
      throw new ValueError("When inputs is an array, neither initialState or constants should be provided");
    }
    if (numConstants != null) {
      constants = inputs.slice(inputs.length - numConstants, inputs.length);
      inputs = inputs.slice(0, inputs.length - numConstants);
    }
    if (inputs.length > 1) {
      initialState = inputs.slice(1, inputs.length);
    }
    inputs = inputs[0];
  }
  function toListOrNull(x) {
    if (x == null || Array.isArray(x)) {
      return x;
    } else {
      return [x];
    }
  }
  initialState = toListOrNull(initialState);
  constants = toListOrNull(constants);
  return { inputs, initialState, constants };
}
function rnn(stepFunction, inputs, initialStates, goBackwards = false, mask, constants, unroll = false, needPerStepOutputs = false) {
  return tidy(() => {
    const ndim = inputs.shape.length;
    if (ndim < 3) {
      throw new ValueError(`Input should be at least 3D, but is ${ndim}D.`);
    }
    const axes = [1, 0].concat(range(2, ndim));
    inputs = transpose(inputs, axes);
    if (constants != null) {
      throw new NotImplementedError("The rnn() functoin of the deeplearn.js backend does not support constants yet.");
    }
    if (unroll) {
      console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend.");
    }
    if (mask != null) {
      mask = cast(cast(mask, "bool"), "float32");
      if (mask.rank === ndim - 1) {
        mask = expandDims(mask, -1);
      }
      mask = transpose(mask, axes);
    }
    if (goBackwards) {
      inputs = reverse(inputs, 0);
      if (mask != null) {
        mask = reverse(mask, 0);
      }
    }
    const perStepOutputs = [];
    let lastOutput;
    let states = initialStates;
    const timeSteps = inputs.shape[0];
    const perStepInputs = unstack(inputs);
    let perStepMasks;
    if (mask != null) {
      perStepMasks = unstack(mask);
    }
    for (let t = 0; t < timeSteps; ++t) {
      const currentInput = perStepInputs[t];
      const stepOutputs = tidy(() => stepFunction(currentInput, states));
      if (mask == null) {
        lastOutput = stepOutputs[0];
        states = stepOutputs[1];
      } else {
        const maskedOutputs = tidy(() => {
          const stepMask = perStepMasks[t];
          const negStepMask = sub(onesLike(stepMask), stepMask);
          const output = add(mul(stepOutputs[0], stepMask), mul(states[0], negStepMask));
          const newStates = states.map((state, i) => {
            return add(mul(stepOutputs[1][i], stepMask), mul(state, negStepMask));
          });
          return { output, newStates };
        });
        lastOutput = maskedOutputs.output;
        states = maskedOutputs.newStates;
      }
      if (needPerStepOutputs) {
        perStepOutputs.push(lastOutput);
      }
    }
    let outputs;
    if (needPerStepOutputs) {
      const axis = 1;
      outputs = stack(perStepOutputs, axis);
    }
    return [lastOutput, outputs, states];
  });
}
var RNN = class _RNN extends Layer {
  constructor(args) {
    super(args);
    let cell;
    if (args.cell == null) {
      throw new ValueError("cell property is missing for the constructor of RNN.");
    } else if (Array.isArray(args.cell)) {
      cell = new StackedRNNCells({ cells: args.cell });
    } else {
      cell = args.cell;
    }
    if (cell.stateSize == null) {
      throw new ValueError("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");
    }
    this.cell = cell;
    this.returnSequences = args.returnSequences == null ? false : args.returnSequences;
    this.returnState = args.returnState == null ? false : args.returnState;
    this.goBackwards = args.goBackwards == null ? false : args.goBackwards;
    this._stateful = args.stateful == null ? false : args.stateful;
    this.unroll = args.unroll == null ? false : args.unroll;
    this.supportsMasking = true;
    this.inputSpec = [new InputSpec({ ndim: 3 })];
    this.stateSpec = null;
    this.states_ = null;
    this.numConstants = null;
    this.keptStates = [];
  }
  // Porting Note: This is the equivalent of `RNN.states` property getter in
  //   PyKeras.
  getStates() {
    if (this.states_ == null) {
      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;
      return range(0, numStates).map((x) => null);
    } else {
      return this.states_;
    }
  }
  // Porting Note: This is the equivalent of the `RNN.states` property setter in
  //   PyKeras.
  setStates(states) {
    this.states_ = states;
  }
  computeOutputShape(inputShape) {
    if (isArrayOfShapes(inputShape)) {
      inputShape = inputShape[0];
    }
    inputShape = inputShape;
    let stateSize = this.cell.stateSize;
    if (!Array.isArray(stateSize)) {
      stateSize = [stateSize];
    }
    const outputDim = stateSize[0];
    let outputShape;
    if (this.returnSequences) {
      outputShape = [inputShape[0], inputShape[1], outputDim];
    } else {
      outputShape = [inputShape[0], outputDim];
    }
    if (this.returnState) {
      const stateShape = [];
      for (const dim of stateSize) {
        stateShape.push([inputShape[0], dim]);
      }
      return [outputShape].concat(stateShape);
    } else {
      return outputShape;
    }
  }
  computeMask(inputs, mask) {
    return tidy(() => {
      if (Array.isArray(mask)) {
        mask = mask[0];
      }
      const outputMask = this.returnSequences ? mask : null;
      if (this.returnState) {
        const stateMask = this.states.map((s) => null);
        return [outputMask].concat(stateMask);
      } else {
        return outputMask;
      }
    });
  }
  /**
   * Get the current state tensors of the RNN.
   *
   * If the state hasn't been set, return an array of `null`s of the correct
   * length.
   */
  get states() {
    if (this.states_ == null) {
      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;
      const output = [];
      for (let i = 0; i < numStates; ++i) {
        output.push(null);
      }
      return output;
    } else {
      return this.states_;
    }
  }
  set states(s) {
    this.states_ = s;
  }
  build(inputShape) {
    const constantShape = null;
    if (this.numConstants != null) {
      throw new NotImplementedError("Constants support is not implemented in RNN yet.");
    }
    if (isArrayOfShapes(inputShape)) {
      inputShape = inputShape[0];
    }
    inputShape = inputShape;
    const batchSize = this.stateful ? inputShape[0] : null;
    const inputDim = inputShape.slice(2);
    this.inputSpec[0] = new InputSpec({ shape: [batchSize, null, ...inputDim] });
    const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));
    if (constantShape != null) {
      throw new NotImplementedError("Constants support is not implemented in RNN yet.");
    } else {
      this.cell.build(stepInputShape);
    }
    let stateSize;
    if (Array.isArray(this.cell.stateSize)) {
      stateSize = this.cell.stateSize;
    } else {
      stateSize = [this.cell.stateSize];
    }
    if (this.stateSpec != null) {
      if (!util_exports.arraysEqual(this.stateSpec.map((spec) => spec.shape[spec.shape.length - 1]), stateSize)) {
        throw new ValueError(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`);
      }
    } else {
      this.stateSpec = stateSize.map((dim) => new InputSpec({ shape: [null, dim] }));
    }
    if (this.stateful) {
      this.resetStates();
    }
  }
  /**
   * Reset the state tensors of the RNN.
   *
   * If the `states` argument is `undefined` or `null`, will set the
   * state tensor(s) of the RNN to all-zero tensors of the appropriate
   * shape(s).
   *
   * If `states` is provided, will set the state tensors of the RNN to its
   * value.
   *
   * @param states Optional externally-provided initial states.
   * @param training Whether this call is done during training. For stateful
   *   RNNs, this affects whether the old states are kept or discarded. In
   *   particular, if `training` is `true`, the old states will be kept so
   *   that subsequent backpropgataion through time (BPTT) may work properly.
   *   Else, the old states will be discarded.
   */
  resetStates(states, training = false) {
    tidy(() => {
      if (!this.stateful) {
        throw new AttributeError("Cannot call resetStates() on an RNN Layer that is not stateful.");
      }
      const batchSize = this.inputSpec[0].shape[0];
      if (batchSize == null) {
        throw new ValueError("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");
      }
      if (this.states_ == null) {
        if (Array.isArray(this.cell.stateSize)) {
          this.states_ = this.cell.stateSize.map((dim) => zeros([batchSize, dim]));
        } else {
          this.states_ = [zeros([batchSize, this.cell.stateSize])];
        }
      } else if (states == null) {
        dispose(this.states_);
        if (this.keptStates != null) {
          dispose(this.keptStates);
          this.keptStates = [];
        }
        if (Array.isArray(this.cell.stateSize)) {
          this.states_ = this.cell.stateSize.map((dim) => zeros([batchSize, dim]));
        } else {
          this.states_[0] = zeros([batchSize, this.cell.stateSize]);
        }
      } else {
        if (!Array.isArray(states)) {
          states = [states];
        }
        if (states.length !== this.states_.length) {
          throw new ValueError(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${states.length} state value(s). Input received: ${states}`);
        }
        if (training === true) {
          this.keptStates.push(this.states_.slice());
        } else {
          dispose(this.states_);
        }
        for (let index = 0; index < this.states_.length; ++index) {
          const value = states[index];
          const dim = Array.isArray(this.cell.stateSize) ? this.cell.stateSize[index] : this.cell.stateSize;
          const expectedShape = [batchSize, dim];
          if (!util_exports.arraysEqual(value.shape, expectedShape)) {
            throw new ValueError(`State ${index} is incompatible with layer ${this.name}: expected shape=${expectedShape}, received shape=${value.shape}`);
          }
          this.states_[index] = value;
        }
      }
      this.states_ = this.states_.map((state) => keep(state.clone()));
    });
  }
  apply(inputs, kwargs) {
    let initialState = kwargs == null ? null : kwargs["initialState"];
    let constants = kwargs == null ? null : kwargs["constants"];
    if (kwargs == null) {
      kwargs = {};
    }
    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);
    inputs = standardized.inputs;
    initialState = standardized.initialState;
    constants = standardized.constants;
    let additionalInputs = [];
    let additionalSpecs = [];
    if (initialState != null) {
      kwargs["initialState"] = initialState;
      additionalInputs = additionalInputs.concat(initialState);
      this.stateSpec = [];
      for (const state of initialState) {
        this.stateSpec.push(new InputSpec({ shape: state.shape }));
      }
      additionalSpecs = additionalSpecs.concat(this.stateSpec);
    }
    if (constants != null) {
      kwargs["constants"] = constants;
      additionalInputs = additionalInputs.concat(constants);
      this.numConstants = constants.length;
    }
    const isTensor = additionalInputs[0] instanceof SymbolicTensor;
    if (isTensor) {
      const fullInput = [inputs].concat(additionalInputs);
      const fullInputSpec = this.inputSpec.concat(additionalSpecs);
      const originalInputSpec = this.inputSpec;
      this.inputSpec = fullInputSpec;
      const output = super.apply(fullInput, kwargs);
      this.inputSpec = originalInputSpec;
      return output;
    } else {
      return super.apply(inputs, kwargs);
    }
  }
  // tslint:disable-next-line:no-any
  call(inputs, kwargs) {
    return tidy(() => {
      const mask = kwargs == null ? null : kwargs["mask"];
      const training = kwargs == null ? null : kwargs["training"];
      let initialState = kwargs == null ? null : kwargs["initialState"];
      inputs = getExactlyOneTensor(inputs);
      if (initialState == null) {
        if (this.stateful) {
          initialState = this.states_;
        } else {
          initialState = this.getInitialState(inputs);
        }
      }
      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;
      if (initialState.length !== numStates) {
        throw new ValueError(`RNN Layer has ${numStates} state(s) but was passed ${initialState.length} initial state(s).`);
      }
      if (this.unroll) {
        console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");
      }
      const cellCallKwargs = { training };
      const step2 = (inputs2, states2) => {
        const outputs2 = this.cell.call([inputs2].concat(states2), cellCallKwargs);
        return [outputs2[0], outputs2.slice(1)];
      };
      const rnnOutputs = rnn(step2, inputs, initialState, this.goBackwards, mask, null, this.unroll, this.returnSequences);
      const lastOutput = rnnOutputs[0];
      const outputs = rnnOutputs[1];
      const states = rnnOutputs[2];
      if (this.stateful) {
        this.resetStates(states, training);
      }
      const output = this.returnSequences ? outputs : lastOutput;
      if (this.returnState) {
        return [output].concat(states);
      } else {
        return output;
      }
    });
  }
  getInitialState(inputs) {
    return tidy(() => {
      let initialState = zeros(inputs.shape);
      initialState = sum(initialState, [1, 2]);
      initialState = expandDims2(initialState);
      if (Array.isArray(this.cell.stateSize)) {
        return this.cell.stateSize.map((dim) => dim > 1 ? tile2(initialState, [1, dim]) : initialState);
      } else {
        return this.cell.stateSize > 1 ? [tile2(initialState, [1, this.cell.stateSize])] : [initialState];
      }
    });
  }
  get trainableWeights() {
    if (!this.trainable) {
      return [];
    }
    return this.cell.trainableWeights;
  }
  get nonTrainableWeights() {
    if (!this.trainable) {
      return this.cell.weights;
    }
    return this.cell.nonTrainableWeights;
  }
  setFastWeightInitDuringBuild(value) {
    super.setFastWeightInitDuringBuild(value);
    if (this.cell != null) {
      this.cell.setFastWeightInitDuringBuild(value);
    }
  }
  getConfig() {
    const baseConfig = super.getConfig();
    const config = {
      returnSequences: this.returnSequences,
      returnState: this.returnState,
      goBackwards: this.goBackwards,
      stateful: this.stateful,
      unroll: this.unroll
    };
    if (this.numConstants != null) {
      config["numConstants"] = this.numConstants;
    }
    const cellConfig = this.cell.getConfig();
    if (this.getClassName() === _RNN.className) {
      config["cell"] = {
        "className": this.cell.getClassName(),
        "config": cellConfig
      };
    }
    return Object.assign({}, cellConfig, baseConfig, config);
  }
  /** @nocollapse */
  static fromConfig(cls, config, customObjects = {}) {
    const cellConfig = config["cell"];
    const cell = deserialize(cellConfig, customObjects);
    return new cls(Object.assign(config, { cell }));
  }
};
RNN.className = "RNN";
serialization_exports.registerClass(RNN);
var RNNCell = class extends Layer {
};
var SimpleRNNCell = class extends RNNCell {
  constructor(args) {
    super(args);
    this.DEFAULT_ACTIVATION = "tanh";
    this.DEFAULT_KERNEL_INITIALIZER = "glorotNormal";
    this.DEFAULT_RECURRENT_INITIALIZER = "orthogonal";
    this.DEFAULT_BIAS_INITIALIZER = "zeros";
    this.units = args.units;
    assertPositiveInteger(this.units, `units`);
    this.activation = getActivation(args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);
    this.useBias = args.useBias == null ? true : args.useBias;
    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);
    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);
    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);
    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);
    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);
    this.biasRegularizer = getRegularizer(args.biasRegularizer);
    this.kernelConstraint = getConstraint(args.kernelConstraint);
    this.recurrentConstraint = getConstraint(args.recurrentConstraint);
    this.biasConstraint = getConstraint(args.biasConstraint);
    this.dropout = min([1, max2([0, args.dropout == null ? 0 : args.dropout])]);
    this.recurrentDropout = min([
      1,
      max2([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])
    ]);
    this.dropoutFunc = args.dropoutFunc;
    this.stateSize = this.units;
    this.dropoutMask = null;
    this.recurrentDropoutMask = null;
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    this.kernel = this.addWeight("kernel", [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);
    this.recurrentKernel = this.addWeight("recurrent_kernel", [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);
    if (this.useBias) {
      this.bias = this.addWeight("bias", [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);
    } else {
      this.bias = null;
    }
    this.built = true;
  }
  // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:
  //   `inputs` and `states`. Here, the two tensors are combined into an
  //   `Tensor[]` Array as the first input argument.
  //   Similarly, PyKeras' equivalent of this method returns two values:
  //    `output` and `[output]`. Here the two are combined into one length-2
  //    `Tensor[]`, consisting of `output` repeated.
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = inputs;
      if (inputs.length !== 2) {
        throw new ValueError(`SimpleRNNCell expects 2 input Tensors, got ${inputs.length}.`);
      }
      let prevOutput = inputs[1];
      inputs = inputs[0];
      const training = kwargs["training"] == null ? false : kwargs["training"];
      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {
        this.dropoutMask = generateDropoutMask({
          ones: () => onesLike(inputs),
          rate: this.dropout,
          training,
          dropoutFunc: this.dropoutFunc
        });
      }
      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {
        this.recurrentDropoutMask = generateDropoutMask({
          ones: () => onesLike(prevOutput),
          rate: this.recurrentDropout,
          training,
          dropoutFunc: this.dropoutFunc
        });
      }
      let h;
      const dpMask = this.dropoutMask;
      const recDpMask = this.recurrentDropoutMask;
      if (dpMask != null) {
        h = dot(mul(inputs, dpMask), this.kernel.read());
      } else {
        h = dot(inputs, this.kernel.read());
      }
      if (this.bias != null) {
        h = biasAdd(h, this.bias.read());
      }
      if (recDpMask != null) {
        prevOutput = mul(prevOutput, recDpMask);
      }
      let output = add(h, dot(prevOutput, this.recurrentKernel.read()));
      if (this.activation != null) {
        output = this.activation.apply(output);
      }
      return [output, output];
    });
  }
  getConfig() {
    const baseConfig = super.getConfig();
    const config = {
      units: this.units,
      activation: serializeActivation(this.activation),
      useBias: this.useBias,
      kernelInitializer: serializeInitializer(this.kernelInitializer),
      recurrentInitializer: serializeInitializer(this.recurrentInitializer),
      biasInitializer: serializeInitializer(this.biasInitializer),
      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),
      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),
      biasRegularizer: serializeRegularizer(this.biasRegularizer),
      activityRegularizer: serializeRegularizer(this.activityRegularizer),
      kernelConstraint: serializeConstraint(this.kernelConstraint),
      recurrentConstraint: serializeConstraint(this.recurrentConstraint),
      biasConstraint: serializeConstraint(this.biasConstraint),
      dropout: this.dropout,
      recurrentDropout: this.recurrentDropout
    };
    return Object.assign({}, baseConfig, config);
  }
};
SimpleRNNCell.className = "SimpleRNNCell";
serialization_exports.registerClass(SimpleRNNCell);
var SimpleRNN = class extends RNN {
  constructor(args) {
    args.cell = new SimpleRNNCell(args);
    super(args);
  }
  call(inputs, kwargs) {
    return tidy(() => {
      if (this.cell.dropoutMask != null) {
        dispose(this.cell.dropoutMask);
        this.cell.dropoutMask = null;
      }
      if (this.cell.recurrentDropoutMask != null) {
        dispose(this.cell.recurrentDropoutMask);
        this.cell.recurrentDropoutMask = null;
      }
      const mask = kwargs == null ? null : kwargs["mask"];
      const training = kwargs == null ? null : kwargs["training"];
      const initialState = kwargs == null ? null : kwargs["initialState"];
      return super.call(inputs, { mask, training, initialState });
    });
  }
  /** @nocollapse */
  static fromConfig(cls, config) {
    return new cls(config);
  }
};
SimpleRNN.className = "SimpleRNN";
serialization_exports.registerClass(SimpleRNN);
var GRUCell = class extends RNNCell {
  constructor(args) {
    super(args);
    this.DEFAULT_ACTIVATION = "tanh";
    this.DEFAULT_RECURRENT_ACTIVATION = "hardSigmoid";
    this.DEFAULT_KERNEL_INITIALIZER = "glorotNormal";
    this.DEFAULT_RECURRENT_INITIALIZER = "orthogonal";
    this.DEFAULT_BIAS_INITIALIZER = "zeros";
    if (args.resetAfter) {
      throw new ValueError(`GRUCell does not support reset_after parameter set to true.`);
    }
    this.units = args.units;
    assertPositiveInteger(this.units, "units");
    this.activation = getActivation(args.activation === void 0 ? this.DEFAULT_ACTIVATION : args.activation);
    this.recurrentActivation = getActivation(args.recurrentActivation === void 0 ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);
    this.useBias = args.useBias == null ? true : args.useBias;
    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);
    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);
    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);
    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);
    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);
    this.biasRegularizer = getRegularizer(args.biasRegularizer);
    this.kernelConstraint = getConstraint(args.kernelConstraint);
    this.recurrentConstraint = getConstraint(args.recurrentConstraint);
    this.biasConstraint = getConstraint(args.biasConstraint);
    this.dropout = min([1, max2([0, args.dropout == null ? 0 : args.dropout])]);
    this.recurrentDropout = min([
      1,
      max2([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])
    ]);
    this.dropoutFunc = args.dropoutFunc;
    this.implementation = args.implementation;
    this.stateSize = this.units;
    this.dropoutMask = null;
    this.recurrentDropoutMask = null;
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const inputDim = inputShape[inputShape.length - 1];
    this.kernel = this.addWeight("kernel", [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);
    this.recurrentKernel = this.addWeight("recurrent_kernel", [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);
    if (this.useBias) {
      this.bias = this.addWeight("bias", [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);
    } else {
      this.bias = null;
    }
    this.built = true;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = inputs;
      if (inputs.length !== 2) {
        throw new ValueError(`GRUCell expects 2 input Tensors (inputs, h, c), got ${inputs.length}.`);
      }
      const training = kwargs["training"] == null ? false : kwargs["training"];
      let hTMinus1 = inputs[1];
      inputs = inputs[0];
      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {
        this.dropoutMask = generateDropoutMask({
          ones: () => onesLike(inputs),
          rate: this.dropout,
          training,
          count: 3,
          dropoutFunc: this.dropoutFunc
        });
      }
      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {
        this.recurrentDropoutMask = generateDropoutMask({
          ones: () => onesLike(hTMinus1),
          rate: this.recurrentDropout,
          training,
          count: 3,
          dropoutFunc: this.dropoutFunc
        });
      }
      const dpMask = this.dropoutMask;
      const recDpMask = this.recurrentDropoutMask;
      let z;
      let r;
      let hh;
      if (0 < this.dropout && this.dropout < 1) {
        inputs = mul(inputs, dpMask[0]);
      }
      let matrixX = dot(inputs, this.kernel.read());
      if (this.useBias) {
        matrixX = biasAdd(matrixX, this.bias.read());
      }
      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {
        hTMinus1 = mul(hTMinus1, recDpMask[0]);
      }
      const recurrentKernelValue = this.recurrentKernel.read();
      const [rk1, rk2] = split(recurrentKernelValue, [2 * this.units, this.units], recurrentKernelValue.rank - 1);
      const matrixInner = dot(hTMinus1, rk1);
      const [xZ, xR, xH] = split(matrixX, 3, matrixX.rank - 1);
      const [recurrentZ, recurrentR] = split(matrixInner, 2, matrixInner.rank - 1);
      z = this.recurrentActivation.apply(add(xZ, recurrentZ));
      r = this.recurrentActivation.apply(add(xR, recurrentR));
      const recurrentH = dot(mul(r, hTMinus1), rk2);
      hh = this.activation.apply(add(xH, recurrentH));
      const h = add(mul(z, hTMinus1), mul(add(1, neg(z)), hh));
      return [h, h];
    });
  }
  getConfig() {
    const baseConfig = super.getConfig();
    const config = {
      units: this.units,
      activation: serializeActivation(this.activation),
      recurrentActivation: serializeActivation(this.recurrentActivation),
      useBias: this.useBias,
      kernelInitializer: serializeInitializer(this.kernelInitializer),
      recurrentInitializer: serializeInitializer(this.recurrentInitializer),
      biasInitializer: serializeInitializer(this.biasInitializer),
      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),
      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),
      biasRegularizer: serializeRegularizer(this.biasRegularizer),
      activityRegularizer: serializeRegularizer(this.activityRegularizer),
      kernelConstraint: serializeConstraint(this.kernelConstraint),
      recurrentConstraint: serializeConstraint(this.recurrentConstraint),
      biasConstraint: serializeConstraint(this.biasConstraint),
      dropout: this.dropout,
      recurrentDropout: this.recurrentDropout,
      implementation: this.implementation,
      resetAfter: false
    };
    return Object.assign({}, baseConfig, config);
  }
};
GRUCell.className = "GRUCell";
serialization_exports.registerClass(GRUCell);
var GRU = class extends RNN {
  constructor(args) {
    if (args.implementation === 0) {
      console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.");
    }
    args.cell = new GRUCell(args);
    super(args);
  }
  call(inputs, kwargs) {
    return tidy(() => {
      if (this.cell.dropoutMask != null) {
        dispose(this.cell.dropoutMask);
        this.cell.dropoutMask = null;
      }
      if (this.cell.recurrentDropoutMask != null) {
        dispose(this.cell.recurrentDropoutMask);
        this.cell.recurrentDropoutMask = null;
      }
      const mask = kwargs == null ? null : kwargs["mask"];
      const training = kwargs == null ? null : kwargs["training"];
      const initialState = kwargs == null ? null : kwargs["initialState"];
      return super.call(inputs, { mask, training, initialState });
    });
  }
  /** @nocollapse */
  static fromConfig(cls, config) {
    if (config["implmentation"] === 0) {
      config["implementation"] = 1;
    }
    return new cls(config);
  }
};
GRU.className = "GRU";
serialization_exports.registerClass(GRU);
var LSTMCell = class extends RNNCell {
  constructor(args) {
    super(args);
    this.DEFAULT_ACTIVATION = "tanh";
    this.DEFAULT_RECURRENT_ACTIVATION = "hardSigmoid";
    this.DEFAULT_KERNEL_INITIALIZER = "glorotNormal";
    this.DEFAULT_RECURRENT_INITIALIZER = "orthogonal";
    this.DEFAULT_BIAS_INITIALIZER = "zeros";
    this.units = args.units;
    assertPositiveInteger(this.units, "units");
    this.activation = getActivation(args.activation === void 0 ? this.DEFAULT_ACTIVATION : args.activation);
    this.recurrentActivation = getActivation(args.recurrentActivation === void 0 ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);
    this.useBias = args.useBias == null ? true : args.useBias;
    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);
    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);
    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);
    this.unitForgetBias = args.unitForgetBias;
    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);
    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);
    this.biasRegularizer = getRegularizer(args.biasRegularizer);
    this.kernelConstraint = getConstraint(args.kernelConstraint);
    this.recurrentConstraint = getConstraint(args.recurrentConstraint);
    this.biasConstraint = getConstraint(args.biasConstraint);
    this.dropout = min([1, max2([0, args.dropout == null ? 0 : args.dropout])]);
    this.recurrentDropout = min([
      1,
      max2([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])
    ]);
    this.dropoutFunc = args.dropoutFunc;
    this.implementation = args.implementation;
    this.stateSize = [this.units, this.units];
    this.dropoutMask = null;
    this.recurrentDropoutMask = null;
  }
  build(inputShape) {
    var _a;
    inputShape = getExactlyOneShape(inputShape);
    const inputDim = inputShape[inputShape.length - 1];
    this.kernel = this.addWeight("kernel", [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);
    this.recurrentKernel = this.addWeight("recurrent_kernel", [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);
    let biasInitializer;
    if (this.useBias) {
      if (this.unitForgetBias) {
        const capturedBiasInit = this.biasInitializer;
        const capturedUnits = this.units;
        biasInitializer = new (_a = class CustomInit extends Initializer {
          apply(shape, dtype) {
            const bI = capturedBiasInit.apply([capturedUnits]);
            const bF = new Ones().apply([capturedUnits]);
            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);
            return concatAlongFirstAxis(concatAlongFirstAxis(bI, bF), bCAndH);
          }
        }, /** @nocollapse */
        _a.className = "CustomInit", _a)();
      } else {
        biasInitializer = this.biasInitializer;
      }
      this.bias = this.addWeight("bias", [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);
    } else {
      this.bias = null;
    }
    this.built = true;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      const training = kwargs["training"] == null ? false : kwargs["training"];
      inputs = inputs;
      if (inputs.length !== 3) {
        throw new ValueError(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${inputs.length}.`);
      }
      let hTMinus1 = inputs[1];
      const cTMinus1 = inputs[2];
      inputs = inputs[0];
      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {
        this.dropoutMask = generateDropoutMask({
          ones: () => onesLike(inputs),
          rate: this.dropout,
          training,
          count: 4,
          dropoutFunc: this.dropoutFunc
        });
      }
      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {
        this.recurrentDropoutMask = generateDropoutMask({
          ones: () => onesLike(hTMinus1),
          rate: this.recurrentDropout,
          training,
          count: 4,
          dropoutFunc: this.dropoutFunc
        });
      }
      const dpMask = this.dropoutMask;
      const recDpMask = this.recurrentDropoutMask;
      let i;
      let f;
      let c;
      let o;
      if (0 < this.dropout && this.dropout < 1) {
        inputs = mul(inputs, dpMask[0]);
      }
      let z = dot(inputs, this.kernel.read());
      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {
        hTMinus1 = mul(hTMinus1, recDpMask[0]);
      }
      z = add(z, dot(hTMinus1, this.recurrentKernel.read()));
      if (this.useBias) {
        z = biasAdd(z, this.bias.read());
      }
      const [z0, z1, z2, z3] = split(z, 4, z.rank - 1);
      i = this.recurrentActivation.apply(z0);
      f = this.recurrentActivation.apply(z1);
      c = add(mul(f, cTMinus1), mul(i, this.activation.apply(z2)));
      o = this.recurrentActivation.apply(z3);
      const h = mul(o, this.activation.apply(c));
      return [h, h, c];
    });
  }
  getConfig() {
    const baseConfig = super.getConfig();
    const config = {
      units: this.units,
      activation: serializeActivation(this.activation),
      recurrentActivation: serializeActivation(this.recurrentActivation),
      useBias: this.useBias,
      kernelInitializer: serializeInitializer(this.kernelInitializer),
      recurrentInitializer: serializeInitializer(this.recurrentInitializer),
      biasInitializer: serializeInitializer(this.biasInitializer),
      unitForgetBias: this.unitForgetBias,
      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),
      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),
      biasRegularizer: serializeRegularizer(this.biasRegularizer),
      activityRegularizer: serializeRegularizer(this.activityRegularizer),
      kernelConstraint: serializeConstraint(this.kernelConstraint),
      recurrentConstraint: serializeConstraint(this.recurrentConstraint),
      biasConstraint: serializeConstraint(this.biasConstraint),
      dropout: this.dropout,
      recurrentDropout: this.recurrentDropout,
      implementation: this.implementation
    };
    return Object.assign({}, baseConfig, config);
  }
};
LSTMCell.className = "LSTMCell";
serialization_exports.registerClass(LSTMCell);
var LSTM = class extends RNN {
  constructor(args) {
    if (args.implementation === 0) {
      console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.");
    }
    args.cell = new LSTMCell(args);
    super(args);
  }
  call(inputs, kwargs) {
    return tidy(() => {
      if (this.cell.dropoutMask != null) {
        dispose(this.cell.dropoutMask);
        this.cell.dropoutMask = null;
      }
      if (this.cell.recurrentDropoutMask != null) {
        dispose(this.cell.recurrentDropoutMask);
        this.cell.recurrentDropoutMask = null;
      }
      const mask = kwargs == null ? null : kwargs["mask"];
      const training = kwargs == null ? null : kwargs["training"];
      const initialState = kwargs == null ? null : kwargs["initialState"];
      return super.call(inputs, { mask, training, initialState });
    });
  }
  /** @nocollapse */
  static fromConfig(cls, config) {
    if (config["implmentation"] === 0) {
      config["implementation"] = 1;
    }
    return new cls(config);
  }
};
LSTM.className = "LSTM";
serialization_exports.registerClass(LSTM);
var StackedRNNCells = class extends RNNCell {
  constructor(args) {
    super(args);
    this.cells = args.cells;
  }
  get stateSize() {
    const stateSize = [];
    for (const cell of this.cells.slice().reverse()) {
      if (Array.isArray(cell.stateSize)) {
        stateSize.push(...cell.stateSize);
      } else {
        stateSize.push(cell.stateSize);
      }
    }
    return stateSize;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = inputs;
      let states = inputs.slice(1);
      const nestedStates = [];
      for (const cell of this.cells.slice().reverse()) {
        if (Array.isArray(cell.stateSize)) {
          nestedStates.push(states.splice(0, cell.stateSize.length));
        } else {
          nestedStates.push(states.splice(0, 1));
        }
      }
      nestedStates.reverse();
      const newNestedStates = [];
      let callInputs;
      for (let i = 0; i < this.cells.length; ++i) {
        const cell = this.cells[i];
        states = nestedStates[i];
        if (i === 0) {
          callInputs = [inputs[0]].concat(states);
        } else {
          callInputs = [callInputs[0]].concat(states);
        }
        callInputs = cell.call(callInputs, kwargs);
        newNestedStates.push(callInputs.slice(1));
      }
      states = [];
      for (const cellStates of newNestedStates.slice().reverse()) {
        states.push(...cellStates);
      }
      return [callInputs[0]].concat(states);
    });
  }
  build(inputShape) {
    if (isArrayOfShapes(inputShape)) {
      inputShape = inputShape[0];
    }
    inputShape = inputShape;
    let outputDim;
    this.cells.forEach((cell, i) => {
      nameScope(`RNNCell_${i}`, () => {
        cell.build(inputShape);
        if (Array.isArray(cell.stateSize)) {
          outputDim = cell.stateSize[0];
        } else {
          outputDim = cell.stateSize;
        }
        inputShape = [inputShape[0], outputDim];
      });
    });
    this.built = true;
  }
  getConfig() {
    const baseConfig = super.getConfig();
    const getCellConfig = (cell) => {
      return {
        "className": cell.getClassName(),
        "config": cell.getConfig()
      };
    };
    const cellConfigs = this.cells.map(getCellConfig);
    const config = { "cells": cellConfigs };
    return Object.assign({}, baseConfig, config);
  }
  /** @nocollapse */
  static fromConfig(cls, config, customObjects = {}) {
    const cells = [];
    for (const cellConfig of config["cells"]) {
      cells.push(deserialize(cellConfig, customObjects));
    }
    return new cls({ cells });
  }
  get trainableWeights() {
    if (!this.trainable) {
      return [];
    }
    const weights = [];
    for (const cell of this.cells) {
      weights.push(...cell.trainableWeights);
    }
    return weights;
  }
  get nonTrainableWeights() {
    const weights = [];
    for (const cell of this.cells) {
      weights.push(...cell.nonTrainableWeights);
    }
    if (!this.trainable) {
      const trainableWeights = [];
      for (const cell of this.cells) {
        trainableWeights.push(...cell.trainableWeights);
      }
      return trainableWeights.concat(weights);
    }
    return weights;
  }
  /**
   * Retrieve the weights of a the model.
   *
   * @returns A flat `Array` of `tf.Tensor`s.
   */
  getWeights() {
    const weights = [];
    for (const cell of this.cells) {
      weights.push(...cell.weights);
    }
    return batchGetValue(weights);
  }
  /**
   * Set the weights of the model.
   *
   * @param weights An `Array` of `tf.Tensor`s with shapes and types matching
   *     the output of `getWeights()`.
   */
  setWeights(weights) {
    const tuples = [];
    for (const cell of this.cells) {
      const numParams = cell.weights.length;
      const inputWeights = weights.splice(numParams);
      for (let i = 0; i < cell.weights.length; ++i) {
        tuples.push([cell.weights[i], inputWeights[i]]);
      }
    }
    batchSetValue(tuples);
  }
};
StackedRNNCells.className = "StackedRNNCells";
serialization_exports.registerClass(StackedRNNCells);
function generateDropoutMask(args) {
  const { ones: ones3, rate, training = false, count: count2 = 1, dropoutFunc } = args;
  const droppedInputs = () => dropoutFunc != null ? dropoutFunc(ones3(), rate) : dropout2(ones3(), rate);
  const createMask = () => inTrainPhase(droppedInputs, ones3, training);
  if (!count2 || count2 <= 1) {
    return keep(createMask().clone());
  }
  const masks = Array(count2).fill(void 0).map(createMask);
  return masks.map((m) => keep(m.clone()));
}

// node_modules/@tensorflow/tfjs-layers/dist/layers/convolutional_recurrent.js
var __rest = function(s, e) {
  var t = {};
  for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)
    t[p] = s[p];
  if (s != null && typeof Object.getOwnPropertySymbols === "function")
    for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {
      if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i]))
        t[p[i]] = s[p[i]];
    }
  return t;
};
var ConvRNN2D = class extends RNN {
  constructor(args) {
    if (args.unroll) {
      throw new NotImplementedError("Unrolling is not possible with convolutional RNNs.");
    }
    if (Array.isArray(args.cell)) {
      throw new NotImplementedError("It is not possible at the moment to stack convolutional cells.");
    }
    super(args);
    this.inputSpec = [new InputSpec({ ndim: 5 })];
  }
  call(inputs, kwargs) {
    return tidy(() => {
      if (this.cell.dropoutMask != null) {
        dispose(this.cell.dropoutMask);
        this.cell.dropoutMask = null;
      }
      if (this.cell.recurrentDropoutMask != null) {
        dispose(this.cell.recurrentDropoutMask);
        this.cell.recurrentDropoutMask = null;
      }
      if (kwargs && kwargs["constants"]) {
        throw new ValueError("ConvRNN2D cell does not support constants");
      }
      const mask = kwargs == null ? null : kwargs["mask"];
      const training = kwargs == null ? null : kwargs["training"];
      const initialState = kwargs == null ? null : kwargs["initialState"];
      return super.call(inputs, { mask, training, initialState });
    });
  }
  computeOutputShape(inputShape) {
    let outShape = this.computeSingleOutputShape(inputShape);
    if (!this.returnSequences) {
      outShape = [outShape[0], ...outShape.slice(2)];
    }
    if (this.returnState) {
      outShape = [outShape, ...Array(2).fill([inputShape[0], ...outShape.slice(-3)])];
    }
    return outShape;
  }
  getInitialState(inputs) {
    return tidy(() => {
      const { stateSize } = this.cell;
      const inputShape = inputs.shape;
      const outputShape = this.computeSingleOutputShape(inputShape);
      const stateShape = [outputShape[0], ...outputShape.slice(2)];
      const initialState = zeros(stateShape);
      if (Array.isArray(stateSize)) {
        return Array(stateSize.length).fill(initialState);
      }
      return [initialState];
    });
  }
  resetStates(states, training = false) {
    tidy(() => {
      if (!this.stateful) {
        throw new AttributeError("Cannot call resetStates() on an RNN Layer that is not stateful.");
      }
      const inputShape = this.inputSpec[0].shape;
      const outputShape = this.computeSingleOutputShape(inputShape);
      const stateShape = [outputShape[0], ...outputShape.slice(2)];
      const batchSize = inputShape[0];
      if (batchSize == null) {
        throw new ValueError("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");
      }
      if (this.getStates() == null) {
        if (Array.isArray(this.cell.stateSize)) {
          this.states_ = this.cell.stateSize.map(() => zeros(stateShape));
        } else {
          this.states_ = [zeros(stateShape)];
        }
      } else if (states == null) {
        dispose(this.states_);
        if (this.keptStates != null) {
          dispose(this.keptStates);
          this.keptStates = [];
        }
        if (Array.isArray(this.cell.stateSize)) {
          this.states_ = this.cell.stateSize.map(() => zeros(stateShape));
        } else {
          this.states_[0] = zeros(stateShape);
        }
      } else {
        if (!Array.isArray(states)) {
          states = [states];
        }
        if (states.length !== this.states_.length) {
          throw new ValueError(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${states.length} state value(s). Input received: ${states}`);
        }
        if (training) {
          this.keptStates.push(this.states_.slice());
        } else {
          dispose(this.states_);
        }
        for (let index = 0; index < this.states_.length; ++index) {
          const value = states[index];
          const expectedShape = stateShape;
          if (!util_exports.arraysEqual(value.shape, expectedShape)) {
            throw new ValueError(`State ${index} is incompatible with layer ${this.name}: expected shape=${expectedShape}, received shape=${value.shape}`);
          }
          this.states_[index] = value;
        }
      }
      this.states_ = this.states_.map((state) => keep(state.clone()));
    });
  }
  computeSingleOutputShape(inputShape) {
    const { dataFormat, filters, kernelSize, padding, strides, dilationRate } = this.cell;
    const isChannelsFirst = dataFormat === "channelsFirst";
    const h = inputShape[isChannelsFirst ? 3 : 2];
    const w = inputShape[isChannelsFirst ? 4 : 3];
    const hOut = convOutputLength(h, kernelSize[0], padding, strides[0], dilationRate[0]);
    const wOut = convOutputLength(w, kernelSize[1], padding, strides[1], dilationRate[1]);
    const outShape = [
      ...inputShape.slice(0, 2),
      ...isChannelsFirst ? [filters, hOut, wOut] : [hOut, wOut, filters]
    ];
    return outShape;
  }
};
ConvRNN2D.className = "ConvRNN2D";
var ConvLSTM2DCell = class extends LSTMCell {
  constructor(args) {
    const { filters, kernelSize, strides, padding, dataFormat, dilationRate } = args;
    super(Object.assign({}, args, { units: filters }));
    this.filters = filters;
    assertPositiveInteger(this.filters, "filters");
    this.kernelSize = normalizeArray(kernelSize, 2, "kernelSize");
    this.kernelSize.forEach((size) => assertPositiveInteger(size, "kernelSize"));
    this.strides = normalizeArray(strides || 1, 2, "strides");
    this.strides.forEach((stride) => assertPositiveInteger(stride, "strides"));
    this.padding = padding || "valid";
    checkPaddingMode(this.padding);
    this.dataFormat = dataFormat || "channelsLast";
    checkDataFormat(this.dataFormat);
    this.dilationRate = normalizeArray(dilationRate || 1, 2, "dilationRate");
    this.dilationRate.forEach((rate) => assertPositiveInteger(rate, "dilationRate"));
  }
  build(inputShape) {
    var _a;
    inputShape = getExactlyOneShape(inputShape);
    const channelAxis = this.dataFormat === "channelsFirst" ? 1 : inputShape.length - 1;
    if (inputShape[channelAxis] == null) {
      throw new ValueError(`The channel dimension of the input should be defined. Found ${inputShape[channelAxis]}`);
    }
    const inputDim = inputShape[channelAxis];
    const numOfKernels = 4;
    const kernelShape = this.kernelSize.concat([inputDim, this.filters * numOfKernels]);
    this.kernel = this.addWeight("kernel", kernelShape, null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);
    const recurrentKernelShape = this.kernelSize.concat([this.filters, this.filters * numOfKernels]);
    this.recurrentKernel = this.addWeight("recurrent_kernel", recurrentKernelShape, null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);
    if (this.useBias) {
      let biasInitializer;
      if (this.unitForgetBias) {
        const init = this.biasInitializer;
        const filters = this.filters;
        biasInitializer = new (_a = class CustomInit extends Initializer {
          apply(shape, dtype) {
            const biasI = init.apply([filters]);
            const biasF = ones([filters]);
            const biasCAndO = init.apply([filters * 2]);
            return concatenate([biasI, biasF, biasCAndO]);
          }
        }, /** @nocollapse */
        _a.className = "CustomInit", _a)();
      } else {
        biasInitializer = this.biasInitializer;
      }
      this.bias = this.addWeight("bias", [this.filters * numOfKernels], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);
    }
    this.built = true;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      if (inputs.length !== 3) {
        throw new ValueError(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${inputs.length}.`);
      }
      const training = kwargs["training"] || false;
      const x = inputs[0];
      const hTMinus1 = inputs[1];
      const cTMinus1 = inputs[2];
      const numOfKernels = 4;
      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {
        this.dropoutMask = generateDropoutMask({
          ones: () => onesLike(x),
          rate: this.dropout,
          training,
          count: numOfKernels,
          dropoutFunc: this.dropoutFunc
        });
      }
      const dropoutMask = this.dropoutMask;
      const applyDropout = (x2, mask, index) => {
        if (!mask || !mask[index]) {
          return x2;
        }
        return mul(mask[index], x2);
      };
      let xI = applyDropout(x, dropoutMask, 0);
      let xF = applyDropout(x, dropoutMask, 1);
      let xC = applyDropout(x, dropoutMask, 2);
      let xO = applyDropout(x, dropoutMask, 3);
      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {
        this.recurrentDropoutMask = generateDropoutMask({
          ones: () => onesLike(hTMinus1),
          rate: this.recurrentDropout,
          training,
          count: numOfKernels,
          dropoutFunc: this.dropoutFunc
        });
      }
      const recDropoutMask = this.recurrentDropoutMask;
      let hI = applyDropout(hTMinus1, recDropoutMask, 0);
      let hF = applyDropout(hTMinus1, recDropoutMask, 1);
      let hC = applyDropout(hTMinus1, recDropoutMask, 2);
      let hO = applyDropout(hTMinus1, recDropoutMask, 3);
      const kernelChannelAxis = 3;
      const [kernelI, kernelF, kernelC, kernelO] = split(this.kernel.read(), numOfKernels, kernelChannelAxis);
      const [biasI, biasF, biasC, biasO] = this.useBias ? split(this.bias.read(), numOfKernels) : [null, null, null, null];
      xI = this.inputConv(xI, kernelI, biasI, this.padding);
      xF = this.inputConv(xF, kernelF, biasF, this.padding);
      xC = this.inputConv(xC, kernelC, biasC, this.padding);
      xO = this.inputConv(xO, kernelO, biasO, this.padding);
      const [recKernelI, recKernelF, recKernelC, recKernelO] = split(this.recurrentKernel.read(), numOfKernels, kernelChannelAxis);
      hI = this.recurrentConv(hI, recKernelI);
      hF = this.recurrentConv(hF, recKernelF);
      hC = this.recurrentConv(hC, recKernelC);
      hO = this.recurrentConv(hO, recKernelO);
      const i = this.recurrentActivation.apply(add(xI, hI));
      const f = this.recurrentActivation.apply(add(xF, hF));
      const c = add(mul(f, cTMinus1), mul(i, this.activation.apply(add(xC, hC))));
      const h = mul(this.recurrentActivation.apply(add(xO, hO)), this.activation.apply(c));
      return [h, h, c];
    });
  }
  getConfig() {
    const _a = super.getConfig(), { "units": _ } = _a, baseConfig = __rest(_a, ["units"]);
    const config = {
      filters: this.filters,
      kernelSize: this.kernelSize,
      padding: this.padding,
      dataFormat: this.dataFormat,
      dilationRate: this.dilationRate,
      strides: this.strides
    };
    return Object.assign({}, baseConfig, config);
  }
  inputConv(x, w, b, padding) {
    const out = conv2d(x, w, this.strides, padding || "valid", this.dataFormat === "channelsFirst" ? "NCHW" : "NHWC", this.dilationRate);
    if (b) {
      return biasAdd(out, b, this.dataFormat);
    }
    return out;
  }
  recurrentConv(x, w) {
    const strides = 1;
    return conv2d(x, w, strides, "same", this.dataFormat === "channelsFirst" ? "NCHW" : "NHWC");
  }
};
ConvLSTM2DCell.className = "ConvLSTM2DCell";
serialization_exports.registerClass(ConvLSTM2DCell);
var ConvLSTM2D = class extends ConvRNN2D {
  constructor(args) {
    const cell = new ConvLSTM2DCell(args);
    super(Object.assign({}, args, { cell }));
  }
  /** @nocollapse */
  static fromConfig(cls, config) {
    return new cls(config);
  }
};
ConvLSTM2D.className = "ConvLSTM2D";
serialization_exports.registerClass(ConvLSTM2D);

// node_modules/@tensorflow/tfjs-layers/dist/layers/core.js
var Dropout = class extends Layer {
  constructor(args) {
    super(args);
    this.rate = Math.max(Math.min(args.rate, 1), 0);
    this.noiseShape = args.noiseShape;
    this.seed = args.seed;
    this.supportsMasking = true;
  }
  getNoiseShape(input2) {
    if (this.noiseShape == null) {
      return this.noiseShape;
    }
    const inputShape = input2.shape;
    const noiseShape = [];
    for (let i = 0; i < this.noiseShape.length; ++i) {
      noiseShape.push(this.noiseShape[i] == null ? inputShape[i] : this.noiseShape[i]);
    }
    return noiseShape;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      const input2 = getExactlyOneTensor(inputs);
      if (0 < this.rate && this.rate < 1) {
        const training = kwargs["training"] == null ? false : kwargs["training"];
        const noiseShape = this.getNoiseShape(input2);
        const output = inTrainPhase(() => dropout2(input2, this.rate, noiseShape, this.seed), () => input2, training);
        return output;
      }
      return inputs;
    });
  }
  getConfig() {
    const config = {
      rate: this.rate,
      noiseShape: this.noiseShape,
      seed: this.seed
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
  dispose() {
    return super.dispose();
  }
};
Dropout.className = "Dropout";
serialization_exports.registerClass(Dropout);
var SpatialDropout1D = class extends Dropout {
  constructor(args) {
    super(args);
    this.inputSpec = [{ ndim: 3 }];
  }
  getNoiseShape(input2) {
    const inputShape = input2.shape;
    return [inputShape[0], 1, inputShape[2]];
  }
};
SpatialDropout1D.className = "SpatialDropout1D";
serialization_exports.registerClass(SpatialDropout1D);
var Dense = class extends Layer {
  constructor(args) {
    super(args);
    this.activation = null;
    this.useBias = true;
    this.kernel = null;
    this.bias = null;
    this.DEFAULT_KERNEL_INITIALIZER = "glorotNormal";
    this.DEFAULT_BIAS_INITIALIZER = "zeros";
    if (args.batchInputShape == null && args.inputShape == null && args.inputDim != null) {
      let batchSize = null;
      if (args.batchSize != null) {
        batchSize = args.batchSize;
      }
      this.batchInputShape = [batchSize, args.inputDim];
    }
    this.units = args.units;
    assertPositiveInteger(this.units, "units");
    this.activation = getActivation(args.activation);
    if (args.useBias != null) {
      this.useBias = args.useBias;
    }
    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);
    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);
    this.kernelConstraint = getConstraint(args.kernelConstraint);
    this.biasConstraint = getConstraint(args.biasConstraint);
    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);
    this.biasRegularizer = getRegularizer(args.biasRegularizer);
    this.activityRegularizer = getRegularizer(args.activityRegularizer);
    this.supportsMasking = true;
    this.inputSpec = [{ minNDim: 2 }];
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const inputLastDim = inputShape[inputShape.length - 1];
    if (this.kernel == null) {
      this.kernel = this.addWeight("kernel", [inputLastDim, this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);
      if (this.useBias) {
        this.bias = this.addWeight("bias", [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);
      }
    }
    this.inputSpec = [{ minNDim: 2, axes: { [-1]: inputLastDim } }];
    this.built = true;
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const outputShape = inputShape.slice();
    outputShape[outputShape.length - 1] = this.units;
    return outputShape;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      const input2 = getExactlyOneTensor(inputs);
      const fusedActivationName = mapActivationToFusedKernel(this.activation.getClassName());
      let output;
      if (fusedActivationName != null) {
        output = dot(input2, this.kernel.read(), fusedActivationName, this.bias ? this.bias.read() : null);
      } else {
        output = dot(input2, this.kernel.read());
        if (this.bias != null) {
          output = biasAdd(output, this.bias.read());
        }
        if (this.activation != null) {
          output = this.activation.apply(output);
        }
      }
      return output;
    });
  }
  getConfig() {
    const config = {
      units: this.units,
      activation: serializeActivation(this.activation),
      useBias: this.useBias,
      kernelInitializer: serializeInitializer(this.kernelInitializer),
      biasInitializer: serializeInitializer(this.biasInitializer),
      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),
      biasRegularizer: serializeRegularizer(this.biasRegularizer),
      activityRegularizer: serializeRegularizer(this.activityRegularizer),
      kernelConstraint: serializeConstraint(this.kernelConstraint),
      biasConstraint: serializeConstraint(this.biasConstraint)
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Dense.className = "Dense";
serialization_exports.registerClass(Dense);
var Flatten = class extends Layer {
  constructor(args) {
    args = args || {};
    super(args);
    this.inputSpec = [{ minNDim: 3 }];
    this.dataFormat = args.dataFormat;
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    for (const dim of inputShape.slice(1)) {
      if (dim == null) {
        throw new ValueError(`The shape of the input to "Flatten" is not fully defined (got ${inputShape.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);
      }
    }
    return [inputShape[0], arrayProd(inputShape, 1)];
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      let input2 = getExactlyOneTensor(inputs);
      if (this.dataFormat === "channelsFirst" && input2.rank > 1) {
        const permutation = [0];
        for (let i = 2; i < input2.rank; ++i) {
          permutation.push(i);
        }
        permutation.push(1);
        input2 = transpose(input2, permutation);
      }
      return batchFlatten(input2);
    });
  }
  getConfig() {
    const config = {};
    if (this.dataFormat != null) {
      config["dataFormat"] = this.dataFormat;
    }
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Flatten.className = "Flatten";
serialization_exports.registerClass(Flatten);
var Activation2 = class extends Layer {
  constructor(args) {
    super(args);
    this.supportsMasking = true;
    this.activation = getActivation(args.activation);
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      const input2 = getExactlyOneTensor(inputs);
      return this.activation.apply(input2);
    });
  }
  getConfig() {
    const config = { activation: serializeActivation(this.activation) };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Activation2.className = "Activation";
serialization_exports.registerClass(Activation2);
var RepeatVector = class extends Layer {
  constructor(args) {
    super(args);
    this.n = args.n;
    this.inputSpec = [{ ndim: 2 }];
  }
  computeOutputShape(inputShape) {
    return [inputShape[0], this.n, inputShape[1]];
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = getExactlyOneTensor(inputs);
      return repeat(inputs, this.n);
    });
  }
  getConfig() {
    const config = {
      n: this.n
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
RepeatVector.className = "RepeatVector";
serialization_exports.registerClass(RepeatVector);
var Reshape2 = class extends Layer {
  constructor(args) {
    super(args);
    this.targetShape = args.targetShape;
    for (let i = 0; i < this.targetShape.length; ++i) {
      if (this.isUnknown(this.targetShape[i])) {
        this.targetShape[i] = null;
      }
    }
  }
  isUnknown(dim) {
    return dim < 0 || dim == null;
  }
  /**
   * Finds and replaces a missing dimension in output shape.
   *
   * This is a near direct port of the internal Numpy function
   * `_fix_unknown_dimension` in `numpy/core/src/multiarray/shape.c`.
   *
   * @param inputShape: Original shape of array begin reshape.
   * @param outputShape: Target shape of the array, with at most a single
   * `null` or negative number, which indicates an underdetermined dimension
   * that should be derived from `inputShape` and the known dimensions of
   *   `outputShape`.
   * @returns: The output shape with `null` replaced with its computed value.
   * @throws: ValueError: If `inputShape` and `outputShape` do not match.
   */
  fixUnknownDimension(inputShape, outputShape) {
    const errorMsg = "Total size of new array must be unchanged.";
    const finalShape = outputShape.slice();
    let known = 1;
    let unknown = null;
    for (let i = 0; i < finalShape.length; ++i) {
      const dim = finalShape[i];
      if (this.isUnknown(dim)) {
        if (unknown === null) {
          unknown = i;
        } else {
          throw new ValueError("Can only specifiy one unknown dimension.");
        }
      } else {
        known *= dim;
      }
    }
    const originalSize = arrayProd(inputShape);
    if (unknown !== null) {
      if (known === 0 || originalSize % known !== 0) {
        throw new ValueError(errorMsg);
      }
      finalShape[unknown] = originalSize / known;
    } else if (originalSize !== known) {
      throw new ValueError(errorMsg);
    }
    return finalShape;
  }
  computeOutputShape(inputShape) {
    let anyUnknownDims = false;
    for (let i = 0; i < inputShape.length; ++i) {
      if (this.isUnknown(inputShape[i])) {
        anyUnknownDims = true;
        break;
      }
    }
    if (anyUnknownDims) {
      return inputShape.slice(0, 1).concat(this.targetShape);
    } else {
      return inputShape.slice(0, 1).concat(this.fixUnknownDimension(inputShape.slice(1), this.targetShape));
    }
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      const input2 = getExactlyOneTensor(inputs);
      const inputShape = input2.shape;
      const outputShape = inputShape.slice(0, 1).concat(this.fixUnknownDimension(inputShape.slice(1), this.targetShape));
      return reshape(input2, outputShape);
    });
  }
  getConfig() {
    const config = {
      targetShape: this.targetShape
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Reshape2.className = "Reshape";
serialization_exports.registerClass(Reshape2);
var Permute = class extends Layer {
  constructor(args) {
    super(args);
    if (args.dims == null) {
      throw new Error("Required configuration field `dims` is missing during Permute constructor call.");
    }
    if (!Array.isArray(args.dims)) {
      throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${args.dims} instead.`);
    }
    const expectedSortedIndices = range(1, args.dims.length + 1);
    if (!util_exports.arraysEqual(args.dims.slice().sort(), expectedSortedIndices)) {
      throw new Error("Invalid permutation `dims`: " + JSON.stringify(args.dims) + " `dims` must contain consecutive integers starting from 1.");
    }
    this.dims = args.dims;
    this.dimsIncludingBatch = [0].concat(this.dims);
    this.inputSpec = [new InputSpec({ ndim: this.dims.length + 1 })];
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const outputShape = inputShape.slice();
    this.dims.forEach((dim, i) => {
      outputShape[i + 1] = inputShape[dim];
    });
    return outputShape;
  }
  call(inputs, kwargs) {
    return transpose(getExactlyOneTensor(inputs), this.dimsIncludingBatch);
  }
  getConfig() {
    const config = {
      dims: this.dims
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Permute.className = "Permute";
serialization_exports.registerClass(Permute);
var Masking = class extends Layer {
  constructor(args) {
    super(args == null ? {} : args);
    this.supportsMasking = true;
    if (args != null) {
      this.maskValue = args.maskValue == null ? 0 : args.maskValue;
    } else {
      this.maskValue = 0;
    }
  }
  computeOutputShape(inputShape) {
    return inputShape;
  }
  getConfig() {
    const baseConfig = super.getConfig();
    const config = { maskValue: this.maskValue };
    Object.assign(config, baseConfig);
    return config;
  }
  computeMask(inputs, mask) {
    const input2 = getExactlyOneTensor(inputs);
    const axis = -1;
    return any(notEqual(input2, this.maskValue), axis);
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      const input2 = getExactlyOneTensor(inputs);
      const axis = -1;
      const keepDims = true;
      const booleanMask = any(notEqual(input2, this.maskValue), axis, keepDims);
      const output = mul(input2, cast(booleanMask, input2.dtype));
      return output;
    });
  }
};
Masking.className = "Masking";
serialization_exports.registerClass(Masking);

// node_modules/@tensorflow/tfjs-layers/dist/layers/embeddings.js
var Embedding = class extends Layer {
  constructor(args) {
    super(args);
    this.embeddings = null;
    this.DEFAULT_EMBEDDINGS_INITIALIZER = "randomUniform";
    if (args.batchInputShape == null && args.inputShape == null) {
      let batchSize = null;
      if (args.batchSize != null) {
        batchSize = args.batchSize;
      }
      if (args.inputLength == null) {
        this.batchInputShape = [batchSize, null];
      } else {
        this.batchInputShape = [batchSize].concat(toList(args.inputLength));
      }
    }
    this.inputDim = args.inputDim;
    assertPositiveInteger(this.inputDim, "inputDim");
    this.outputDim = args.outputDim;
    assertPositiveInteger(this.outputDim, "outputDim");
    this.embeddingsInitializer = getInitializer(args.embeddingsInitializer || this.DEFAULT_EMBEDDINGS_INITIALIZER);
    this.embeddingsRegularizer = getRegularizer(args.embeddingsRegularizer);
    this.activityRegularizer = getRegularizer(args.activityRegularizer);
    this.embeddingsConstraint = getConstraint(args.embeddingsConstraint);
    this.maskZero = args.maskZero;
    this.supportsMasking = args.maskZero;
    this.inputLength = args.inputLength;
  }
  build(inputShape) {
    this.embeddings = this.addWeight("embeddings", [this.inputDim, this.outputDim], this.dtype, this.embeddingsInitializer, this.embeddingsRegularizer, true, this.embeddingsConstraint);
    this.built = true;
  }
  // Override warnOnIncompatibleInputShape because an embedding layer allows
  // the input to have varying ranks.
  warnOnIncompatibleInputShape(inputShape) {
  }
  computeMask(inputs, mask) {
    return tidy(() => {
      if (!this.maskZero) {
        return null;
      } else {
        inputs = getExactlyOneTensor(inputs);
        return notEqual(inputs, zerosLike(inputs));
      }
    });
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    if (this.inputLength == null) {
      return [...inputShape, this.outputDim];
    }
    const inLens = toList(this.inputLength);
    if (inLens.length !== inputShape.length - 1) {
      throw new ValueError(`"inputLength" is ${this.inputLength}, but received input shape has shape ${inputShape}`);
    } else {
      let i = 0;
      for (let k = 0; k < inLens.length; ++k) {
        const s1 = inLens[k];
        const s2 = inputShape[k + 1];
        if (s1 != null && s2 != null && s1 !== s2) {
          throw new ValueError(`"inputLength" is ${this.inputLength}, but received input shape has shape ${inputShape}`);
        } else if (s1 == null) {
          inLens[i] = s2;
        }
        i++;
      }
    }
    return [inputShape[0], ...inLens, this.outputDim];
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      let input2 = getExactlyOneTensor(inputs);
      if (input2.dtype !== "int32") {
        input2 = cast2(input2, "int32");
      }
      const output = gather2(this.embeddings.read(), reshape(input2, [input2.size]));
      return reshape(output, getExactlyOneShape(this.computeOutputShape(input2.shape)));
    });
  }
  getConfig() {
    const config = {
      inputDim: this.inputDim,
      outputDim: this.outputDim,
      embeddingsInitializer: serializeInitializer(this.embeddingsInitializer),
      embeddingsRegularizer: serializeRegularizer(this.embeddingsRegularizer),
      activityRegularizer: serializeRegularizer(this.activityRegularizer),
      embeddingsConstraint: serializeConstraint(this.embeddingsConstraint),
      maskZero: this.maskZero,
      inputLength: this.inputLength
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Embedding.className = "Embedding";
serialization_exports.registerClass(Embedding);

// node_modules/@tensorflow/tfjs-layers/dist/layers/merge.js
var Merge = class extends Layer {
  constructor(args) {
    super(args || {});
    this.supportsMasking = true;
  }
  /**
   * Logic for merging multiple tensors, to be overridden by subclasses.
   * @param inputs
   */
  mergeFunction(inputs) {
    throw new NotImplementedError();
  }
  /**
   * Computes the shape of the result of an elementwise operation.
   *
   * @param shape1: Shape of the first tensor.
   * @param shape2: Shape of the second tensor.
   * @returns Expected output shape when an elementwise operation is carried
   *   out on 2 tensors with shapes `shape1` and `shape2`.
   * @throws ValueError: If `shape1` and `shape2` are not compatible for
   *   element-wise operations.
   */
  computeElementwiseOpOutputShape(shape1, shape2) {
    if (shape1 == null || shape2 == null) {
      return null;
    } else if (shape1.length < shape2.length) {
      return this.computeElementwiseOpOutputShape(shape2, shape1);
    } else if (shape2.length === 0) {
      return shape1;
    }
    const outputShape = shape1.slice(0, shape1.length - shape2.length);
    for (let k = 0; k < shape2.length; ++k) {
      const i = shape1[shape1.length - shape2.length + k];
      const j = shape2[k];
      if (i == null || j == null || i < 0 || j < 0) {
        outputShape.push(null);
      } else if (i === 1) {
        outputShape.push(j);
      } else if (j === 1) {
        outputShape.push(i);
      } else {
        if (i !== j) {
          throw new ValueError("Operands could not be broadcast together with shapes " + JSON.stringify(shape1) + " " + JSON.stringify(shape2));
        }
        outputShape.push(i);
      }
    }
    return outputShape;
  }
  build(inputShape) {
    if (Array.isArray(inputShape) && !Array.isArray(inputShape[0])) {
      inputShape = [getExactlyOneShape(inputShape)];
    }
    inputShape = inputShape;
    if (inputShape.length < 2) {
      throw new ValueError(`A merge layer should be called on an Array of at least 2 inputs. Got ${inputShape.length} input(s).`);
    }
    let batchSizes = [];
    for (const shape of inputShape) {
      if (shape != null && shape[0] !== null) {
        batchSizes.push(shape[0]);
      }
    }
    batchSizes = unique(batchSizes);
    if (batchSizes.length > 1) {
      throw new ValueError(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(inputShape)}.`);
    }
    let outputShape = inputShape[0] == null ? null : inputShape[0].slice(1);
    for (let i = 1; i < inputShape.length; ++i) {
      const shape = inputShape[i] == null ? null : inputShape[i].slice(1);
      outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);
    }
    const allRanks = inputShape.map((shape) => shape.length);
    if (inputShape.indexOf(null) === -1 && unique(allRanks).length === 1) {
      this.reshapeRequired = false;
    } else {
      this.reshapeRequired = true;
    }
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = inputs;
      if (this.reshapeRequired) {
        const reshapedInputs = [];
        const inputDims = inputs.map((input2) => input2.rank);
        if (inputDims.indexOf(null) === -1) {
          const maxNDim = max2(inputDims);
          for (let x of inputs) {
            const xNDim = x.rank;
            for (let k = 0; k < maxNDim - xNDim; ++k) {
              x = expandDims2(x, 1);
            }
            reshapedInputs.push(x);
          }
          return this.mergeFunction(reshapedInputs);
        } else {
          let transposed = false;
          for (const x of inputs) {
            const xNDim = x.rank;
            if (xNDim == null) {
              const xShape = x.shape;
              const batchSize = xShape[0];
              const newShape = xShape.slice(1).concat([batchSize]);
              let xTransposed = reshape(x, [batchSize].concat(arrayProd(xShape.slice(1))));
              xTransposed = transpose(xTransposed, [1, 0]);
              xTransposed = reshape(xTransposed, newShape);
              reshapedInputs.push(xTransposed);
              transposed = true;
            } else if (xNDim > 1) {
              const dims = range(1, xNDim).concat([0]);
              reshapedInputs.push(transpose(x, dims));
              transposed = true;
            } else {
              reshapedInputs.push(x);
            }
          }
          let y = this.mergeFunction(reshapedInputs);
          const yNDim = y.rank;
          if (transposed) {
            if (yNDim == null) {
              const yShape = y.shape;
              const yNDim2 = yShape.length;
              const batchSize = yShape[yNDim2 - 1];
              const newShape = [batchSize].concat(yShape.slice(0, yShape.length - 1));
              y = reshape(transpose(reshape(y, [-1, batchSize]), [1, 0]), newShape);
            } else if (yNDim > 1) {
              const dims = [yNDim - 1].concat(range(0, yNDim - 1));
              y = transpose(y, dims);
            }
          }
          return y;
        }
      } else {
        return this.mergeFunction(inputs);
      }
    });
  }
  computeOutputShape(inputShape) {
    inputShape = inputShape;
    let outputShape;
    if (inputShape[0] == null) {
      outputShape = null;
    } else {
      outputShape = inputShape[0].slice(1);
    }
    for (let i = 1; i < inputShape.length; ++i) {
      const shape = inputShape[i] == null ? null : inputShape[i].slice(1);
      outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);
    }
    let batchSizes = [];
    for (const shape of inputShape) {
      if (shape != null && shape[0] !== null) {
        batchSizes.push(shape[0]);
      }
    }
    batchSizes = unique(batchSizes);
    if (batchSizes.length === 1) {
      outputShape = batchSizes.concat(outputShape);
    } else {
      outputShape = [null].concat(outputShape);
    }
    return outputShape;
  }
  computeMask(inputs, mask) {
    return tidy(() => {
      if (mask == null) {
        return null;
      }
      if (!Array.isArray(mask)) {
        throw new ValueError("`mask` should be an Array");
      }
      if (!Array.isArray(inputs)) {
        throw new ValueError("`inputs` should be an Array");
      }
      if (mask.length !== inputs.length) {
        throw new ValueError(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${inputs.length} vs ${mask.length})`);
      }
      if (mask.every((m) => m == null)) {
        return null;
      }
      mask = mask.map((m) => m == null ? m : expandDims(m, 0));
      let output = mask[0];
      for (let i = 1; i < mask.length - 1; ++i) {
        output = logicalAnd(output, mask[i]);
      }
      return output;
    });
  }
};
var Add2 = class extends Merge {
  constructor(args) {
    super(args);
  }
  mergeFunction(inputs) {
    return tidy(() => {
      let output = inputs[0].clone();
      for (let i = 1; i < inputs.length; ++i) {
        output = add(output, inputs[i]);
      }
      return output;
    });
  }
};
Add2.className = "Add";
serialization_exports.registerClass(Add2);
var Multiply2 = class extends Merge {
  constructor(args) {
    super(args);
  }
  mergeFunction(inputs) {
    return tidy(() => {
      let output = inputs[0].clone();
      for (let i = 1; i < inputs.length; ++i) {
        output = mul(output, inputs[i]);
      }
      return output;
    });
  }
};
Multiply2.className = "Multiply";
serialization_exports.registerClass(Multiply2);
var Average = class extends Merge {
  constructor(args) {
    super(args);
  }
  mergeFunction(inputs) {
    return tidy(() => {
      let output = inputs[0].clone();
      for (let i = 1; i < inputs.length; ++i) {
        output = add(output, inputs[i]);
      }
      return mul(1 / inputs.length, output);
    });
  }
};
Average.className = "Average";
serialization_exports.registerClass(Average);
var Maximum2 = class extends Merge {
  constructor(args) {
    super(args);
  }
  mergeFunction(inputs) {
    return tidy(() => {
      let output = inputs[0];
      for (let i = 1; i < inputs.length; ++i) {
        output = maximum(output, inputs[i]);
      }
      return output;
    });
  }
};
Maximum2.className = "Maximum";
serialization_exports.registerClass(Maximum2);
var Minimum2 = class extends Merge {
  constructor(args) {
    super(args);
  }
  mergeFunction(inputs) {
    return tidy(() => {
      let output = inputs[0];
      for (let i = 1; i < inputs.length; ++i) {
        output = minimum(output, inputs[i]);
      }
      return output;
    });
  }
};
Minimum2.className = "Minimum";
serialization_exports.registerClass(Minimum2);
var Concatenate = class extends Merge {
  constructor(args) {
    super(args);
    this.DEFAULT_AXIS = -1;
    if (args == null) {
      args = {};
    }
    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;
    this.supportsMasking = true;
    this.reshapeRequired = false;
  }
  build(inputShape) {
    if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0])) || inputShape.length === 1) {
      throw new ValueError("A `Concatenate` layer should be called on a list of at least 2 inputs");
    }
    inputShape = inputShape;
    let allNoneShape = true;
    for (const shape of inputShape) {
      if (shape != null) {
        allNoneShape = false;
        break;
      }
    }
    if (allNoneShape) {
      return;
    }
    const shapeSet = [];
    for (let i = 0; i < inputShape.length; ++i) {
      const shapeWithoutConcatAxis = inputShape[i].slice();
      shapeWithoutConcatAxis.splice(this.axis, 1);
      let exists = false;
      for (const shape of shapeSet) {
        if (util_exports.arraysEqual(shape, shapeWithoutConcatAxis)) {
          exists = true;
          break;
        }
      }
      if (!exists) {
        shapeSet.push(shapeWithoutConcatAxis);
      }
    }
    if (shapeSet.length > 1) {
      throw new ValueError("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: " + JSON.stringify(inputShape));
    }
  }
  mergeFunction(inputs) {
    return tidy(() => {
      return concatenate(inputs, this.axis);
    });
  }
  computeOutputShape(inputShape) {
    if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0]))) {
      throw new ValueError("A `Concatenate` layer should be called on a list of inputs.");
    }
    const inputShapes = inputShape;
    const outputShape = inputShapes[0].slice();
    const axis = this.axis < 0 ? outputShape.length + this.axis : this.axis;
    for (const shape of inputShapes.slice(1)) {
      if (outputShape[axis] == null || shape[axis] == null) {
        outputShape[axis] = null;
        break;
      }
      outputShape[axis] += shape[axis];
    }
    return outputShape;
  }
  computeMask(inputs, mask) {
    if (mask == null) {
      return null;
    }
    if (!Array.isArray(mask)) {
      throw new ValueError("`mask` should be an array for Concatenate");
    }
    if (!Array.isArray(inputs)) {
      throw new ValueError("`inputs` should be an array for Concatenate");
    }
    if (mask.length !== inputs.length) {
      throw new ValueError(`Mismatch in the length of mask (${mask.length}) and the legnth of inputs (${inputs.length})`);
    }
    return tidy(() => {
      let allNullMasks = true;
      mask.forEach((m) => {
        if (m != null) {
          allNullMasks = false;
          return;
        }
      });
      if (allNullMasks) {
        return null;
      }
      const outputMasks = [];
      for (let i = 0; i < inputs.length; ++i) {
        if (mask[i] == null) {
          outputMasks.push(cast(onesLike(inputs[i]), "bool"));
        } else if (mask[i].rank < inputs[i].rank) {
          outputMasks.push(expandDims(mask[i], -1));
        } else {
          outputMasks.push(mask[i]);
        }
      }
      const concatenatedMasks = concat(outputMasks, this.axis);
      return all(concatenatedMasks, -1, false);
    });
  }
  getConfig() {
    const config = {
      "axis": this.axis
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Concatenate.className = "Concatenate";
serialization_exports.registerClass(Concatenate);
function interpretAxis(axis, dim) {
  while (axis < 0) {
    axis += dim;
  }
  return axis;
}
function batchDot(x, y, axes) {
  if (x.shape.length > 3 || y.shape.length > 3) {
    throw new NotImplementedError("batchDot is not implemented for tensors of 4D or higher rank yet");
  }
  util_exports.assert(x.shape.length >= 2, () => `batchDot requires the rank of x to be >= 2, but got ${x.shape.length}`);
  util_exports.assert(x.shape.length >= 2, () => `batchDot requires the rank of y to be >= 2, but got ${y.shape.length}`);
  if (typeof axes === "number") {
    axes = [axes, axes];
  }
  if (x.dtype === "complex64" || y.dtype === "complex64") {
    throw new NotImplementedError("batchDot is not implemented for complex64-type Tensors yet.");
  }
  const xNDim = x.shape.length;
  const yNDim = y.shape.length;
  if (axes == null) {
    axes = [xNDim - 1, yNDim - 2];
  }
  const axesArray = axes;
  return tidy(() => {
    let diff;
    if (xNDim > yNDim) {
      diff = xNDim - yNDim;
      const diffShape = [];
      for (let i = 0; i < diff; ++i) {
        diffShape.push(1);
      }
      y = reshape(y, y.shape.concat(diffShape));
    } else if (yNDim > xNDim) {
      diff = yNDim - xNDim;
      const diffShape = [];
      for (let i = 0; i < diff; ++i) {
        diffShape.push(1);
      }
      x = reshape(x, x.shape.concat(diffShape));
    } else {
      diff = 0;
    }
    let out;
    if (x.shape.length === 2 && y.shape.length === 2) {
      if (axesArray[0] === axesArray[1]) {
        out = sum(mul(x, y), axesArray[0]);
      } else {
        out = sum(mul(transpose(x, [1, 0]), y), axesArray[1]);
      }
    } else {
      const adjX = axesArray[0] !== x.shape.length - 1;
      const adjY = axesArray[1] === y.shape.length - 1;
      out = matMul(x, y, adjX, adjY);
    }
    if (diff > 0) {
      let idx;
      if (xNDim > yNDim) {
        idx = xNDim + yNDim - 3;
      } else {
        idx = xNDim - 1;
      }
      const squeezeAxes = [];
      for (let i = idx; i < idx + diff; ++i) {
        squeezeAxes.push(i);
      }
      out = squeeze(out, squeezeAxes);
    }
    if (out.shape.length === 1) {
      out = expandDims(out, 1);
    }
    return out;
  });
}
var Dot = class extends Merge {
  constructor(args) {
    super(args);
    this.axes = args.axes;
    this.normalize = args.normalize == null ? false : args.normalize;
    this.supportsMasking = true;
    this.reshapeRequired = false;
  }
  build(inputShape) {
    util_exports.assert(Array.isArray(inputShape) && inputShape.length === 2 && Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), () => "A `Dot` layer should be called on a list of exactly 2 inputs.");
    const shape1 = inputShape[0];
    const shape2 = inputShape[1];
    if (shape1.length > 3 || shape2.length > 3) {
      throw new NotImplementedError("Dot layer does not support tensors of 4D or higher rank yet.");
    }
    const axes = this.interpretAxes(shape1, shape2);
    if (shape1[axes[0]] !== shape2[axes[1]]) {
      throw new ValueError(`Dimension incompatibility: ${shape1[axes[0]]} !== ${shape2[axes[1]]}`);
    }
  }
  mergeFunction(inputs) {
    if (inputs.length !== 2) {
      throw new ValueError(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${inputs.length} input(s).`);
    }
    let x1 = inputs[0];
    let x2 = inputs[1];
    let axes;
    if (!Array.isArray(this.axes)) {
      axes = [
        interpretAxis(this.axes, x1.shape.length),
        interpretAxis(this.axes, x2.shape.length)
      ];
    } else {
      axes = this.axes.map((axis, i) => interpretAxis(axis, inputs[i].shape.length));
    }
    if (this.normalize) {
      x1 = l2Normalize(x1, axes[0]);
      x2 = l2Normalize(x2, axes[1]);
    }
    return batchDot(x1, x2, axes);
  }
  interpretAxes(shape1, shape2) {
    let axes;
    if (!Array.isArray(this.axes)) {
      axes = [
        interpretAxis(this.axes, shape1.length),
        interpretAxis(this.axes, shape2.length)
      ];
    } else {
      axes = this.axes;
    }
    return axes;
  }
  computeOutputShape(inputShape) {
    util_exports.assert(Array.isArray(inputShape) && inputShape.length === 2 && Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), () => "A `Dot` layer should be called on a list of exactly 2 inputs.");
    const shape1 = inputShape[0].slice();
    const shape2 = inputShape[1].slice();
    if (shape1.length > 3 || shape2.length > 3) {
      throw new NotImplementedError("Dot layer does not support tensors of 4D or higher rank yet.");
    }
    const axes = this.interpretAxes(shape1, shape2);
    shape1.splice(axes[0], 1);
    shape2.splice(axes[1], 1);
    shape2.splice(0, 1);
    const outputShape = shape1.concat(shape2);
    if (outputShape.length === 1) {
      outputShape.push(1);
    }
    return outputShape;
  }
  computeMask(inputs, mask) {
    return null;
  }
  getConfig() {
    const config = {
      "axes": this.axes,
      "normalize": this.normalize
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
Dot.className = "Dot";
serialization_exports.registerClass(Dot);

// node_modules/@tensorflow/tfjs-layers/dist/layers/noise.js
var GaussianNoise = class extends Layer {
  constructor(args) {
    super(args);
    this.supportsMasking = true;
    this.stddev = args.stddev;
  }
  computeOutputShape(inputShape) {
    return inputShape;
  }
  getConfig() {
    const baseConfig = super.getConfig();
    const config = { stddev: this.stddev };
    Object.assign(config, baseConfig);
    return config;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      const input2 = getExactlyOneTensor(inputs);
      const noised = () => add(randomNormal2(input2.shape, 0, this.stddev), input2);
      const output = inTrainPhase(noised, () => input2, kwargs["training"] || false);
      return output;
    });
  }
};
GaussianNoise.className = "GaussianNoise";
serialization_exports.registerClass(GaussianNoise);
var GaussianDropout = class extends Layer {
  constructor(args) {
    super(args);
    this.supportsMasking = true;
    this.rate = args.rate;
  }
  computeOutputShape(inputShape) {
    return inputShape;
  }
  getConfig() {
    const baseConfig = super.getConfig();
    const config = { rate: this.rate };
    Object.assign(config, baseConfig);
    return config;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      const input2 = getExactlyOneTensor(inputs);
      if (this.rate > 0 && this.rate < 1) {
        const noised = () => {
          const stddev = Math.sqrt(this.rate / (1 - this.rate));
          return mul(input2, randomNormal2(input2.shape, 1, stddev));
        };
        return inTrainPhase(noised, () => input2, kwargs["training"] || false);
      }
      return input2;
    });
  }
};
GaussianDropout.className = "GaussianDropout";
serialization_exports.registerClass(GaussianDropout);
var AlphaDropout = class extends Layer {
  constructor(args) {
    super(args);
    this.supportsMasking = true;
    this.rate = args.rate;
    this.noiseShape = args.noiseShape;
  }
  _getNoiseShape(inputs) {
    return this.noiseShape || getExactlyOneTensor(inputs).shape;
  }
  computeOutputShape(inputShape) {
    return inputShape;
  }
  getConfig() {
    const baseConfig = super.getConfig();
    const config = { rate: this.rate };
    Object.assign(config, baseConfig);
    return config;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      if (this.rate < 1 && this.rate > 0) {
        const noiseShape = this._getNoiseShape(inputs);
        const droppedInputs = () => {
          const input2 = getExactlyOneTensor(inputs);
          const alpha = 1.6732632423543772;
          const scale = 1.0507009873554805;
          const alphaP = -alpha * scale;
          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);
          keptIdx = cast2(keptIdx, "float32");
          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;
          const b = -a * alphaP * this.rate;
          const x = add(mul(input2, keptIdx), mul(add(keptIdx, -1), alphaP));
          return add(mul(x, a), b);
        };
        return inTrainPhase(droppedInputs, () => getExactlyOneTensor(inputs), kwargs["training"] || false);
      }
      return inputs;
    });
  }
};
AlphaDropout.className = "AlphaDropout";
serialization_exports.registerClass(AlphaDropout);

// node_modules/@tensorflow/tfjs-layers/dist/layers/normalization.js
function batchNormalization(x, mean2, variance, beta, gamma, epsilon2 = 1e-3) {
  let out;
  if (x.rank === 2) {
    out = batchNorm2d(x, mean2, variance, beta, gamma, epsilon2);
  } else if (x.rank === 3) {
    out = batchNorm3d(x, mean2, variance, beta, gamma, epsilon2);
  } else if (x.rank === 4) {
    out = batchNorm4d(x, mean2, variance, beta, gamma, epsilon2);
  } else {
    throw new NotImplementedError(`batchNormalization is not implemented for array of rank ${x.rank} yet`);
  }
  return out;
}
function regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon2 = 1e-3) {
  return tidy(() => {
    const meanAndVariance = moments(x, reductionAxes);
    const mean2 = meanAndVariance.mean;
    const variance = meanAndVariance.variance;
    const normed = batchNormalization(x, mean2, variance, beta, gamma, epsilon2);
    return [normed, mean2, variance];
  });
}
function broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon2 = 1e-3) {
  return tidy(() => {
    const meanAndVariance = moments(x, reductionAxes);
    const mean2 = meanAndVariance.mean;
    const variance = meanAndVariance.variance;
    const targetShape = [];
    for (const axis of range(0, x.rank)) {
      if (reductionAxes.indexOf(axis) !== -1) {
        targetShape.push(1);
      } else {
        targetShape.push(x.shape[axis]);
      }
    }
    const broadcastMean = reshape(mean2, targetShape);
    const broadcastVariance = reshape(variance, targetShape);
    const broadcastGamma = gamma == null ? null : reshape(gamma, targetShape);
    const broadcastBeta = beta == null ? null : reshape(beta, targetShape);
    const normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon2);
    return [normed, mean2, variance];
  });
}
function normalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon2 = 1e-3) {
  if (util_exports.arraysEqual(reductionAxes.slice().sort(), range(0, x.rank - 1))) {
    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon2);
  } else {
    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon2);
  }
}
var BatchNormalization = class extends Layer {
  constructor(args) {
    if (args == null) {
      args = {};
    }
    super(args);
    this.supportsMasking = true;
    this.axis = args.axis == null ? -1 : args.axis;
    this.momentum = args.momentum == null ? 0.99 : args.momentum;
    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;
    this.center = args.center == null ? true : args.center;
    this.scale = args.scale == null ? true : args.scale;
    this.betaInitializer = getInitializer(args.betaInitializer || "zeros");
    this.gammaInitializer = getInitializer(args.gammaInitializer || "ones");
    this.movingMeanInitializer = getInitializer(args.movingMeanInitializer || "zeros");
    this.movingVarianceInitializer = getInitializer(args.movingVarianceInitializer || "ones");
    this.betaConstraint = getConstraint(args.betaConstraint);
    this.gammaConstraint = getConstraint(args.gammaConstraint);
    this.betaRegularizer = getRegularizer(args.betaRegularizer);
    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;
    const dim = inputShape[axis];
    if (dim == null) {
      throw new ValueError(`Axis ${axis} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(inputShape)}.`);
    }
    this.inputSpec = [new InputSpec({ ndim: inputShape.length, axes: { [axis]: dim } })];
    const shape = [dim];
    if (this.scale) {
      this.gamma = this.addWeight("gamma", shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);
    }
    if (this.center) {
      this.beta = this.addWeight("beta", shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);
    }
    this.movingMean = this.addWeight("moving_mean", shape, null, this.movingMeanInitializer, null, false);
    this.movingVariance = this.addWeight("moving_variance", shape, null, this.movingVarianceInitializer, null, false);
    this.built = true;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      const training = kwargs["training"] == null ? false : kwargs["training"];
      const input2 = getExactlyOneTensor(inputs);
      const inputShape = input2.shape;
      const ndim = inputShape.length;
      const reductionAxes = range(0, ndim);
      const axis = this.axis >= 0 ? this.axis : this.axis + ndim;
      reductionAxes.splice(axis, 1);
      const broadcastShape = pyListRepeat(1, ndim);
      broadcastShape[axis] = inputShape[axis];
      const sortedReductionAxes = reductionAxes.slice();
      sortedReductionAxes.sort();
      const needsBroadcasting = !util_exports.arraysEqual(sortedReductionAxes, range(0, ndim).slice(0, ndim - 1));
      const normalizeInference = () => {
        if (needsBroadcasting) {
          const broadcastMovingMean = reshape(this.movingMean.read(), broadcastShape);
          const broadcastMovingVariance = reshape(this.movingVariance.read(), broadcastShape);
          const broadcastBeta = this.center ? reshape(this.beta.read(), broadcastShape) : null;
          const broadcastGamma = this.scale ? reshape(this.gamma.read(), broadcastShape) : null;
          return batchNormalization(input2, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, this.epsilon);
        } else {
          return batchNormalization(input2, this.movingMean.read(), this.movingVariance.read(), this.beta == null ? null : this.beta.read(), this.gamma == null ? null : this.gamma.read(), this.epsilon);
        }
      };
      if (!training) {
        return normalizeInference();
      }
      const [normedTraining, mean2, variance] = normalizeBatchInTraining(input2, this.gamma.read(), this.beta.read(), reductionAxes, this.epsilon);
      const doMovingAverage = (variable2, value, momentum) => {
        tidy(() => {
          const decay = 1 - momentum;
          const origValue = variable2.read();
          const updateDelta = mul(sub(origValue, value), decay);
          variable2.write(sub(origValue, updateDelta));
        });
      };
      const updateMovingMeanAndVariance = () => {
        doMovingAverage(this.movingMean, mean2, this.momentum);
        doMovingAverage(this.movingVariance, variance, this.momentum);
      };
      updateMovingMeanAndVariance();
      return normedTraining;
    });
  }
  getConfig() {
    const config = {
      axis: this.axis,
      momentum: this.momentum,
      epsilon: this.epsilon,
      center: this.center,
      scale: this.scale,
      betaInitializer: serializeInitializer(this.betaInitializer),
      gammaInitializer: serializeInitializer(this.gammaInitializer),
      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),
      movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),
      betaRegularizer: serializeRegularizer(this.betaRegularizer),
      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),
      betaConstraint: serializeConstraint(this.betaConstraint),
      gammaConstraint: serializeConstraint(this.gammaConstraint)
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
BatchNormalization.className = "BatchNormalization";
serialization_exports.registerClass(BatchNormalization);
var LayerNormalization = class extends Layer {
  constructor(args) {
    if (args == null) {
      args = {};
    }
    super(args);
    this.axis = args.axis == null ? -1 : args.axis;
    if (typeof this.axis === "number") {
      if (!Number.isInteger(this.axis)) {
        throw new Error(`Expected axis to be an integer, but received ${this.axis}`);
      }
    } else if (Array.isArray(this.axis)) {
      for (const axis of this.axis) {
        if (!Number.isInteger(axis)) {
          throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`);
        }
      }
    } else {
      throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);
    }
    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;
    this.center = args.center == null ? true : args.center;
    this.scale = args.scale == null ? true : args.scale;
    this.betaInitializer = getInitializer(args.betaInitializer || "zeros");
    this.gammaInitializer = getInitializer(args.gammaInitializer || "ones");
    this.betaRegularizer = getRegularizer(args.betaRegularizer);
    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);
    this.supportsMasking = true;
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const nDims = inputShape.length;
    if (typeof this.axis === "number") {
      this.axis = [this.axis];
    }
    for (let i = 0; i < this.axis.length; ++i) {
      if (this.axis[i] < 0) {
        this.axis[i] += nDims;
      }
    }
    for (const axis of this.axis) {
      if (axis < 0 || axis >= nDims) {
        throw new Error(`Invalid axis: ${axis}`);
      }
    }
    if (this.axis.length !== unique(this.axis).length) {
      throw new Error(`Found duplicate axes in: ${this.axis}`);
    }
    const paramShape = this.axis.map((axis) => inputShape[axis]);
    const trainable = true;
    if (this.scale) {
      this.gamma = this.addWeight("gamma", paramShape, "float32", this.gammaInitializer, this.gammaRegularizer, trainable);
    } else {
      this.gamma = null;
    }
    if (this.center) {
      this.beta = this.addWeight("beta", paramShape, "float32", this.betaInitializer, this.betaRegularizer, trainable);
    } else {
      this.beta = null;
    }
    this.built = true;
  }
  call(inputs, kwargs) {
    const input2 = getExactlyOneTensor(inputs);
    const inputShape = input2.shape;
    const nDims = inputShape.length;
    return tidy(() => {
      const keepDims = true;
      let { mean: mean2, variance } = moments(input2, this.axis, keepDims);
      const broadcastShape = pyListRepeat(1, nDims);
      for (const dim of this.axis) {
        broadcastShape[dim] = inputShape[dim];
      }
      const broadcast = (v) => {
        if (v != null && v.shape.length !== nDims) {
          return reshape(v, broadcastShape);
        } else {
          return v;
        }
      };
      let scale = this.scale ? broadcast(this.gamma.read()) : null;
      let offset = this.center ? broadcast(this.beta.read()) : null;
      const momentsTiling = [];
      const scaleOffsetTiling = [];
      for (let i = 0; i < nDims; ++i) {
        if (this.axis.indexOf(i) !== -1) {
          momentsTiling.push(inputShape[i]);
          scaleOffsetTiling.push(1);
        } else {
          momentsTiling.push(1);
          scaleOffsetTiling.push(inputShape[i]);
        }
      }
      mean2 = tile(mean2, momentsTiling);
      variance = tile(variance, momentsTiling);
      if (scale != null) {
        scale = tile(scale, scaleOffsetTiling);
      }
      if (offset != null) {
        offset = tile(offset, scaleOffsetTiling);
      }
      return batchNormalization(input2, mean2, variance, offset, scale, this.epsilon);
    });
  }
  getConfig() {
    const config = {
      axis: this.axis,
      epsilon: this.epsilon,
      center: this.center,
      scale: this.scale,
      betaInitializer: serializeInitializer(this.betaInitializer),
      gammaInitializer: serializeInitializer(this.gammaInitializer),
      betaRegularizer: serializeRegularizer(this.betaRegularizer),
      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
LayerNormalization.className = "LayerNormalization";
serialization_exports.registerClass(LayerNormalization);

// node_modules/@tensorflow/tfjs-layers/dist/layers/padding.js
function spatial2dPadding(x, padding, dataFormat) {
  return tidy(() => {
    if (x.rank !== 4) {
      throw new ValueError(`temporalPadding expects input tensor to be 4-D, but received a ${x.rank}-D tensor.`);
    }
    if (padding == null) {
      padding = [[1, 1], [1, 1]];
    }
    if (padding.length !== 2 || padding[0].length !== 2 || padding[1].length !== 2) {
      throw new ValueError("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");
    }
    if (dataFormat == null) {
      dataFormat = imageDataFormat();
    }
    if (dataFormat !== "channelsLast" && dataFormat !== "channelsFirst") {
      throw new ValueError(`Unknown data format: ${dataFormat}. Supported data formats are 'channelsLast' and 'channelsFirst.`);
    }
    let pattern;
    if (dataFormat === "channelsFirst") {
      pattern = [[0, 0], [0, 0], padding[0], padding[1]];
    } else {
      pattern = [[0, 0], padding[0], padding[1], [0, 0]];
    }
    return pad(x, pattern);
  });
}
var ZeroPadding2D = class extends Layer {
  constructor(args) {
    if (args == null) {
      args = {};
    }
    super(args);
    this.dataFormat = args.dataFormat == null ? imageDataFormat() : args.dataFormat;
    if (args.padding == null) {
      this.padding = [[1, 1], [1, 1]];
    } else if (typeof args.padding === "number") {
      this.padding = [[args.padding, args.padding], [args.padding, args.padding]];
    } else {
      args.padding = args.padding;
      if (args.padding.length !== 2) {
        throw new ValueError(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${args.padding.length} array.`);
      }
      let heightPadding;
      let widthPadding;
      if (typeof args.padding[0] === "number") {
        heightPadding = [args.padding[0], args.padding[0]];
        widthPadding = [args.padding[1], args.padding[1]];
      } else {
        args.padding = args.padding;
        if (args.padding[0].length !== 2) {
          throw new ValueError(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${args.padding[0].length} array.`);
        }
        heightPadding = args.padding[0];
        if (args.padding[1].length !== 2) {
          throw new ValueError(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${args.padding[1].length} array.`);
        }
        widthPadding = args.padding[1];
      }
      this.padding = [heightPadding, widthPadding];
    }
    this.inputSpec = [new InputSpec({ ndim: 4 })];
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    let rows;
    let cols;
    if (this.dataFormat === "channelsFirst") {
      if (inputShape[2] != null && inputShape[2] >= 0) {
        rows = inputShape[2] + this.padding[0][0] + this.padding[0][1];
      } else {
        rows = null;
      }
      if (inputShape[3] != null && inputShape[3] >= 0) {
        cols = inputShape[3] + this.padding[1][0] + this.padding[1][1];
      } else {
        cols = null;
      }
      return [inputShape[0], inputShape[1], rows, cols];
    } else {
      if (inputShape[1] != null && inputShape[1] >= 0) {
        rows = inputShape[1] + this.padding[0][0] + this.padding[0][1];
      } else {
        rows = null;
      }
      if (inputShape[2] != null && inputShape[2] >= 0) {
        cols = inputShape[2] + this.padding[1][0] + this.padding[1][1];
      } else {
        cols = null;
      }
      return [inputShape[0], rows, cols, inputShape[3]];
    }
  }
  call(inputs, kwargs) {
    return tidy(() => spatial2dPadding(getExactlyOneTensor(inputs), this.padding, this.dataFormat));
  }
  getConfig() {
    const config = {
      padding: this.padding,
      dataFormat: this.dataFormat
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
ZeroPadding2D.className = "ZeroPadding2D";
serialization_exports.registerClass(ZeroPadding2D);

// node_modules/@tensorflow/tfjs-layers/dist/layers/pooling.js
function pool2d(x, poolSize, strides, padding, dataFormat, poolMode) {
  return tidy(() => {
    checkDataFormat(dataFormat);
    checkPoolMode(poolMode);
    checkPaddingMode(padding);
    if (strides == null) {
      strides = [1, 1];
    }
    if (padding == null) {
      padding = "valid";
    }
    if (dataFormat == null) {
      dataFormat = imageDataFormat();
    }
    if (poolMode == null) {
      poolMode = "max";
    }
    x = preprocessConv2DInput(x, dataFormat);
    let y;
    const paddingString = padding === "same" ? "same" : "valid";
    if (poolMode === "max") {
      y = maxPool(x, poolSize, strides, paddingString);
    } else {
      y = avgPool(
        // TODO(cais): Rank check?
        x,
        poolSize,
        strides,
        paddingString
      );
    }
    if (dataFormat === "channelsFirst") {
      y = transpose(y, [0, 3, 1, 2]);
    }
    return y;
  });
}
function pool3d(x, poolSize, strides, padding, dataFormat, poolMode) {
  return tidy(() => {
    checkDataFormat(dataFormat);
    checkPoolMode(poolMode);
    checkPaddingMode(padding);
    if (strides == null) {
      strides = [1, 1, 1];
    }
    if (padding == null) {
      padding = "valid";
    }
    if (dataFormat == null) {
      dataFormat = imageDataFormat();
    }
    if (poolMode == null) {
      poolMode = "max";
    }
    x = preprocessConv3DInput(x, dataFormat);
    let y;
    const paddingString = padding === "same" ? "same" : "valid";
    if (poolMode === "max") {
      y = maxPool3d(x, poolSize, strides, paddingString);
    } else {
      y = avgPool3d(x, poolSize, strides, paddingString);
    }
    if (dataFormat === "channelsFirst") {
      y = transpose(y, [0, 4, 1, 2, 3]);
    }
    return y;
  });
}
var Pooling1D = class extends Layer {
  /**
   *
   * @param args Parameters for the Pooling layer.
   *
   * config.poolSize defaults to 2.
   */
  constructor(args) {
    if (args.poolSize == null) {
      args.poolSize = 2;
    }
    super(args);
    if (typeof args.poolSize === "number") {
      this.poolSize = [args.poolSize];
    } else if (Array.isArray(args.poolSize) && args.poolSize.length === 1 && typeof args.poolSize[0] === "number") {
      this.poolSize = args.poolSize;
    } else {
      throw new ValueError(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(args.poolSize)}`);
    }
    assertPositiveInteger(this.poolSize, "poolSize");
    if (args.strides == null) {
      this.strides = this.poolSize;
    } else {
      if (typeof args.strides === "number") {
        this.strides = [args.strides];
      } else if (Array.isArray(args.strides) && args.strides.length === 1 && typeof args.strides[0] === "number") {
        this.strides = args.strides;
      } else {
        throw new ValueError(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(args.strides)}`);
      }
    }
    assertPositiveInteger(this.strides, "strides");
    this.padding = args.padding == null ? "valid" : args.padding;
    checkPaddingMode(this.padding);
    this.inputSpec = [new InputSpec({ ndim: 3 })];
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const length = convOutputLength(inputShape[1], this.poolSize[0], this.padding, this.strides[0]);
    return [inputShape[0], length, inputShape[2]];
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      inputs = expandDims2(getExactlyOneTensor(inputs), 2);
      const output = this.poolingFunction(getExactlyOneTensor(inputs), [this.poolSize[0], 1], [this.strides[0], 1], this.padding, "channelsLast");
      return squeeze(output, [2]);
    });
  }
  getConfig() {
    const config = {
      poolSize: this.poolSize,
      padding: this.padding,
      strides: this.strides
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
var MaxPooling1D = class extends Pooling1D {
  constructor(args) {
    super(args);
  }
  poolingFunction(inputs, poolSize, strides, padding, dataFormat) {
    checkDataFormat(dataFormat);
    checkPaddingMode(padding);
    return pool2d(inputs, poolSize, strides, padding, dataFormat, "max");
  }
};
MaxPooling1D.className = "MaxPooling1D";
serialization_exports.registerClass(MaxPooling1D);
var AveragePooling1D = class extends Pooling1D {
  constructor(args) {
    super(args);
  }
  poolingFunction(inputs, poolSize, strides, padding, dataFormat) {
    checkDataFormat(dataFormat);
    checkPaddingMode(padding);
    return pool2d(inputs, poolSize, strides, padding, dataFormat, "avg");
  }
};
AveragePooling1D.className = "AveragePooling1D";
serialization_exports.registerClass(AveragePooling1D);
var Pooling2D = class extends Layer {
  constructor(args) {
    if (args.poolSize == null) {
      args.poolSize = [2, 2];
    }
    super(args);
    this.poolSize = Array.isArray(args.poolSize) ? args.poolSize : [args.poolSize, args.poolSize];
    if (args.strides == null) {
      this.strides = this.poolSize;
    } else if (Array.isArray(args.strides)) {
      if (args.strides.length !== 2) {
        throw new ValueError(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${args.strides.length}.`);
      }
      this.strides = args.strides;
    } else {
      this.strides = [args.strides, args.strides];
    }
    assertPositiveInteger(this.poolSize, "poolSize");
    assertPositiveInteger(this.strides, "strides");
    this.padding = args.padding == null ? "valid" : args.padding;
    this.dataFormat = args.dataFormat == null ? "channelsLast" : args.dataFormat;
    checkDataFormat(this.dataFormat);
    checkPaddingMode(this.padding);
    this.inputSpec = [new InputSpec({ ndim: 4 })];
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    let rows = this.dataFormat === "channelsFirst" ? inputShape[2] : inputShape[1];
    let cols = this.dataFormat === "channelsFirst" ? inputShape[3] : inputShape[2];
    rows = convOutputLength(rows, this.poolSize[0], this.padding, this.strides[0]);
    cols = convOutputLength(cols, this.poolSize[1], this.padding, this.strides[1]);
    if (this.dataFormat === "channelsFirst") {
      return [inputShape[0], inputShape[1], rows, cols];
    } else {
      return [inputShape[0], rows, cols, inputShape[3]];
    }
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      return this.poolingFunction(getExactlyOneTensor(inputs), this.poolSize, this.strides, this.padding, this.dataFormat);
    });
  }
  getConfig() {
    const config = {
      poolSize: this.poolSize,
      padding: this.padding,
      strides: this.strides,
      dataFormat: this.dataFormat
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
var MaxPooling2D = class extends Pooling2D {
  constructor(args) {
    super(args);
  }
  poolingFunction(inputs, poolSize, strides, padding, dataFormat) {
    checkDataFormat(dataFormat);
    checkPaddingMode(padding);
    return pool2d(inputs, poolSize, strides, padding, dataFormat, "max");
  }
};
MaxPooling2D.className = "MaxPooling2D";
serialization_exports.registerClass(MaxPooling2D);
var AveragePooling2D = class extends Pooling2D {
  constructor(args) {
    super(args);
  }
  poolingFunction(inputs, poolSize, strides, padding, dataFormat) {
    checkDataFormat(dataFormat);
    checkPaddingMode(padding);
    return pool2d(inputs, poolSize, strides, padding, dataFormat, "avg");
  }
};
AveragePooling2D.className = "AveragePooling2D";
serialization_exports.registerClass(AveragePooling2D);
var Pooling3D = class extends Layer {
  constructor(args) {
    if (args.poolSize == null) {
      args.poolSize = [2, 2, 2];
    }
    super(args);
    this.poolSize = Array.isArray(args.poolSize) ? args.poolSize : [args.poolSize, args.poolSize, args.poolSize];
    if (args.strides == null) {
      this.strides = this.poolSize;
    } else if (Array.isArray(args.strides)) {
      if (args.strides.length !== 3) {
        throw new ValueError(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${args.strides.length}.`);
      }
      this.strides = args.strides;
    } else {
      this.strides = [args.strides, args.strides, args.strides];
    }
    assertPositiveInteger(this.poolSize, "poolSize");
    assertPositiveInteger(this.strides, "strides");
    this.padding = args.padding == null ? "valid" : args.padding;
    this.dataFormat = args.dataFormat == null ? "channelsLast" : args.dataFormat;
    checkDataFormat(this.dataFormat);
    checkPaddingMode(this.padding);
    this.inputSpec = [new InputSpec({ ndim: 5 })];
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    let depths = this.dataFormat === "channelsFirst" ? inputShape[2] : inputShape[1];
    let rows = this.dataFormat === "channelsFirst" ? inputShape[3] : inputShape[2];
    let cols = this.dataFormat === "channelsFirst" ? inputShape[4] : inputShape[3];
    depths = convOutputLength(depths, this.poolSize[0], this.padding, this.strides[0]);
    rows = convOutputLength(rows, this.poolSize[1], this.padding, this.strides[1]);
    cols = convOutputLength(cols, this.poolSize[2], this.padding, this.strides[2]);
    if (this.dataFormat === "channelsFirst") {
      return [inputShape[0], inputShape[1], depths, rows, cols];
    } else {
      return [inputShape[0], depths, rows, cols, inputShape[4]];
    }
  }
  call(inputs, kwargs) {
    return tidy(() => {
      this.invokeCallHook(inputs, kwargs);
      return this.poolingFunction(getExactlyOneTensor(inputs), this.poolSize, this.strides, this.padding, this.dataFormat);
    });
  }
  getConfig() {
    const config = {
      poolSize: this.poolSize,
      padding: this.padding,
      strides: this.strides,
      dataFormat: this.dataFormat
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
var MaxPooling3D = class extends Pooling3D {
  constructor(args) {
    super(args);
  }
  poolingFunction(inputs, poolSize, strides, padding, dataFormat) {
    checkDataFormat(dataFormat);
    checkPaddingMode(padding);
    return pool3d(inputs, poolSize, strides, padding, dataFormat, "max");
  }
};
MaxPooling3D.className = "MaxPooling3D";
serialization_exports.registerClass(MaxPooling3D);
var AveragePooling3D = class extends Pooling3D {
  constructor(args) {
    super(args);
  }
  poolingFunction(inputs, poolSize, strides, padding, dataFormat) {
    checkDataFormat(dataFormat);
    checkPaddingMode(padding);
    return pool3d(inputs, poolSize, strides, padding, dataFormat, "avg");
  }
};
AveragePooling3D.className = "AveragePooling3D";
serialization_exports.registerClass(AveragePooling3D);
var GlobalPooling1D = class extends Layer {
  constructor(args) {
    super(args);
    this.inputSpec = [new InputSpec({ ndim: 3 })];
  }
  computeOutputShape(inputShape) {
    return [inputShape[0], inputShape[2]];
  }
  call(inputs, kwargs) {
    throw new NotImplementedError();
  }
};
var GlobalAveragePooling1D = class extends GlobalPooling1D {
  constructor(args) {
    super(args || {});
  }
  call(inputs, kwargs) {
    return tidy(() => {
      const input2 = getExactlyOneTensor(inputs);
      return mean(input2, 1);
    });
  }
};
GlobalAveragePooling1D.className = "GlobalAveragePooling1D";
serialization_exports.registerClass(GlobalAveragePooling1D);
var GlobalMaxPooling1D = class extends GlobalPooling1D {
  constructor(args) {
    super(args || {});
  }
  call(inputs, kwargs) {
    return tidy(() => {
      const input2 = getExactlyOneTensor(inputs);
      return max(input2, 1);
    });
  }
};
GlobalMaxPooling1D.className = "GlobalMaxPooling1D";
serialization_exports.registerClass(GlobalMaxPooling1D);
var GlobalPooling2D = class extends Layer {
  constructor(args) {
    super(args);
    this.dataFormat = args.dataFormat == null ? "channelsLast" : args.dataFormat;
    checkDataFormat(this.dataFormat);
    this.inputSpec = [new InputSpec({ ndim: 4 })];
  }
  computeOutputShape(inputShape) {
    inputShape = inputShape;
    if (this.dataFormat === "channelsLast") {
      return [inputShape[0], inputShape[3]];
    } else {
      return [inputShape[0], inputShape[1]];
    }
  }
  call(inputs, kwargs) {
    throw new NotImplementedError();
  }
  getConfig() {
    const config = { dataFormat: this.dataFormat };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
};
var GlobalAveragePooling2D = class extends GlobalPooling2D {
  call(inputs, kwargs) {
    return tidy(() => {
      const input2 = getExactlyOneTensor(inputs);
      if (this.dataFormat === "channelsLast") {
        return mean(input2, [1, 2]);
      } else {
        return mean(input2, [2, 3]);
      }
    });
  }
};
GlobalAveragePooling2D.className = "GlobalAveragePooling2D";
serialization_exports.registerClass(GlobalAveragePooling2D);
var GlobalMaxPooling2D = class extends GlobalPooling2D {
  call(inputs, kwargs) {
    return tidy(() => {
      const input2 = getExactlyOneTensor(inputs);
      if (this.dataFormat === "channelsLast") {
        return max(input2, [1, 2]);
      } else {
        return max(input2, [2, 3]);
      }
    });
  }
};
GlobalMaxPooling2D.className = "GlobalMaxPooling2D";
serialization_exports.registerClass(GlobalMaxPooling2D);

// node_modules/@tensorflow/tfjs-layers/dist/layers/wrappers.js
var Wrapper = class extends Layer {
  constructor(args) {
    super(args);
    this.layer = args.layer;
  }
  build(inputShape) {
    this.built = true;
  }
  // TODO(cais): Implement activityRegularizer getter.
  get trainable() {
    if (this.layer != null) {
      return this.layer.trainable;
    } else {
      return false;
    }
  }
  set trainable(value) {
    if (this.layer != null) {
      this.layer.trainable = value;
    }
  }
  get trainableWeights() {
    return this.layer.trainableWeights;
  }
  // TODO(cais): Implement setter for trainableWeights.
  get nonTrainableWeights() {
    return this.layer.nonTrainableWeights;
  }
  // TODO(cais): Implement setter for nonTrainableWeights.
  get updates() {
    return this.layer._updates;
  }
  // TODO(cais): Implement getUpdatesFor().
  get losses() {
    return this.layer.losses;
  }
  // TODO(cais): Implement getLossesFor().
  getWeights() {
    return this.layer.getWeights();
  }
  setWeights(weights) {
    this.layer.setWeights(weights);
  }
  getConfig() {
    const config = {
      "layer": {
        "className": this.layer.getClassName(),
        "config": this.layer.getConfig()
      }
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
  setFastWeightInitDuringBuild(value) {
    super.setFastWeightInitDuringBuild(value);
    if (this.layer != null) {
      this.layer.setFastWeightInitDuringBuild(value);
    }
  }
  /** @nocollapse */
  static fromConfig(cls, config, customObjects = {}) {
    const layerConfig = config["layer"];
    const layer = deserialize(layerConfig, customObjects);
    delete config["layer"];
    const newConfig = { layer };
    Object.assign(newConfig, config);
    return new cls(newConfig);
  }
};
var TimeDistributed = class extends Wrapper {
  constructor(args) {
    super(args);
    this.supportsMasking = true;
  }
  build(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    if (inputShape.length < 3) {
      throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(inputShape)}`);
    }
    this.inputSpec = [{ shape: inputShape }];
    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));
    if (!this.layer.built) {
      this.layer.build(childInputShape);
      this.layer.built = true;
    }
    super.build(inputShape);
  }
  computeOutputShape(inputShape) {
    inputShape = getExactlyOneShape(inputShape);
    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));
    const childOutputShape = this.layer.computeOutputShape(childInputShape);
    const timesteps = inputShape[1];
    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = getExactlyOneTensor(inputs);
      const step2 = (inputs2, states) => {
        const output = getExactlyOneTensor(this.layer.call(inputs2, kwargs));
        return [output, []];
      };
      const rnnOutputs = rnn(
        step2,
        inputs,
        [],
        false,
        null,
        null,
        false,
        true
        /* needPerStepOutputs */
      );
      const y = rnnOutputs[1];
      return y;
    });
  }
};
TimeDistributed.className = "TimeDistributed";
serialization_exports.registerClass(TimeDistributed);
function checkBidirectionalMergeMode(value) {
  checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, "BidirectionalMergeMode", value);
}
var DEFAULT_BIDIRECTIONAL_MERGE_MODE = "concat";
var Bidirectional = class extends Wrapper {
  constructor(args) {
    super(args);
    const layerConfig = args.layer.getConfig();
    const forwDict = {};
    forwDict["className"] = args.layer.getClassName();
    forwDict["config"] = layerConfig;
    this.forwardLayer = deserialize(forwDict);
    layerConfig["goBackwards"] = layerConfig["goBackwards"] === true ? false : true;
    const backDict = {};
    backDict["className"] = args.layer.getClassName();
    backDict["config"] = layerConfig;
    this.backwardLayer = deserialize(backDict);
    this.forwardLayer.name = "forward_" + this.forwardLayer.name;
    this.backwardLayer.name = "backward_" + this.backwardLayer.name;
    this.mergeMode = args.mergeMode === void 0 ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;
    checkBidirectionalMergeMode(this.mergeMode);
    if (args.weights) {
      throw new NotImplementedError("weights support is not implemented for Bidirectional layer yet.");
    }
    this._stateful = args.layer.stateful;
    this.returnSequences = args.layer.returnSequences;
    this.returnState = args.layer.returnState;
    this.supportsMasking = true;
    this._trainable = true;
    this.inputSpec = args.layer.inputSpec;
    this.numConstants = null;
  }
  get trainable() {
    return this._trainable;
  }
  set trainable(value) {
    this._trainable = value;
    if (this.forwardLayer != null) {
      this.forwardLayer.trainable = value;
    }
    if (this.backwardLayer != null) {
      this.backwardLayer.trainable = value;
    }
  }
  getWeights() {
    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());
  }
  setWeights(weights) {
    const numWeights = weights.length;
    const numeightsOver2 = Math.floor(numWeights / 2);
    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));
    this.backwardLayer.setWeights(weights.slice(numeightsOver2));
  }
  computeOutputShape(inputShape) {
    let layerShapes = this.forwardLayer.computeOutputShape(inputShape);
    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {
      layerShapes = [layerShapes];
    }
    layerShapes = layerShapes;
    let outputShape;
    let outputShapes;
    let stateShape;
    if (this.returnState) {
      stateShape = layerShapes.slice(1);
      outputShape = layerShapes[0];
    } else {
      outputShape = layerShapes[0];
    }
    outputShape = outputShape;
    if (this.mergeMode === "concat") {
      outputShape[outputShape.length - 1] *= 2;
      outputShapes = [outputShape];
    } else if (this.mergeMode == null) {
      outputShapes = [outputShape, outputShape.slice()];
    } else {
      outputShapes = [outputShape];
    }
    if (this.returnState) {
      if (this.mergeMode == null) {
        return outputShapes.concat(stateShape).concat(stateShape.slice());
      }
      return [outputShape].concat(stateShape).concat(stateShape.slice());
    }
    return singletonOrArray(outputShapes);
  }
  apply(inputs, kwargs) {
    let initialState = kwargs == null ? null : kwargs["initialState"];
    let constants = kwargs == null ? null : kwargs["constants"];
    if (kwargs == null) {
      kwargs = {};
    }
    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);
    inputs = standardized.inputs;
    initialState = standardized.initialState;
    constants = standardized.constants;
    if (Array.isArray(inputs)) {
      initialState = inputs.slice(1);
      inputs = inputs[0];
    }
    if ((initialState == null || initialState.length === 0) && constants == null) {
      return super.apply(inputs, kwargs);
    }
    const additionalInputs = [];
    const additionalSpecs = [];
    if (initialState != null) {
      const numStates = initialState.length;
      if (numStates % 2 > 0) {
        throw new ValueError("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");
      }
      kwargs["initialState"] = initialState;
      additionalInputs.push(...initialState);
      const stateSpecs = initialState.map((state) => new InputSpec({ shape: state.shape }));
      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);
      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);
      additionalSpecs.push(...stateSpecs);
    }
    if (constants != null) {
      throw new NotImplementedError("Support for constants in Bidirectional layers is not implemented yet.");
    }
    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;
    for (const tensor of additionalInputs) {
      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {
        throw new ValueError("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");
      }
    }
    if (isSymbolicTensor) {
      const fullInput = [inputs].concat(additionalInputs);
      const fullInputSpec = this.inputSpec.concat(additionalSpecs);
      const originalInputSpec = this.inputSpec;
      this.inputSpec = fullInputSpec;
      const output = super.apply(fullInput, kwargs);
      this.inputSpec = originalInputSpec;
      return output;
    } else {
      return super.apply(inputs, kwargs);
    }
  }
  call(inputs, kwargs) {
    return tidy(() => {
      const initialState = kwargs["initialState"];
      let y;
      let yRev;
      if (initialState == null) {
        y = this.forwardLayer.call(inputs, kwargs);
        yRev = this.backwardLayer.call(inputs, kwargs);
      } else {
        const forwardState = initialState.slice(0, initialState.length / 2);
        const backwardState = initialState.slice(initialState.length / 2);
        y = this.forwardLayer.call(inputs, Object.assign(kwargs, { initialState: forwardState }));
        yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, { initialState: backwardState }));
      }
      let states;
      if (this.returnState) {
        if (Array.isArray(y)) {
          states = y.slice(1).concat(yRev.slice(1));
        } else {
        }
        y = y[0];
        yRev = yRev[0];
      }
      if (this.returnSequences) {
        yRev = reverse(yRev, 1);
      }
      let output;
      if (this.mergeMode === "concat") {
        output = concatenate([y, yRev]);
      } else if (this.mergeMode === "sum") {
        output = add(y, yRev);
      } else if (this.mergeMode === "ave") {
        output = mul(0.5, add(y, yRev));
      } else if (this.mergeMode === "mul") {
        output = mul(y, yRev);
      } else if (this.mergeMode == null) {
        output = [y, yRev];
      }
      if (this.returnState) {
        if (this.mergeMode == null) {
          return output.concat(states);
        }
        return [output].concat(states);
      }
      return output;
    });
  }
  resetStates(states) {
    this.forwardLayer.resetStates();
    this.backwardLayer.resetStates();
  }
  build(inputShape) {
    nameScope(this.forwardLayer.name, () => {
      this.forwardLayer.build(inputShape);
    });
    nameScope(this.backwardLayer.name, () => {
      this.backwardLayer.build(inputShape);
    });
    this.built = true;
  }
  computeMask(inputs, mask) {
    if (Array.isArray(mask)) {
      mask = mask[0];
    }
    let outputMask;
    if (this.returnSequences) {
      if (this.mergeMode == null) {
        outputMask = [mask, mask];
      } else {
        outputMask = mask;
      }
    } else {
      if (this.mergeMode == null) {
        outputMask = [null, null];
      } else {
        outputMask = null;
      }
    }
    if (this.returnState) {
      const states = this.forwardLayer.states;
      const stateMask = states.map((state) => null);
      if (Array.isArray(outputMask)) {
        return outputMask.concat(stateMask).concat(stateMask);
      } else {
        return [outputMask].concat(stateMask).concat(stateMask);
      }
    } else {
      return outputMask;
    }
  }
  get trainableWeights() {
    return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);
  }
  get nonTrainableWeights() {
    return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);
  }
  // TODO(cais): Implement constraints().
  setFastWeightInitDuringBuild(value) {
    super.setFastWeightInitDuringBuild(value);
    if (this.forwardLayer != null) {
      this.forwardLayer.setFastWeightInitDuringBuild(value);
    }
    if (this.backwardLayer != null) {
      this.backwardLayer.setFastWeightInitDuringBuild(value);
    }
  }
  getConfig() {
    const config = {
      "mergeMode": this.mergeMode
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
  /** @nocollapse */
  static fromConfig(cls, config) {
    const rnnLayer = deserialize(config["layer"]);
    delete config["layer"];
    if (config["numConstants"] != null) {
      throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants present is not supported yet.`);
    }
    const newConfig = config;
    newConfig["layer"] = rnnLayer;
    return new cls(newConfig);
  }
};
Bidirectional.className = "Bidirectional";
serialization_exports.registerClass(Bidirectional);

// node_modules/@tensorflow/tfjs-layers/dist/layers/preprocessing/image_preprocessing.js
var Rescaling = class extends Layer {
  constructor(args) {
    super(args);
    this.scale = args.scale;
    if (args.offset) {
      this.offset = args.offset;
    } else {
      this.offset = 0;
    }
  }
  getConfig() {
    const config = {
      "scale": this.scale,
      "offset": this.offset
    };
    const baseConfig = super.getConfig();
    Object.assign(config, baseConfig);
    return config;
  }
  call(inputs, kwargs) {
    return tidy(() => {
      inputs = getExactlyOneTensor(inputs);
      if (inputs.dtype !== "float32") {
        inputs = cast2(inputs, "float32");
      }
      return add(mul(inputs, this.scale), this.offset);
    });
  }
};
Rescaling.className = "Rescaling";
serialization_exports.registerClass(Rescaling);

// node_modules/@tensorflow/tfjs-layers/dist/exports_layers.js
function inputLayer(args) {
  return new InputLayer(args);
}
function elu3(args) {
  return new ELU(args);
}
function reLU(args) {
  return new ReLU(args);
}
function leakyReLU(args) {
  return new LeakyReLU(args);
}
function prelu2(args) {
  return new PReLU(args);
}
function softmax2(args) {
  return new Softmax3(args);
}
function thresholdedReLU(args) {
  return new ThresholdedReLU(args);
}
function conv1d2(args) {
  return new Conv1D(args);
}
function conv2d2(args) {
  return new Conv2D2(args);
}
function conv2dTranspose2(args) {
  return new Conv2DTranspose(args);
}
function conv3d2(args) {
  return new Conv3D2(args);
}
function conv3dTranspose2(args) {
  return new Conv3DTranspose(args);
}
function separableConv2d2(args) {
  return new SeparableConv2D(args);
}
function cropping2D(args) {
  return new Cropping2D(args);
}
function upSampling2d(args) {
  return new UpSampling2D(args);
}
function depthwiseConv2d3(args) {
  return new DepthwiseConv2D(args);
}
function activation(args) {
  return new Activation2(args);
}
function dense(args) {
  return new Dense(args);
}
function dropout3(args) {
  return new Dropout(args);
}
function spatialDropout1d(args) {
  return new SpatialDropout1D(args);
}
function flatten2(args) {
  return new Flatten(args);
}
function repeatVector(args) {
  return new RepeatVector(args);
}
function reshape2(args) {
  return new Reshape2(args);
}
function permute(args) {
  return new Permute(args);
}
function embedding(args) {
  return new Embedding(args);
}
function add2(args) {
  return new Add2(args);
}
function average(args) {
  return new Average(args);
}
function concatenate2(args) {
  return new Concatenate(args);
}
function maximum2(args) {
  return new Maximum2(args);
}
function minimum2(args) {
  return new Minimum2(args);
}
function multiply(args) {
  return new Multiply2(args);
}
function dot2(args) {
  return new Dot(args);
}
function batchNormalization2(args) {
  return new BatchNormalization(args);
}
function layerNormalization(args) {
  return new LayerNormalization(args);
}
function zeroPadding2d(args) {
  return new ZeroPadding2D(args);
}
function averagePooling1d(args) {
  return new AveragePooling1D(args);
}
function avgPool1d(args) {
  return averagePooling1d(args);
}
function avgPooling1d(args) {
  return averagePooling1d(args);
}
function averagePooling2d(args) {
  return new AveragePooling2D(args);
}
function avgPool2d(args) {
  return averagePooling2d(args);
}
function avgPooling2d(args) {
  return averagePooling2d(args);
}
function averagePooling3d(args) {
  return new AveragePooling3D(args);
}
function avgPool3d2(args) {
  return averagePooling3d(args);
}
function avgPooling3d(args) {
  return averagePooling3d(args);
}
function globalAveragePooling1d(args) {
  return new GlobalAveragePooling1D(args);
}
function globalAveragePooling2d(args) {
  return new GlobalAveragePooling2D(args);
}
function globalMaxPooling1d(args) {
  return new GlobalMaxPooling1D(args);
}
function globalMaxPooling2d(args) {
  return new GlobalMaxPooling2D(args);
}
function maxPooling1d(args) {
  return new MaxPooling1D(args);
}
function maxPooling2d(args) {
  return new MaxPooling2D(args);
}
function maxPooling3d(args) {
  return new MaxPooling3D(args);
}
function gru(args) {
  return new GRU(args);
}
function gruCell(args) {
  return new GRUCell(args);
}
function lstm(args) {
  return new LSTM(args);
}
function lstmCell(args) {
  return new LSTMCell(args);
}
function simpleRNN(args) {
  return new SimpleRNN(args);
}
function simpleRNNCell(args) {
  return new SimpleRNNCell(args);
}
function convLstm2d(args) {
  return new ConvLSTM2D(args);
}
function convLstm2dCell(args) {
  return new ConvLSTM2DCell(args);
}
function rnn2(args) {
  return new RNN(args);
}
function stackedRNNCells(args) {
  return new StackedRNNCells(args);
}
function bidirectional(args) {
  return new Bidirectional(args);
}
function timeDistributed(args) {
  return new TimeDistributed(args);
}
var globalMaxPool1d = globalMaxPooling1d;
var globalMaxPool2d = globalMaxPooling2d;
var maxPool1d = maxPooling1d;
var maxPool2d = maxPooling2d;
function gaussianNoise(args) {
  return new GaussianNoise(args);
}
function gaussianDropout(args) {
  return new GaussianDropout(args);
}
function alphaDropout(args) {
  return new AlphaDropout(args);
}
function masking(args) {
  return new Masking(args);
}
function rescaling(args) {
  return new Rescaling(args);
}

// node_modules/@tensorflow/tfjs-layers/dist/exports_metrics.js
var exports_metrics_exports = {};
__export(exports_metrics_exports, {
  MAPE: () => MAPE2,
  MSE: () => MSE2,
  binaryAccuracy: () => binaryAccuracy2,
  binaryCrossentropy: () => binaryCrossentropy3,
  categoricalAccuracy: () => categoricalAccuracy2,
  categoricalCrossentropy: () => categoricalCrossentropy3,
  cosineProximity: () => cosineProximity2,
  mape: () => mape2,
  meanAbsoluteError: () => meanAbsoluteError2,
  meanAbsolutePercentageError: () => meanAbsolutePercentageError2,
  meanSquaredError: () => meanSquaredError2,
  mse: () => mse2,
  precision: () => precision2,
  recall: () => recall2,
  sparseCategoricalAccuracy: () => sparseCategoricalAccuracy2
});
function binaryAccuracy2(yTrue, yPred) {
  return binaryAccuracy(yTrue, yPred);
}
function binaryCrossentropy3(yTrue, yPred) {
  return binaryCrossentropy2(yTrue, yPred);
}
function sparseCategoricalAccuracy2(yTrue, yPred) {
  return sparseCategoricalAccuracy(yTrue, yPred);
}
function categoricalAccuracy2(yTrue, yPred) {
  return categoricalAccuracy(yTrue, yPred);
}
function categoricalCrossentropy3(yTrue, yPred) {
  return categoricalCrossentropy2(yTrue, yPred);
}
function precision2(yTrue, yPred) {
  return precision(yTrue, yPred);
}
function recall2(yTrue, yPred) {
  return recall(yTrue, yPred);
}
function cosineProximity2(yTrue, yPred) {
  return cosineProximity(yTrue, yPred);
}
function meanAbsoluteError2(yTrue, yPred) {
  return meanAbsoluteError(yTrue, yPred);
}
function meanAbsolutePercentageError2(yTrue, yPred) {
  return meanAbsolutePercentageError(yTrue, yPred);
}
function MAPE2(yTrue, yPred) {
  return meanAbsolutePercentageError(yTrue, yPred);
}
function mape2(yTrue, yPred) {
  return meanAbsolutePercentageError(yTrue, yPred);
}
function meanSquaredError2(yTrue, yPred) {
  return meanSquaredError(yTrue, yPred);
}
function MSE2(yTrue, yPred) {
  return meanSquaredError(yTrue, yPred);
}
function mse2(yTrue, yPred) {
  return meanSquaredError(yTrue, yPred);
}

// node_modules/@tensorflow/tfjs-layers/dist/exports_models.js
var exports_models_exports = {};
__export(exports_models_exports, {
  modelFromJSON: () => modelFromJSON
});

// node_modules/@tensorflow/tfjs-layers/dist/exports_regularizers.js
var exports_regularizers_exports = {};
__export(exports_regularizers_exports, {
  l1: () => l12,
  l1l2: () => l1l2,
  l2: () => l22
});
function l1l2(config) {
  return new L1L2(config);
}
function l12(config) {
  return l1(config);
}
function l22(config) {
  return l2(config);
}

// node_modules/@tensorflow/tfjs-layers/dist/callbacks.js
var Callback = class extends BaseCallback {
  constructor() {
    super(...arguments);
    this.model = null;
  }
  setModel(model2) {
    if (!(model2 instanceof LayersModel)) {
      throw new Error("model must be a LayersModel, not some other Container");
    }
    this.model = model2;
  }
};
function less2(currVal, prevVal) {
  return currVal < prevVal;
}
function greater2(currVal, prevVal) {
  return currVal > prevVal;
}
var EarlyStopping = class extends Callback {
  constructor(args) {
    super();
    if (args == null) {
      args = {};
    }
    if (args.restoreBestWeights) {
      throw new NotImplementedError("restoreBestWeights = True is not implemented in EarlyStopping yet.");
    }
    this.monitor = args.monitor || "val_loss";
    this.minDelta = Math.abs(args.minDelta || 0);
    this.patience = args.patience || 0;
    this.verbose = args.verbose || 0;
    this.mode = args.mode || "auto";
    this.baseline = args.baseline;
    if (["auto", "min", "max"].indexOf(this.mode) === -1) {
      console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`);
      this.mode = "auto";
    }
    if (this.mode === "min") {
      this.monitorFunc = less2;
    } else if (this.mode === "max") {
      this.monitorFunc = greater2;
    } else {
      if (this.monitor.indexOf("acc") !== -1) {
        this.monitorFunc = greater2;
      } else {
        this.monitorFunc = less2;
      }
    }
    if (this.monitorFunc === less2) {
      this.minDelta *= -1;
    }
  }
  async onTrainBegin(logs) {
    this.wait = 0;
    this.stoppedEpoch = 0;
    if (this.baseline != null) {
      this.best = this.baseline;
    } else {
      this.best = this.monitorFunc === less2 ? Infinity : -Infinity;
    }
  }
  async onEpochEnd(epoch, logs) {
    await resolveScalarsInLogs(logs);
    const current = this.getMonitorValue(logs);
    if (current == null) {
      return;
    }
    if (this.monitorFunc(current - this.minDelta, this.best)) {
      this.best = current;
      this.wait = 0;
    } else {
      this.wait++;
      if (this.wait >= this.patience) {
        this.stoppedEpoch = epoch;
        this.model.stopTraining = true;
      }
    }
  }
  async onTrainEnd(logs) {
    if (this.stoppedEpoch > 0 && this.verbose) {
      console.log(`Epoch ${this.stoppedEpoch}: early stopping.`);
    }
  }
  getMonitorValue(logs) {
    if (logs == null) {
      logs = {};
    }
    const monitorValue = logs[this.monitor];
    if (monitorValue == null) {
      console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: ${Object.keys(logs)}`);
    }
    return monitorValue;
  }
};
function earlyStopping(args) {
  return new EarlyStopping(args);
}
var callbacks = { earlyStopping };

export {
  LayerVariable,
  InputSpec,
  SymbolicTensor,
  exports_constraints_exports,
  exports_initializers_exports,
  CallbackList,
  History,
  CustomCallback,
  version,
  LayersModel,
  Sequential,
  model,
  sequential,
  loadLayersModel,
  input,
  registerCallbackConstructor,
  RNN,
  exports_layers_exports,
  exports_metrics_exports,
  exports_models_exports,
  exports_regularizers_exports,
  Callback,
  EarlyStopping,
  callbacks
};
/*! Bundled license information:

@tensorflow/tfjs-layers/dist/errors.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/utils/executor_utils.js:
  (**
   * @license
   * Copyright 2022 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/utils/generic_utils.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/backend/state.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/keras_format/common.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/common.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/utils/math_utils.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/backend/common.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/backend/tfjs_backend.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/keras_format/initializer_config.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/initializers.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/utils/types_utils.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/utils/variable_utils.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/variables.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/engine/topology.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/engine/input_layer.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/engine/executor.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/flags_layers.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Abs_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Acos_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Acosh_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Add_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/AddN_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/ArgMax_grad.js:
  (**
   * @license
   * Copyright 2020 Google Inc. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/ArgMin_grad.js:
  (**
   * @license
   * Copyright 2020 Google Inc. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Asin_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Asinh_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Atan2_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Atan_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Atanh_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/ops/avg_pool_3d_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/AvgPool3D_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/ops/avg_pool_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/AvgPool_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/BatchMatMul_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/BatchToSpaceND_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/BroadcastTo_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Cast_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Ceil_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/ClipByValue_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/ComplexAbs_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Concat_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Conv2D_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Conv2DBackpropInput_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/ops/conv3d_backprop_filter.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Conv3D_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Cos_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Cosh_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Cumsum_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/DepthwiseConv2dNative_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Dilation2D_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Elu_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Erf_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Exp_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/ExpandDims_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Expm1_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Floor_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/FloorDiv_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/FusedBatchNorm_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/GatherV2_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/GreaterEqual_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Identity_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/IsFinite_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/IsInf_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/IsNan_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/LeakyRelu_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Log1p_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Log_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/LogSoftmax_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/ops/local_response_normalization_backprop.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/LRN_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/min_max_grad_util.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Max_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Maximum_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/ops/max_pool_3d_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/MaxPool3D_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/ops/max_pool_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/MaxPool_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Mean_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Min_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Minimum_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/MirrorPad_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Mod_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Multiply_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Neg_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/OneHot_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/OnesLike_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Pack_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/PadV2_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Pow_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Prelu_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Prod_grad.js:
  (**
   * @license
   * Copyright 2022 Google Inc. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/RealDiv_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Reciprocal_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Relu6_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Relu_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Reshape_grad.js:
  (**
   * @license
   * Copyright 2020 Google Inc. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/ResizeBilinear_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/ResizeNearestNeighbor_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Reverse_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Round_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Rsqrt_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Select_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Selu_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Sigmoid_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Sign_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Sin_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Sinh_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Slice_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Softmax_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Softplus_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/SpaceToBatchND_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/SplitV_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Sqrt_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Square_grad.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/SquaredDifference_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Step_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Sub_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Sum_grad.js:
  (**
   * @license
   * Copyright 2020 Google Inc. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Tan_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Tanh_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Tile_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Transpose_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/Unpack_grad.js:
  (**
   * @license
   * Copyright 2020 Google Inc. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/UnsortedSegmentSum_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/gradients/ZerosLike_grad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-core/dist/register_all_gradients.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/constraints.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/exports_constraints.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/exports_initializers.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/logs.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/base_callbacks.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/serialization.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/losses.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/metrics.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/optimizers.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/user_defined_metadata.js:
  (**
   * @license
   * Copyright 2019 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/utils/layer_utils.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/utils/serialization_utils.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/version.js:
  (** @license See the LICENSE file. *)

@tensorflow/tfjs-layers/dist/engine/container.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/engine/training_utils.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/engine/training_dataset.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/engine/training_tensors.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/engine/training.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/models.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/exports.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/activations.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/regularizers.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/advanced_activations.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/utils/conv_utils.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/convolutional.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/convolutional_depthwise.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/recurrent.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/convolutional_recurrent.js:
  (**
   * @license
   * Copyright 2020 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/core.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/embeddings.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/merge.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/noise.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/normalization.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/padding.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/pooling.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/wrappers.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/layers/preprocessing/image_preprocessing.js:
  (**
   * @license
   * Copyright 2022 CodeSmith LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/exports_layers.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/exports_models.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/exports_regularizers.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/callbacks.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)

@tensorflow/tfjs-layers/dist/index.js:
  (**
   * @license
   * Copyright 2018 Google LLC
   *
   * Use of this source code is governed by an MIT-style
   * license that can be found in the LICENSE file or at
   * https://opensource.org/licenses/MIT.
   * =============================================================================
   *)
*/
//# sourceMappingURL=chunk-HJEXW4TU.js.map
